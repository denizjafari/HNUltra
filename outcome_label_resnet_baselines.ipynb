{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZbZRovgc-In"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from torch.utils import data\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "import io\n",
    "import uuid\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import wandb\n",
    "wandb_username = 'andreasabo'\n",
    "local_username = 'andreasabo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2aVSXNyc-Iv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:1') \n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylfRtoN9c-Iy"
   },
   "outputs": [],
   "source": [
    "# root directory\n",
    "root_dir = \"/home/andreasabo/Documents/HNProject/\"\n",
    "split_file_base = \"/home/andreasabo/Documents/HNUltra/\"\n",
    "\n",
    "# data directory on current machine: abhishekmoturu, andreasabo, denizjafari, navidkorhani\n",
    "data_dir = \"/home/\" + local_username + \"/Documents/HNProject/all_label_img/\"\n",
    "\n",
    "# read target df\n",
    "csv_path = os.path.join(root_dir, \"all_splits_1000000.csv\")\n",
    "data_df = pd.read_csv(csv_path, usecols=['subj_id', 'scan_num', 'view_label', 'image_ids', 'reflux_label', 'function_label', 'surgery_label', 'outcome_train', 'view_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  function_label image_ids reflux_label surgery_label view_label  subj_id  \\\n",
      "0        Missing  1323_2_1      Missing       Missing    Missing     1323   \n",
      "1        Missing  1323_2_2      Missing       Missing    Missing     1323   \n",
      "2        Missing  1323_2_3      Missing       Missing    Missing     1323   \n",
      "3        Missing  1323_2_4      Missing       Missing    Missing     1323   \n",
      "4        Missing  1323_2_5      Missing       Missing    Missing     1323   \n",
      "\n",
      "   scan_num  outcome_train  \n",
      "0         2            NaN  \n",
      "1         2            NaN  \n",
      "2         2            NaN  \n",
      "3         2            NaN  \n",
      "4         2            NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72459"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_df.head())\n",
    "len(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_OCR_7uy52w"
   },
   "source": [
    "### **Reading Data Indicies and Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 18\n"
     ]
    }
   ],
   "source": [
    "# data_df = data_df[data_df.view_label != \"Missing\"]\n",
    "# data_df = data_df[data_df.view_label != \"Other\"]\n",
    "train_df = data_df[data_df.view_train == 1]\n",
    "test_df = data_df[data_df.view_train == 0]\n",
    "\n",
    "num_train_subj = len(train_df.subj_id.unique())\n",
    "num_test_subj = len(test_df.subj_id.unique())\n",
    "print(num_train_subj, num_test_subj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nnwavxcqGBv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 9581 images (7230 train and 2351 test)\n",
      "Reading splits from file\n"
     ]
    }
   ],
   "source": [
    "# Drop the images for which we do not have view labels or the label is \"other\"\n",
    "data_df = data_df[data_df.view_label != \"Missing\"]\n",
    "data_df = data_df[data_df.view_label != \"Other\"]\n",
    "train_df = data_df[data_df.outcome_train == 1]\n",
    "test_df = data_df[data_df.outcome_train == 0]\n",
    "\n",
    "print(f\"We have {len(test_df) + len(train_df)} images ({len(train_df)} train and {len(test_df)} test)\")\n",
    "unique_subj = train_df.subj_id.unique()\n",
    "\n",
    "# Create the splits for 5-fold cross validation based on subj_id\n",
    "data_split_file = split_file_base + 'data_splits_outcome.json'\n",
    "if not os.path.isfile(data_split_file):\n",
    "\n",
    "    kf = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    fold = 0\n",
    "    all_folds = {}\n",
    "    for train_subj, val_subj in kf.split(unique_subj):\n",
    "        train_ids  = unique_subj[train_subj]\n",
    "        val_ids = unique_subj[val_subj]\n",
    "        \n",
    "        # Save the image names\n",
    "        train_images = train_df[train_df.subj_id.isin(train_ids)].image_ids.tolist()\n",
    "        val_images = train_df[train_df.subj_id.isin(val_ids)].image_ids.tolist()\n",
    "        \n",
    "        # Save the scan number\n",
    "        train_scan = train_df[train_df.subj_id.isin(train_ids)].scan_num.tolist()\n",
    "        val_scan = train_df[train_df.subj_id.isin(val_ids)].scan_num.tolist()\n",
    "        \n",
    "        # Save the view \n",
    "        train_views = train_df[train_df.subj_id.isin(train_ids)].view_label.tolist()\n",
    "        val_views = train_df[train_df.subj_id.isin(val_ids)].view_label.tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Save the outcome labels\n",
    "        train_function = train_df[train_df.subj_id.isin(train_ids)].function_label.tolist()\n",
    "        val_function = train_df[train_df.subj_id.isin(val_ids)].function_label.tolist()\n",
    "        \n",
    "        train_reflux = train_df[train_df.subj_id.isin(train_ids)].reflux_label.tolist()\n",
    "        val_reflux = train_df[train_df.subj_id.isin(val_ids)].reflux_label.tolist() \n",
    "        \n",
    "        train_surgery = train_df[train_df.subj_id.isin(train_ids)].surgery_label.tolist()\n",
    "        val_surgery = train_df[train_df.subj_id.isin(val_ids)].surgery_label.tolist()\n",
    "        \n",
    "        \n",
    "        val_labels = train_df[train_df.subj_id.isin(val_ids)].view_label.tolist()\n",
    "        cur_fold = {'train_images': train_images, 'val_images': val_images, 'train_reflux': train_reflux, \n",
    "                    'val_reflux': val_reflux, 'train_function': train_function, 'val_function': val_function, \n",
    "                    'train_surgery': train_surgery, 'val_surgery': val_surgery, 'train_scan': train_scan,\n",
    "                    'val_scan': val_scan, 'train_views': train_views, 'val_views': val_views}\n",
    "        \n",
    "        all_folds[fold] = cur_fold\n",
    "        fold += 1\n",
    "\n",
    "    print(\"Saving data splits\")\n",
    "    with open(data_split_file, 'w') as f:\n",
    "        json.dump(all_folds, f)\n",
    "        \n",
    "else: # just load from file\n",
    "    print(\"Reading splits from file\")\n",
    "    with open(data_split_file, 'r') as f:\n",
    "        all_folds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset: right_sag, right_trav, left_sag, left_trav, bladder, other\n",
    "num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 100\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 75\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model; when True we only update the reshaped layer params\n",
    "feature_extract = False\n",
    "\n",
    "# Flag for whether or not to use pretrained model\n",
    "pretrain = False\n",
    "\n",
    "# Flag for whether or not to sample sets of images by scan (ensures that all scans are seen the same number of times)\n",
    "sample_by_scan = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting, layers_to_train):\n",
    "    ct = 0 \n",
    "    if feature_extracting:\n",
    "        for child in model.children():\n",
    "            ct +=1\n",
    "            if ct <layers_to_train:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract,layers_to_train, use_pretrained=True, num_input_channels = 1):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        source for conv1: https://discuss.pytorch.org/t/grayscale-images-for-resenet-and-deeplabv3/48693/2\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract, layers_to_train)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.conv1 =  nn.Conv2d(num_input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        model_ft.fc = nn.Sequential(\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(num_ftrs, num_classes),\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        nn.Sigmoid\n",
    "        input_size = 256\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "        \n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from: https://gist.github.com/stefanonardo/693d96ceb2f531fa05db530f3e21517d\n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=True):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if np.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iikM7_G3c-JR"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class ScanDataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, image_list, reflux_labels, surgery_labels, function_labels, view_labels, scan_labels, binarize_labels = True, transformations=None, sample_by_scan=True , target_view=None):\n",
    "        'Initialization'\n",
    "        self.image_list = image_list\n",
    "        self.subj_id = np.asarray([int(s[0:4]) for s in image_list])\n",
    "\n",
    "        self.reflux_labels = reflux_labels        \n",
    "        self.surgery_labels = surgery_labels\n",
    "        self.function_labels = function_labels       \n",
    "        self.view_labels = view_labels\n",
    "        self.scan_labels = np.asarray(scan_labels)\n",
    "        self.binarize_labels = binarize_labels\n",
    "        self.transformations = transformations\n",
    "        self.sample_by_scan = sample_by_scan\n",
    "        self.target_view = target_view\n",
    "        self.image_return_order = ['Saggital_Right', 'Transverse_Right', 'Saggital_Left', 'Transverse_Left', 'Bladder']\n",
    "        \n",
    "        # Create a list of indices that we will access the images in\n",
    "        random.seed(0)\n",
    "        list_of_inds = [num for num in range(len(image_list))]\n",
    "        \n",
    "        # UPDATE: don't need this since the dataloader shuffles for us\n",
    "        #random.shuffle(list_of_inds) #shuffle method\n",
    "\n",
    "        self.index_order = list_of_inds\n",
    "        self.all_view_names = list(set((view_labels)))\n",
    "        \n",
    "        # If we are going to group by scans, create the scan list\n",
    "        self.all_scan_ids = np.asarray([(str(self.subj_id[i]) + \"_\" + str(self.scan_labels[i])) for i in range(len(self.scan_labels))])\n",
    "        self.unique_scans = list(set(self.all_scan_ids))\n",
    "        \n",
    "        if target_view is not None:\n",
    "            self.images_of_view = []\n",
    "            for ind, v in enumerate(view_labels):\n",
    "                if v == target_view:\n",
    "                    self.images_of_view.append(ind)\n",
    "\n",
    "        \n",
    "  def __len__(self):\n",
    "        'Denotes the total number of image_list or scan_list'\n",
    "        if self.target_view is not None:\n",
    "            return len(self.images_of_view)\n",
    "        \n",
    "        if self.sample_by_scan:\n",
    "            return len(self.unique_scans)\n",
    "        else:\n",
    "            return len(self.image_list)\n",
    "\n",
    "  def __getitem__(self, ind):\n",
    "        'Generates one sample of data'\n",
    "        output_image_path_dict = {}\n",
    "        if not self.sample_by_scan and self.target_view is not None:\n",
    "            index = self.images_of_view[ind]\n",
    "            output_image_path_dict[self.target_view] = data_dir + self.image_list[index] + '.jpg'\n",
    "        \n",
    "        else:\n",
    "            if self.sample_by_scan:\n",
    "                cur_scan = self.unique_scans[ind]\n",
    "                images_of_scan = (self.all_scan_ids == cur_scan)\n",
    "                indexes_in_scan = [i for i, x in enumerate(images_of_scan) if x]\n",
    "                index = indexes_in_scan[0] # For now, just take the first index\n",
    "\n",
    "            else:\n",
    "                # Select sample based on current images\n",
    "                index = self.index_order[ind]\n",
    "                image_id = self.image_list[index]\n",
    "\n",
    "                # We want to use the current image, as well as images from the same scan (from the same patient)\n",
    "                # To fill images of the other 4 classes of images\n",
    "                # First finds all of the images in this scan\n",
    "                images_of_scan = (self.subj_id == self.subj_id[index]) & (self.scan_labels == self.scan_labels[index])\n",
    "                indexes_in_scan = [i for i, x in enumerate(images_of_scan) if x]\n",
    "\n",
    "            # Group images in the scan by view and select one of each (if available)\n",
    "            dict_of_scans_by_view = {}\n",
    "            # looping through views\n",
    "            for inds in indexes_in_scan:\n",
    "                if self.view_labels[inds] not in dict_of_scans_by_view:\n",
    "                    dict_of_scans_by_view[self.view_labels[inds]] = [inds]\n",
    "                else:\n",
    "                    dict_of_scans_by_view[self.view_labels[inds]].append(inds)\n",
    "\n",
    "            # If we directly picked an image, make sure this one is the only one for the view\n",
    "            if not self.sample_by_scan:\n",
    "                dict_of_scans_by_view[self.view_labels[index]] = [index]\n",
    "\n",
    "\n",
    "            output_image_path_dict = {}\n",
    "            # Now loop through all the views and randomly select an image.  \n",
    "            for view in dict_of_scans_by_view:\n",
    "                    \n",
    "                    random_view_from_scan_ind = random.choice(dict_of_scans_by_view[view])\n",
    "                    output_image_path_dict[view] = data_dir + self.image_list[random_view_from_scan_ind] + '.jpg'\n",
    "                    \n",
    "                    if self.target_view is view and self.sample_by_scan:\n",
    "                        index = random_view_from_scan_ind\n",
    "                    \n",
    "\n",
    "        output_images = []\n",
    "        for view in self.image_return_order:\n",
    "            if view not in output_image_path_dict:\n",
    "                empty_im = [np.ones((256, 256))*np.nan]\n",
    "                empty_im = torch.FloatTensor(empty_im)\n",
    "                output_images.append( empty_im)\n",
    "            else:\n",
    "                image = Image.open(output_image_path_dict[view]).convert('L')\n",
    "                if self.transformations:\n",
    "                    image = self.transformations(image)\n",
    "                image = ToTensor()(image)\n",
    "                output_images.append(image)\n",
    "\n",
    "\n",
    "\n",
    "        y = [self.reflux_labels[index], self.surgery_labels[index], self.function_labels[index]]\n",
    "        for i, outcome in enumerate(y):\n",
    "            # Convert \"yes\" and \"no\" to 0/1\n",
    "            if outcome == \"No\":\n",
    "                y[i] = 0\n",
    "            elif outcome == \"Yes\":\n",
    "                y[i] = 1\n",
    "            elif outcome == \"Missing\":\n",
    "                y[i] = np.nan\n",
    "\n",
    "            # Should we also binarize the function labels?\n",
    "            elif self.binarize_labels:\n",
    "                if float(outcome) > 60 or float(outcome) < 40:\n",
    "                    y[i] = 1\n",
    "                else:\n",
    "                    y[i] = 0\n",
    "            else:\n",
    "                y[i] = float(outcome)\n",
    "\n",
    "        y = torch.FloatTensor(y)\n",
    "        output_images = torch.stack(output_images)\n",
    "        return output_images, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, view_to_use, num_epochs=2, is_inception=False, final_testing=False):\n",
    "    es = EarlyStopping(patience = 20)\n",
    "    stop_now = 0\n",
    "    results_dict = {}\n",
    "    \n",
    "    all_views = ['Saggital_Right', 'Transverse_Right', 'Saggital_Left', 'Transverse_Left', 'Bladder']\n",
    "\n",
    "    if view_to_use not in all_views:\n",
    "        print(f\"View {view_to_use} does not exist\")\n",
    "        return\n",
    "    \n",
    "    view_index = all_views.index(view_to_use)\n",
    "    since = time.time()\n",
    "    \n",
    "    classnames = ['Reflux', 'Surgery', 'Function']\n",
    "    val_acc_history = []\n",
    "    \n",
    "    val_metrics_list = []\n",
    "    train_metrics_list = []\n",
    "    best_auc = 0\n",
    "    best_epoch = 0\n",
    "    lowest_loss = 999999\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_accs = [0.0]*3\n",
    "    epoch_with_best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 54)\n",
    "\n",
    "        if stop_now:\n",
    "            break\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                if epoch % 1 is not 0:\n",
    "                    continue\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            running_preds = []\n",
    "            running_labels = []\n",
    "            \n",
    "            running_num_samples = 0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "#                 labels = labels.type(torch.long)\n",
    "                # Only use the image for the specified view \n",
    "                inputs = inputs[:, view_index, :, :]\n",
    "#                 inputs = inputs.squeeze()\n",
    "#                 print(\"-\"*45)\n",
    "                \n",
    "                # Remove the indices with missing images\n",
    "                input_sums = torch.sum(torch.sum(torch.sum(torch.isnan(inputs), 1), 1), 1)\n",
    "                inputs_good = inputs[input_sums < 1, :, :, :]\n",
    "                labels_good = labels[input_sums < 1, :]\n",
    "#                 print(inputs_good.shape)\n",
    "                # Create a mask of whether we have each an outcome label at each label index\n",
    "                label_missing = torch.isnan(labels_good)\n",
    "\n",
    "                inputs = inputs_good.to(device)\n",
    "                labels = labels_good.to(device)\n",
    "                label_missing = label_missing.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                 # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        \n",
    "                        \n",
    "                        running_preds += outputs.tolist()\n",
    "                        running_labels += labels.tolist()\n",
    "                        \n",
    "                        # multiply the outputs and labels by the label mask so that missing labels do not contribute \n",
    "                        # to loss\n",
    "                        outputs_for_loss = outputs * (~label_missing)\n",
    "                        outputs_for_loss[abs(outputs_for_loss) < 0.000001] = 0\n",
    "                        labels_for_loss = labels.clone()\n",
    "                        labels_for_loss[labels_for_loss != labels_for_loss] = 0\n",
    "                    \n",
    "                        # Create a weights matrix so that we can place higher value on the function outcome\n",
    "                        # which is often missing\n",
    "                        loss_weights = torch.ones_like(outputs)\n",
    "                        loss_weights[label_missing] = 0\n",
    "                        col_sum = torch.sum(loss_weights, 0)\n",
    "                        loss_weights = (loss_weights / col_sum) * inputs.size(0)\n",
    "                        loss_weights[loss_weights > 1] = 1.25\n",
    "                        #print(loss_weights)\n",
    "#                         criterion = nn.BCEWithLogitsLoss(weight=loss_weights)\n",
    "                        criterion = nn.BCEWithLogitsLoss()\n",
    "                        loss = criterion(outputs_for_loss, labels_for_loss)\n",
    "    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_num_samples += inputs.size(0)\n",
    "                \n",
    "            # Epoch is done    \n",
    "            epoch_loss = running_loss / running_num_samples\n",
    "            \n",
    "            # Calculate AUROC and AUPRC for each outcome\n",
    "            results_dict.update({phase + '_loss': epoch_loss})\n",
    "            preds = np.asarray(running_preds)\n",
    "            true_labels = np.asarray(running_labels)\n",
    "            \n",
    "            sum_auc = 0\n",
    "            sum_ave_prec = 0\n",
    "            # source: https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\n",
    "            fpr = dict()\n",
    "            tpr = dict()\n",
    "            roc_auc = dict()\n",
    "            precision = dict()\n",
    "            recall = dict()\n",
    "            no_skill = dict()\n",
    "            average_precision = dict()\n",
    "            only_one_class = 0\n",
    "            for ind, cl in enumerate(classnames):\n",
    "                try:\n",
    "\n",
    "                    # Remove the nans when computing metrics\n",
    "                    true_outcome_labels = true_labels[:, ind]\n",
    "                    predicted_outcome_labels = preds[:, ind]\n",
    "                    non_nan_locs = np.squeeze(np.argwhere(~np.isnan(true_outcome_labels)))\n",
    "                    true_outcome_labels = true_outcome_labels[non_nan_locs]\n",
    "                    predicted_outcome_labels = predicted_outcome_labels[non_nan_locs]\n",
    "\n",
    "                    auroc = roc_auc_score(true_outcome_labels, predicted_outcome_labels)\n",
    "                    auprc = average_precision_score(true_outcome_labels, predicted_outcome_labels)\n",
    "                    results_dict[phase+ \"_\" + cl+\"_auroc\"] = auroc\n",
    "                    results_dict[phase+ \"_\" + cl+\"_auprc\"] = auprc\n",
    "                    sum_auc += auroc\n",
    "\n",
    "\n",
    "                    fpr[cl], tpr[cl], _ = roc_curve(true_outcome_labels, predicted_outcome_labels)\n",
    "                    precision[cl], recall[cl], _ = precision_recall_curve(true_outcome_labels, predicted_outcome_labels)\n",
    "                    roc_auc[cl] = auroc\n",
    "                    average_precision[cl] = auprc\n",
    "                    no_skill[cl] = len(true_outcome_labels[true_outcome_labels==1]) / len(true_outcome_labels)\n",
    "\n",
    "                    sum_ave_prec += auprc\n",
    "\n",
    "                    results_dict[phase+\"_\"+cl +\"_auroc\"] = auroc\n",
    "                    results_dict[phase+\"_\"+cl +\"_auprc\"] = auprc                \n",
    "\n",
    "\n",
    "                except:\n",
    "                    only_one_class = 1\n",
    "                    \n",
    "                results_dict[phase+\"_sum_auroc\"] = sum_auc\n",
    "                results_dict[phase+\"_sum_auprc\"] = sum_ave_prec\n",
    "#             plt.figure()\n",
    "#             lw = 2\n",
    "#             if not only_one_class:\n",
    "#                 for cl in classnames:\n",
    "#                     plt.plot(fpr[cl], tpr[cl], \n",
    "#                              lw=lw, label=cl + '  ROC curve (area = %0.2f)' % roc_auc[cl])\n",
    "#     #                 plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "#                     plt.xlim([0.0, 1.0])\n",
    "#                     plt.ylim([0.0, 1.05])\n",
    "#                     plt.xlabel('False Positive Rate')\n",
    "#                     plt.ylabel('True Positive Rate')\n",
    "#                     plt.title(phase + ' ROC of Outcomes, Epoch: ' + str(epoch))\n",
    "#                     plt.legend(loc=\"lower right\")\n",
    "#     #             plt.show()\n",
    "\n",
    "#                 # Logging to wandb for analysis \n",
    "#                 wandb.log({phase + \"_auroc_plot_\"+ str(epoch): wandb.Image(plt)}, step=epoch)\n",
    "\n",
    "#                 plt.figure()\n",
    "#                 lw = 2\n",
    "#                 #source: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "#                 for cl in classnames:\n",
    "\n",
    "#                     plt.plot(recall[cl], precision[cl], \n",
    "#                              lw=lw, label=cl + '  PR curve (area = %0.2f)' % average_precision[cl])\n",
    "#     #                 plt.plot([0, 1], [no_skill[cl], no_skill[cl]], color='navy', lw = lw, linestyle='--', label='No Skill')\n",
    "\n",
    "#                     plt.xlim([0.0, 1.0])\n",
    "#                     plt.ylim([0.0, 1.05])\n",
    "#                     plt.xlabel('Recall')\n",
    "#                     plt.ylabel('Precision')\n",
    "#                     plt.title(phase + ' Precision-Recall Curve of Outcomes, Epoch: '+ str(epoch))\n",
    "#                     plt.legend(loc=\"upper right\")\n",
    "#     #             plt.show()\n",
    "#                 wandb.log({phase + \"_auprc_plot_\"+ str(epoch): wandb.Image(plt), \"epoch\": epoch}, step=epoch)\n",
    "            \n",
    "            wandb.log(results_dict, step=epoch)\n",
    "            \n",
    "            \n",
    "            if phase == \"val\" and epoch_loss < lowest_loss and epoch >= 30:\n",
    "                lowest_loss = epoch_loss\n",
    "                best_epoch = epoch\n",
    "                results_dict.update({\"best_epoch\":best_epoch})\n",
    "                wandb.log(results_dict, step=epoch)\n",
    "\n",
    "                metrics_from_best_epoch = copy.deepcopy(results_dict)\n",
    "                print(metrics_from_best_epoch)\n",
    "            print('{} loss:\\t{:.4f} | {} sum of auc:\\t{:.4f} \\n'.format(phase, epoch_loss, phase, sum_auc))\n",
    "            \n",
    "            if phase == 'val' and epoch >= 30:\n",
    "                if es.step(epoch_loss) and not final_testing:\n",
    "                    stop_now = 1\n",
    "                    print(\"EARLY STOPPING \" + str(epoch))\n",
    "                    break\n",
    "    \n",
    "    return metrics_from_best_epoch\n",
    "\n",
    "\n",
    "# all_views = ['Saggital_Right', 'Transverse_Right', 'Saggital_Left', 'Transverse_Left', 'Bladder']\n",
    "# view_to_use = 'Saggital_Left' \n",
    "# fold = '0'\n",
    "# # Use the first fold for now\n",
    "# partition = all_folds[fold]\n",
    "\n",
    "# # Test out dataloaders\n",
    "# shuffle = True\n",
    "# num_workers = 0\n",
    "# binarize_outcomes = True\n",
    "# params = {'batch_size': batch_size,\n",
    "#           'shuffle': shuffle,\n",
    "#           'num_workers': num_workers}\n",
    "\n",
    "# # Tranforms\n",
    "# trans = transforms.Compose([transforms.RandomAffine(degrees=8, translate=(0.1, 0.1), scale=(0.95,1.25))])\n",
    "\n",
    "# # Generators\n",
    "# training_set = ScanDataset(partition['train_images'], partition['train_reflux'], partition['train_surgery'],\n",
    "#                        partition['train_function'], partition['train_views'],partition['train_scan'], binarize_outcomes, trans, sample_by_scan)\n",
    "# val_set = ScanDataset(partition['val_images'], partition['val_reflux'], partition['val_surgery'],\n",
    "#                        partition['val_function'], partition['val_views'],partition['val_scan'], binarize_outcomes, trans, sample_by_scan, target_view=view_to_use)\n",
    "\n",
    "# training_generator = data.DataLoader(training_set, **params)\n",
    "# val_generator = data.DataLoader(val_set, **params)\n",
    "\n",
    "\n",
    "# dataloaders_dict = {'train': training_generator, 'val': val_generator}\n",
    "\n",
    "\n",
    "# random_str = str(uuid.uuid4()).split(\"-\")[0]\n",
    "\n",
    "# wandb.init(project='hnultra_outcome', entity=wandb_username, name=view_to_use + '_fold_' + fold, group=random_str)\n",
    "\n",
    "\n",
    "\n",
    "# lr = 0.001\n",
    "# wd = 0.001\n",
    "\n",
    "\n",
    "# model_ft, _ = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "# model_ft = model_ft.to(device)\n",
    "# wandb.watch(model_ft)\n",
    "\n",
    "# # print(model)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model_ft.parameters(), lr=lr, weight_decay=wd)\n",
    "# train_model(model_ft, dataloaders_dict, criterion, optimizer, view_to_use, num_epochs=200, is_inception=False, final_testing=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train5fold(view_to_use, network_configs, model_name, lr, wd, amsgrad, feature_extract,layer, i):\n",
    "    random_str = str(uuid.uuid4()).split(\"-\")[0]\n",
    "    binarize_outcomes = True\n",
    "    folds = ['0', '1', '2', '3', '4']\n",
    "    project = 'hnultra_weigh_function_0.25_no_data_augment_min30_epochs'\n",
    "    group = random_str + '_' + view_to_use\n",
    "    \n",
    "    best_metrics_per_fold = []\n",
    "    for fold in folds:\n",
    "\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%d-%m-%Y.%H:%M:%S\")\n",
    "        wandb.init(project=project, entity=wandb_username, name='fold_' + fold, group=group)\n",
    "        partition = all_folds[fold]\n",
    "\n",
    "        model_ft, _ = initialize_model(model_name, num_classes, feature_extract,layer, use_pretrained=True)\n",
    "        model_ft = model_ft.to(device)\n",
    "        wandb.watch(model_ft)\n",
    "\n",
    "        # Gather the parameters to be optimized/updated in this run. If we are\n",
    "        #  finetuning we will be updating all parameters. However, if we are\n",
    "        #  doing feature extract method, we will only update the parameters\n",
    "        #  that we have just initialized, i.e. the parameters with requires_grad\n",
    "        #  is True.\n",
    "        params_to_update = model_ft.parameters()\n",
    "        #print(\"Params to learn:\")\n",
    "        if feature_extract:\n",
    "            params_to_update = []\n",
    "            for name,param in model_ft.named_parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    params_to_update.append(param)\n",
    "#                     print(\"\\t\",name)\n",
    "        else:\n",
    "            for name,param in model_ft.named_parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    pass\n",
    "#                     print(\"\\t\",name)\n",
    "#         print(model_ft)\n",
    "        # Observe that all parameters are being optimized\n",
    "#         optimizer_ft = optim.Adam(params_to_update, lr=lr, weight_decay=wd, amsgrad=amsgrad)\n",
    "\n",
    "        # Setup the loss fxn\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model_ft.parameters(), lr=lr, weight_decay=wd)\n",
    "        \n",
    "        shuffle = True\n",
    "        num_workers = 0\n",
    "        params = {'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                  'num_workers': num_workers}\n",
    "\n",
    "        config_dict = {'i': i, 'batch_size': batch_size, 'shuffle': shuffle, 'num_workers': num_workers, 'fold': int(fold),\n",
    "                       'lr': lr, 'wd': wd, 'amsgrad': amsgrad, 'model_name': model_name, 'num_classes': num_classes, \n",
    "                       'num_epochs': num_epochs, 'feature_extract': feature_extract, \"pretrain\": pretrain, 'view': view_to_use }\n",
    "\n",
    "        wandb.config.update(config_dict)\n",
    "        # Tranforms\n",
    "        # Tranforms\n",
    "        trans = transforms.Compose([transforms.RandomAffine(degrees=8, translate=(0.1, 0.1), scale=(0.95,1.25))])\n",
    "\n",
    "        # Generators\n",
    "        training_set = ScanDataset(partition['train_images'], partition['train_reflux'], partition['train_surgery'],\n",
    "                               partition['train_function'], partition['train_views'],partition['train_scan'], binarize_outcomes, None, sample_by_scan=True, )\n",
    "        val_set = ScanDataset(partition['val_images'], partition['val_reflux'], partition['val_surgery'],\n",
    "                               partition['val_function'], partition['val_views'],partition['val_scan'], binarize_outcomes, None, False, target_view=view_to_use)\n",
    "\n",
    "        training_generator = data.DataLoader(training_set, **params)\n",
    "        val_generator = data.DataLoader(val_set, **params)\n",
    "\n",
    "\n",
    "        dataloaders_dict = {'train': training_generator, 'val': val_generator}\n",
    "\n",
    "        # Train & Evaluate\n",
    "        metrics_from_best_epoch = train_model(model_ft, dataloaders_dict, criterion, optimizer,  view_to_use, num_epochs, is_inception=(model_name==\"inception\"), final_testing = False)\n",
    "        best_metrics_per_fold.append(metrics_from_best_epoch)\n",
    "\n",
    "    # Calculate the performance metrics on the best model in each fold\n",
    "    wandb.init(project=project, entity=wandb_username, name='ALL', group=group)\n",
    "    config_dict['fold'] = -1\n",
    "    wandb.config.update(config_dict)\n",
    "    wandb.config.update(network_configs)\n",
    "\n",
    "\n",
    "    metrics_all = {}\n",
    "    for fold in best_metrics_per_fold:\n",
    "        for key in fold:\n",
    "            if key not in metrics_all:\n",
    "                metrics_all[key] = [fold[key]]\n",
    "            else:\n",
    "                metrics_all[key].append(fold[key]) \n",
    "    # print(metrics_all)\n",
    "\n",
    "    metrics_to_log = {}\n",
    "    for m in metrics_all:\n",
    "        metric_list = np.asarray(metrics_all[m])\n",
    "\n",
    "        metrics_to_log[m + '_mean'] = metric_list.mean()    \n",
    "        metrics_to_log[m + '_stdev'] = metric_list.std()\n",
    "\n",
    "    wandb.config.update(metrics_to_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/aop2jhl0\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/aop2jhl0</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.4573 | train sum of auc:\t1.6371 \n",
      "\n",
      "val loss:\t0.5826 | val sum of auc:\t1.5242 \n",
      "\n",
      "Epoch 2/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3621 | train sum of auc:\t1.9999 \n",
      "\n",
      "val loss:\t0.9012 | val sum of auc:\t1.5674 \n",
      "\n",
      "Epoch 3/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2935 | train sum of auc:\t2.4175 \n",
      "\n",
      "val loss:\t1.1964 | val sum of auc:\t1.4823 \n",
      "\n",
      "Epoch 4/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3010 | train sum of auc:\t2.3544 \n",
      "\n",
      "val loss:\t1.2125 | val sum of auc:\t1.6928 \n",
      "\n",
      "Epoch 5/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2664 | train sum of auc:\t2.5394 \n",
      "\n",
      "val loss:\t0.8461 | val sum of auc:\t1.7977 \n",
      "\n",
      "Epoch 6/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2764 | train sum of auc:\t2.4612 \n",
      "\n",
      "val loss:\t1.1896 | val sum of auc:\t1.4800 \n",
      "\n",
      "Epoch 7/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2165 | train sum of auc:\t2.7028 \n",
      "\n",
      "val loss:\t1.5924 | val sum of auc:\t1.1437 \n",
      "\n",
      "Epoch 8/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1992 | train sum of auc:\t2.7074 \n",
      "\n",
      "val loss:\t0.7229 | val sum of auc:\t1.0668 \n",
      "\n",
      "Epoch 9/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2037 | train sum of auc:\t2.7687 \n",
      "\n",
      "val loss:\t0.5026 | val sum of auc:\t1.3319 \n",
      "\n",
      "Epoch 10/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1792 | train sum of auc:\t2.7991 \n",
      "\n",
      "val loss:\t0.4625 | val sum of auc:\t1.4076 \n",
      "\n",
      "Epoch 11/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1845 | train sum of auc:\t2.7294 \n",
      "\n",
      "val loss:\t0.5120 | val sum of auc:\t1.3902 \n",
      "\n",
      "Epoch 12/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1576 | train sum of auc:\t2.8036 \n",
      "\n",
      "val loss:\t0.6531 | val sum of auc:\t1.3365 \n",
      "\n",
      "Epoch 13/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1619 | train sum of auc:\t2.7930 \n",
      "\n",
      "val loss:\t0.6521 | val sum of auc:\t1.3566 \n",
      "\n",
      "Epoch 14/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1634 | train sum of auc:\t2.8015 \n",
      "\n",
      "val loss:\t0.5692 | val sum of auc:\t1.4496 \n",
      "\n",
      "Epoch 15/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1695 | train sum of auc:\t2.7794 \n",
      "\n",
      "val loss:\t0.6310 | val sum of auc:\t1.4014 \n",
      "\n",
      "Epoch 16/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1367 | train sum of auc:\t2.8798 \n",
      "\n",
      "val loss:\t0.6609 | val sum of auc:\t1.4279 \n",
      "\n",
      "Epoch 17/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1168 | train sum of auc:\t2.9378 \n",
      "\n",
      "val loss:\t0.7050 | val sum of auc:\t1.5820 \n",
      "\n",
      "Epoch 18/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1336 | train sum of auc:\t2.8560 \n",
      "\n",
      "val loss:\t0.7081 | val sum of auc:\t1.6672 \n",
      "\n",
      "Epoch 19/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1260 | train sum of auc:\t2.8736 \n",
      "\n",
      "val loss:\t0.7673 | val sum of auc:\t1.6693 \n",
      "\n",
      "Epoch 20/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1301 | train sum of auc:\t2.8544 \n",
      "\n",
      "val loss:\t0.8855 | val sum of auc:\t1.3485 \n",
      "\n",
      "Epoch 21/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1239 | train sum of auc:\t2.9266 \n",
      "\n",
      "val loss:\t0.8730 | val sum of auc:\t1.1802 \n",
      "\n",
      "Epoch 22/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1209 | train sum of auc:\t2.9406 \n",
      "\n",
      "val loss:\t0.7786 | val sum of auc:\t1.1848 \n",
      "\n",
      "Epoch 23/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1036 | train sum of auc:\t2.9375 \n",
      "\n",
      "val loss:\t0.7628 | val sum of auc:\t1.2139 \n",
      "\n",
      "Epoch 24/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0847 | train sum of auc:\t2.9449 \n",
      "\n",
      "val loss:\t0.7961 | val sum of auc:\t1.2807 \n",
      "\n",
      "Epoch 25/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1128 | train sum of auc:\t2.9386 \n",
      "\n",
      "val loss:\t0.8012 | val sum of auc:\t1.3199 \n",
      "\n",
      "Epoch 26/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0751 | train sum of auc:\t2.9818 \n",
      "\n",
      "val loss:\t0.8343 | val sum of auc:\t1.3537 \n",
      "\n",
      "Epoch 27/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1269 | train sum of auc:\t2.9210 \n",
      "\n",
      "val loss:\t0.9371 | val sum of auc:\t1.4084 \n",
      "\n",
      "Epoch 28/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0908 | train sum of auc:\t2.9680 \n",
      "\n",
      "val loss:\t1.2963 | val sum of auc:\t1.4735 \n",
      "\n",
      "Epoch 29/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0621 | train sum of auc:\t2.9865 \n",
      "\n",
      "val loss:\t1.7754 | val sum of auc:\t1.4821 \n",
      "\n",
      "Epoch 30/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0748 | train sum of auc:\t2.9647 \n",
      "\n",
      "val loss:\t1.7562 | val sum of auc:\t1.4034 \n",
      "\n",
      "Epoch 31/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0820 | train sum of auc:\t2.9650 \n",
      "\n",
      "{'train_loss': 0.08198419777566895, 'train_Reflux_auroc': 0.9787835926449788, 'train_Reflux_auprc': 0.9538226765886917, 'train_sum_auroc': 2.9650428124085972, 'train_sum_auprc': 2.9155368462574067, 'train_Surgery_auroc': 0.9986559139784946, 'train_Surgery_auprc': 0.9866452991452993, 'train_Function_auroc': 0.987603305785124, 'train_Function_auprc': 0.9750688705234161, 'val_loss': 1.3615657366239107, 'val_Reflux_auroc': 0.5886120339608711, 'val_Reflux_auprc': 0.6371094776062926, 'val_sum_auroc': 1.3791686157661325, 'val_sum_auprc': 0.9800855548725188, 'val_Surgery_auroc': 0.2926574221413958, 'val_Surgery_auprc': 0.07581731770887393, 'val_Function_auroc': 0.49789915966386555, 'val_Function_auprc': 0.26715875955735235, 'best_epoch': 30}\n",
      "val loss:\t1.3616 | val sum of auc:\t1.3792 \n",
      "\n",
      "Epoch 32/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0813 | train sum of auc:\t2.9761 \n",
      "\n",
      "{'train_loss': 0.08130669736248605, 'train_Reflux_auroc': 0.9816124469589816, 'train_Reflux_auprc': 0.8907669896827215, 'train_sum_auroc': 2.9761361295325175, 'train_sum_auprc': 2.871286470202202, 'train_Surgery_auroc': 0.9986559139784946, 'train_Surgery_auprc': 0.9880952380952382, 'train_Function_auroc': 0.9958677685950413, 'train_Function_auprc': 0.9924242424242424, 'val_loss': 1.0749446000808325, 'val_Reflux_auroc': 0.5901439645625692, 'val_Reflux_auprc': 0.641337402764289, 'val_sum_auroc': 1.4028673967406236, 'val_sum_auprc': 1.0058487573117565, 'val_Surgery_auroc': 0.2990679699931803, 'val_Surgery_auprc': 0.07647104898319043, 'val_Function_auroc': 0.5136554621848739, 'val_Function_auprc': 0.288040305564277, 'best_epoch': 31}\n",
      "val loss:\t1.0749 | val sum of auc:\t1.4029 \n",
      "\n",
      "Epoch 33/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0622 | train sum of auc:\t2.9521 \n",
      "\n",
      "{'train_loss': 0.062232580255059636, 'train_Reflux_auroc': 0.9892503536067893, 'train_Reflux_auprc': 0.9851272015655578, 'train_sum_auroc': 2.9520602709621615, 'train_sum_auprc': 2.9442181106564673, 'train_Surgery_auroc': 1.0, 'train_Surgery_auprc': 1.0, 'train_Function_auroc': 0.9628099173553719, 'train_Function_auprc': 0.9590909090909092, 'val_loss': 0.9659723985908378, 'val_Reflux_auroc': 0.5979881875230713, 'val_Reflux_auprc': 0.6565146298068874, 'val_sum_auroc': 1.357144202736613, 'val_sum_auprc': 0.9643719549598417, 'val_Surgery_auroc': 0.32848374630597865, 'val_Surgery_auprc': 0.07938714195230455, 'val_Function_auroc': 0.43067226890756305, 'val_Function_auprc': 0.22847018320064985, 'best_epoch': 32}\n",
      "val loss:\t0.9660 | val sum of auc:\t1.3571 \n",
      "\n",
      "Epoch 34/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0901 | train sum of auc:\t2.9370 \n",
      "\n",
      "{'train_loss': 0.09014151486403801, 'train_Reflux_auroc': 0.986987270155587, 'train_Reflux_auprc': 0.9710877239519131, 'train_sum_auroc': 2.9369561673385602, 'train_sum_auprc': 2.8231349822491714, 'train_Surgery_auroc': 0.9623655913978495, 'train_Surgery_auprc': 0.871527777777778, 'train_Function_auroc': 0.987603305785124, 'train_Function_auprc': 0.9805194805194806, 'val_loss': 0.9040057480844677, 'val_Reflux_auroc': 0.572905131044666, 'val_Reflux_auprc': 0.6403912126605504, 'val_sum_auroc': 1.3183289382124526, 'val_sum_auprc': 0.9623472217554678, 'val_Surgery_auroc': 0.3252557399408957, 'val_Surgery_auprc': 0.07914894828515995, 'val_Function_auroc': 0.4201680672268908, 'val_Function_auprc': 0.2428070608097574, 'best_epoch': 33}\n",
      "val loss:\t0.9040 | val sum of auc:\t1.3183 \n",
      "\n",
      "Epoch 35/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.0598 | train sum of auc:\t2.9793 \n",
      "\n",
      "{'train_loss': 0.059816557722275746, 'train_Reflux_auroc': 0.9929278642149929, 'train_Reflux_auprc': 0.9840271120103052, 'train_sum_auroc': 2.9792870573190537, 'train_sum_auprc': 2.936707764690958, 'train_Surgery_auroc': 0.9946236559139785, 'train_Surgery_auprc': 0.9666666666666667, 'train_Function_auroc': 0.9917355371900827, 'train_Function_auprc': 0.986013986013986, 'val_loss': 0.8660617944521781, 'val_Reflux_auroc': 0.544499815430048, 'val_Reflux_auprc': 0.607958628016991, 'val_sum_auroc': 1.2984778967739745, 'val_sum_auprc': 0.982764545638265, 'val_Surgery_auroc': 0.24137303932711984, 'val_Surgery_auprc': 0.07132219672785152, 'val_Function_auroc': 0.5126050420168067, 'val_Function_auprc': 0.30348372089342246, 'best_epoch': 34}\n",
      "val loss:\t0.8661 | val sum of auc:\t1.2985 \n",
      "\n",
      "Epoch 36/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0502 | train sum of auc:\t2.9884 \n",
      "\n",
      "val loss:\t0.9693 | val sum of auc:\t1.3254 \n",
      "\n",
      "Epoch 37/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0785 | train sum of auc:\t2.9749 \n",
      "\n",
      "val loss:\t0.9062 | val sum of auc:\t1.3477 \n",
      "\n",
      "Epoch 38/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0671 | train sum of auc:\t2.9769 \n",
      "\n",
      "val loss:\t0.9723 | val sum of auc:\t1.4794 \n",
      "\n",
      "Epoch 39/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0761 | train sum of auc:\t2.9697 \n",
      "\n",
      "val loss:\t1.0017 | val sum of auc:\t1.5731 \n",
      "\n",
      "Epoch 40/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0735 | train sum of auc:\t2.9690 \n",
      "\n",
      "val loss:\t1.0444 | val sum of auc:\t1.5408 \n",
      "\n",
      "Epoch 41/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0549 | train sum of auc:\t2.9926 \n",
      "\n",
      "val loss:\t1.1405 | val sum of auc:\t1.4979 \n",
      "\n",
      "Epoch 42/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0606 | train sum of auc:\t2.9862 \n",
      "\n",
      "val loss:\t1.0849 | val sum of auc:\t1.4675 \n",
      "\n",
      "Epoch 43/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0698 | train sum of auc:\t2.9677 \n",
      "\n",
      "val loss:\t1.0251 | val sum of auc:\t1.3358 \n",
      "\n",
      "Epoch 44/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0562 | train sum of auc:\t2.9850 \n",
      "\n",
      "val loss:\t1.0662 | val sum of auc:\t1.2935 \n",
      "\n",
      "Epoch 45/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0518 | train sum of auc:\t2.9873 \n",
      "\n",
      "val loss:\t1.1045 | val sum of auc:\t1.3587 \n",
      "\n",
      "Epoch 46/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0660 | train sum of auc:\t2.9859 \n",
      "\n",
      "val loss:\t1.0913 | val sum of auc:\t1.4384 \n",
      "\n",
      "Epoch 47/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0803 | train sum of auc:\t2.9671 \n",
      "\n",
      "val loss:\t0.9685 | val sum of auc:\t1.6216 \n",
      "\n",
      "Epoch 48/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0663 | train sum of auc:\t2.9828 \n",
      "\n",
      "{'train_loss': 0.0663350891760167, 'train_Reflux_auroc': 0.9841584158415841, 'train_Reflux_auprc': 0.9655391641884776, 'train_sum_auroc': 2.9828143298200787, 'train_sum_auprc': 2.952184463333777, 'train_Surgery_auroc': 0.9986559139784946, 'train_Surgery_auprc': 0.9866452991452993, 'train_Function_auroc': 1.0, 'train_Function_auprc': 1.0, 'val_loss': 0.8597303267218109, 'val_Reflux_auroc': 0.594905869324474, 'val_Reflux_auprc': 0.6345964607450312, 'val_sum_auroc': 1.756815136327479, 'val_sum_auprc': 1.1000098053953975, 'val_Surgery_auroc': 0.4528756535576267, 'val_Surgery_auprc': 0.10214835400791424, 'val_Function_auroc': 0.7090336134453782, 'val_Function_auprc': 0.3632649906424522, 'best_epoch': 47}\n",
      "val loss:\t0.8597 | val sum of auc:\t1.7568 \n",
      "\n",
      "Epoch 49/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0325 | train sum of auc:\t2.9977 \n",
      "\n",
      "{'train_loss': 0.032519381602897364, 'train_Reflux_auroc': 0.9997171145685997, 'train_Reflux_auprc': 0.9992063492063492, 'train_sum_auroc': 2.9977009855363415, 'train_sum_auprc': 2.9825396825396826, 'train_Surgery_auroc': 0.997983870967742, 'train_Surgery_auprc': 0.9833333333333335, 'train_Function_auroc': 1.0, 'train_Function_auprc': 1.0, 'val_loss': 0.8055060108502706, 'val_Reflux_auroc': 0.5769102990033222, 'val_Reflux_auprc': 0.612554764058417, 'val_sum_auroc': 1.6581492551449968, 'val_sum_auprc': 1.0336878404434267, 'val_Surgery_auroc': 0.43628097294839735, 'val_Surgery_auprc': 0.09799182997349097, 'val_Function_auroc': 0.6449579831932772, 'val_Function_auprc': 0.3231412464115186, 'best_epoch': 48}\n",
      "val loss:\t0.8055 | val sum of auc:\t1.6581 \n",
      "\n",
      "Epoch 50/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0464 | train sum of auc:\t2.9923 \n",
      "\n",
      "val loss:\t0.9967 | val sum of auc:\t1.4633 \n",
      "\n",
      "Epoch 51/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0483 | train sum of auc:\t2.9924 \n",
      "\n",
      "val loss:\t1.1306 | val sum of auc:\t1.3940 \n",
      "\n",
      "Epoch 52/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0646 | train sum of auc:\t2.9761 \n",
      "\n",
      "val loss:\t1.0607 | val sum of auc:\t1.4047 \n",
      "\n",
      "Epoch 53/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0687 | train sum of auc:\t2.9861 \n",
      "\n",
      "val loss:\t0.9598 | val sum of auc:\t1.4694 \n",
      "\n",
      "Epoch 54/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0544 | train sum of auc:\t2.9885 \n",
      "\n",
      "val loss:\t0.9128 | val sum of auc:\t1.5132 \n",
      "\n",
      "Epoch 55/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0288 | train sum of auc:\t2.9993 \n",
      "\n",
      "val loss:\t0.9558 | val sum of auc:\t1.5216 \n",
      "\n",
      "Epoch 56/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0503 | train sum of auc:\t2.9907 \n",
      "\n",
      "val loss:\t1.0887 | val sum of auc:\t1.5070 \n",
      "\n",
      "Epoch 57/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0464 | train sum of auc:\t2.9873 \n",
      "\n",
      "val loss:\t1.5477 | val sum of auc:\t1.4547 \n",
      "\n",
      "Epoch 58/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0280 | train sum of auc:\t2.9956 \n",
      "\n",
      "val loss:\t2.2456 | val sum of auc:\t1.3771 \n",
      "\n",
      "Epoch 59/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0335 | train sum of auc:\t2.9977 \n",
      "\n",
      "val loss:\t2.3500 | val sum of auc:\t1.3849 \n",
      "\n",
      "Epoch 60/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0404 | train sum of auc:\t2.9921 \n",
      "\n",
      "val loss:\t2.0351 | val sum of auc:\t1.3959 \n",
      "\n",
      "Epoch 61/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0147 | train sum of auc:\t3.0000 \n",
      "\n",
      "val loss:\t1.6984 | val sum of auc:\t1.4059 \n",
      "\n",
      "Epoch 62/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0332 | train sum of auc:\t2.9978 \n",
      "\n",
      "val loss:\t1.4486 | val sum of auc:\t1.4399 \n",
      "\n",
      "Epoch 63/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0462 | train sum of auc:\t2.9941 \n",
      "\n",
      "val loss:\t1.2788 | val sum of auc:\t1.4305 \n",
      "\n",
      "Epoch 64/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0221 | train sum of auc:\t2.9983 \n",
      "\n",
      "val loss:\t1.3100 | val sum of auc:\t1.4713 \n",
      "\n",
      "Epoch 65/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0175 | train sum of auc:\t3.0000 \n",
      "\n",
      "val loss:\t1.2938 | val sum of auc:\t1.4993 \n",
      "\n",
      "Epoch 66/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0503 | train sum of auc:\t2.9898 \n",
      "\n",
      "val loss:\t1.2321 | val sum of auc:\t1.4255 \n",
      "\n",
      "Epoch 67/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0185 | train sum of auc:\t2.9917 \n",
      "\n",
      "val loss:\t1.2426 | val sum of auc:\t1.3978 \n",
      "\n",
      "Epoch 68/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0647 | train sum of auc:\t2.9803 \n",
      "\n",
      "val loss:\t1.3370 | val sum of auc:\t1.4848 \n",
      "\n",
      "Epoch 69/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0283 | train sum of auc:\t2.9972 \n",
      "\n",
      "val loss:\t1.3132 | val sum of auc:\t1.5521 \n",
      "\n",
      "EARLY STOPPING 68\n",
      "Epoch 70/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/tr8t2now\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/tr8t2now</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.6084 | train sum of auc:\t1.2523 \n",
      "\n",
      "val loss:\t0.4172 | val sum of auc:\t1.8489 \n",
      "\n",
      "Epoch 2/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.4141 | train sum of auc:\t2.1070 \n",
      "\n",
      "val loss:\t0.2992 | val sum of auc:\t2.0881 \n",
      "\n",
      "Epoch 3/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3736 | train sum of auc:\t2.1386 \n",
      "\n",
      "val loss:\t0.4066 | val sum of auc:\t1.9353 \n",
      "\n",
      "Epoch 4/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2875 | train sum of auc:\t2.3973 \n",
      "\n",
      "val loss:\t0.4570 | val sum of auc:\t1.9181 \n",
      "\n",
      "Epoch 5/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2938 | train sum of auc:\t2.4088 \n",
      "\n",
      "val loss:\t0.3794 | val sum of auc:\t2.0311 \n",
      "\n",
      "Epoch 6/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2835 | train sum of auc:\t2.4716 \n",
      "\n",
      "val loss:\t0.3835 | val sum of auc:\t2.1129 \n",
      "\n",
      "Epoch 7/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2504 | train sum of auc:\t2.5913 \n",
      "\n",
      "val loss:\t0.8724 | val sum of auc:\t2.1939 \n",
      "\n",
      "Epoch 8/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2859 | train sum of auc:\t2.4473 \n",
      "\n",
      "val loss:\t0.8657 | val sum of auc:\t2.1833 \n",
      "\n",
      "Epoch 9/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2687 | train sum of auc:\t2.5506 \n",
      "\n",
      "val loss:\t0.7984 | val sum of auc:\t2.0833 \n",
      "\n",
      "Epoch 10/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2181 | train sum of auc:\t2.7019 \n",
      "\n",
      "val loss:\t0.4346 | val sum of auc:\t2.0602 \n",
      "\n",
      "Epoch 11/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1912 | train sum of auc:\t2.8083 \n",
      "\n",
      "val loss:\t0.4144 | val sum of auc:\t2.0245 \n",
      "\n",
      "Epoch 12/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2137 | train sum of auc:\t2.6873 \n",
      "\n",
      "val loss:\t0.4839 | val sum of auc:\t1.9849 \n",
      "\n",
      "Epoch 13/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1902 | train sum of auc:\t2.7330 \n",
      "\n",
      "val loss:\t0.4873 | val sum of auc:\t1.9832 \n",
      "\n",
      "Epoch 14/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1663 | train sum of auc:\t2.7765 \n",
      "\n",
      "val loss:\t0.6007 | val sum of auc:\t1.8923 \n",
      "\n",
      "Epoch 15/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1559 | train sum of auc:\t2.9032 \n",
      "\n",
      "val loss:\t0.8184 | val sum of auc:\t1.7742 \n",
      "\n",
      "Epoch 16/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1323 | train sum of auc:\t2.9078 \n",
      "\n",
      "val loss:\t0.9725 | val sum of auc:\t1.8200 \n",
      "\n",
      "Epoch 17/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1398 | train sum of auc:\t2.8941 \n",
      "\n",
      "val loss:\t0.7098 | val sum of auc:\t1.9437 \n",
      "\n",
      "Epoch 18/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1513 | train sum of auc:\t2.8743 \n",
      "\n",
      "val loss:\t0.4833 | val sum of auc:\t1.9414 \n",
      "\n",
      "Epoch 19/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1089 | train sum of auc:\t2.9246 \n",
      "\n",
      "val loss:\t0.4458 | val sum of auc:\t1.8859 \n",
      "\n",
      "Epoch 20/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1177 | train sum of auc:\t2.9521 \n",
      "\n",
      "val loss:\t0.4470 | val sum of auc:\t1.8723 \n",
      "\n",
      "Epoch 21/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1562 | train sum of auc:\t2.8208 \n",
      "\n",
      "val loss:\t0.4354 | val sum of auc:\t1.7333 \n",
      "\n",
      "Epoch 22/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1185 | train sum of auc:\t2.9399 \n",
      "\n",
      "val loss:\t0.5733 | val sum of auc:\t1.7034 \n",
      "\n",
      "Epoch 23/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1702 | train sum of auc:\t2.8294 \n",
      "\n",
      "val loss:\t0.9855 | val sum of auc:\t1.7014 \n",
      "\n",
      "Epoch 24/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1364 | train sum of auc:\t2.9096 \n",
      "\n",
      "val loss:\t0.8225 | val sum of auc:\t1.9079 \n",
      "\n",
      "Epoch 25/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1225 | train sum of auc:\t2.8606 \n",
      "\n",
      "val loss:\t0.6477 | val sum of auc:\t2.0616 \n",
      "\n",
      "Epoch 26/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0993 | train sum of auc:\t2.9652 \n",
      "\n",
      "val loss:\t0.4430 | val sum of auc:\t2.1063 \n",
      "\n",
      "Epoch 27/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0970 | train sum of auc:\t2.9652 \n",
      "\n",
      "val loss:\t0.4290 | val sum of auc:\t1.9905 \n",
      "\n",
      "Epoch 28/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1094 | train sum of auc:\t2.9324 \n",
      "\n",
      "val loss:\t0.4783 | val sum of auc:\t1.9599 \n",
      "\n",
      "Epoch 29/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1004 | train sum of auc:\t2.9531 \n",
      "\n",
      "val loss:\t0.4613 | val sum of auc:\t1.9615 \n",
      "\n",
      "Epoch 30/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1041 | train sum of auc:\t2.9565 \n",
      "\n",
      "val loss:\t0.4529 | val sum of auc:\t1.8880 \n",
      "\n",
      "Epoch 31/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1270 | train sum of auc:\t2.8975 \n",
      "\n",
      "{'train_loss': 0.12697968871271523, 'train_Reflux_auroc': 0.9537357743691243, 'train_Reflux_auprc': 0.9382929210865397, 'train_sum_auroc': 2.897458718092068, 'train_sum_auprc': 2.786063628420089, 'train_Surgery_auroc': 0.9595959595959596, 'train_Surgery_auprc': 0.8803184065479148, 'train_Function_auroc': 0.9841269841269842, 'train_Function_auprc': 0.9674523007856342, 'val_loss': 0.5099546034951826, 'val_Reflux_auroc': 0.5398391812865497, 'val_Reflux_auprc': 0.15353323211446612, 'val_sum_auroc': 1.8352010985334508, 'val_sum_auprc': 0.9929495796249834, 'val_Surgery_auroc': 0.5730031948881789, 'val_Surgery_auprc': 0.0866062220233037, 'val_Function_auroc': 0.7223587223587223, 'val_Function_auprc': 0.7528101254872136, 'best_epoch': 30}\n",
      "val loss:\t0.5100 | val sum of auc:\t1.8352 \n",
      "\n",
      "Epoch 32/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1651 | train sum of auc:\t2.8536 \n",
      "\n",
      "{'train_loss': 0.16506778312860615, 'train_Reflux_auroc': 0.9317169717961404, 'train_Reflux_auprc': 0.9015368655353917, 'train_sum_auroc': 2.853554393633562, 'train_sum_auprc': 2.6263663463993554, 'train_Surgery_auroc': 0.9588744588744589, 'train_Surgery_auprc': 0.789320545355028, 'train_Function_auroc': 0.962962962962963, 'train_Function_auprc': 0.9355089355089357, 'val_loss': 0.4600698103954842, 'val_Reflux_auroc': 0.5544590643274854, 'val_Reflux_auprc': 0.17794947952344528, 'val_sum_auroc': 1.967930842815238, 'val_sum_auprc': 0.9822382552534812, 'val_Surgery_auroc': 0.7468051118210862, 'val_Surgery_auprc': 0.17656363737328215, 'val_Function_auroc': 0.6666666666666666, 'val_Function_auprc': 0.6277251383567537, 'best_epoch': 31}\n",
      "val loss:\t0.4601 | val sum of auc:\t1.9679 \n",
      "\n",
      "Epoch 33/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0929 | train sum of auc:\t2.9490 \n",
      "\n",
      "val loss:\t0.4820 | val sum of auc:\t2.0552 \n",
      "\n",
      "Epoch 34/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1119 | train sum of auc:\t2.9444 \n",
      "\n",
      "val loss:\t0.4909 | val sum of auc:\t1.9574 \n",
      "\n",
      "Epoch 35/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0948 | train sum of auc:\t2.9651 \n",
      "\n",
      "val loss:\t0.9283 | val sum of auc:\t1.5318 \n",
      "\n",
      "Epoch 36/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1336 | train sum of auc:\t2.8692 \n",
      "\n",
      "val loss:\t0.8402 | val sum of auc:\t1.6127 \n",
      "\n",
      "Epoch 37/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1108 | train sum of auc:\t2.8690 \n",
      "\n",
      "val loss:\t0.7048 | val sum of auc:\t1.5614 \n",
      "\n",
      "Epoch 38/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0863 | train sum of auc:\t2.9518 \n",
      "\n",
      "val loss:\t0.6474 | val sum of auc:\t1.6254 \n",
      "\n",
      "Epoch 39/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0769 | train sum of auc:\t2.9817 \n",
      "\n",
      "val loss:\t0.5943 | val sum of auc:\t1.8269 \n",
      "\n",
      "Epoch 40/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0683 | train sum of auc:\t2.9816 \n",
      "\n",
      "val loss:\t0.6328 | val sum of auc:\t1.9123 \n",
      "\n",
      "Epoch 41/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0679 | train sum of auc:\t2.9819 \n",
      "\n",
      "val loss:\t0.7044 | val sum of auc:\t1.8563 \n",
      "\n",
      "Epoch 42/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0520 | train sum of auc:\t2.9931 \n",
      "\n",
      "val loss:\t0.8120 | val sum of auc:\t1.7142 \n",
      "\n",
      "Epoch 43/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.0617 | train sum of auc:\t2.9668 \n",
      "\n",
      "val loss:\t0.7844 | val sum of auc:\t1.5815 \n",
      "\n",
      "Epoch 44/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0523 | train sum of auc:\t2.9901 \n",
      "\n",
      "val loss:\t0.6273 | val sum of auc:\t1.6151 \n",
      "\n",
      "Epoch 45/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0734 | train sum of auc:\t2.9683 \n",
      "\n",
      "val loss:\t0.5526 | val sum of auc:\t1.7126 \n",
      "\n",
      "Epoch 46/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0757 | train sum of auc:\t2.9718 \n",
      "\n",
      "val loss:\t0.6950 | val sum of auc:\t1.7734 \n",
      "\n",
      "Epoch 47/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0617 | train sum of auc:\t2.9849 \n",
      "\n",
      "val loss:\t0.9842 | val sum of auc:\t1.8465 \n",
      "\n",
      "Epoch 48/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0587 | train sum of auc:\t2.9545 \n",
      "\n",
      "val loss:\t1.2063 | val sum of auc:\t1.9249 \n",
      "\n",
      "Epoch 49/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0518 | train sum of auc:\t2.9919 \n",
      "\n",
      "val loss:\t1.1076 | val sum of auc:\t1.9468 \n",
      "\n",
      "Epoch 50/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0556 | train sum of auc:\t2.9608 \n",
      "\n",
      "val loss:\t0.8768 | val sum of auc:\t1.8793 \n",
      "\n",
      "Epoch 51/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0538 | train sum of auc:\t2.9901 \n",
      "\n",
      "val loss:\t0.6225 | val sum of auc:\t1.9249 \n",
      "\n",
      "Epoch 52/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0426 | train sum of auc:\t2.9958 \n",
      "\n",
      "val loss:\t0.4797 | val sum of auc:\t2.0076 \n",
      "\n",
      "EARLY STOPPING 51\n",
      "Epoch 53/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/22llj6jv\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/22llj6jv</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.4935 | train sum of auc:\t1.3799 \n",
      "\n",
      "val loss:\t0.5648 | val sum of auc:\t1.5135 \n",
      "\n",
      "Epoch 2/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3967 | train sum of auc:\t1.8529 \n",
      "\n",
      "val loss:\t0.6766 | val sum of auc:\t1.2025 \n",
      "\n",
      "Epoch 3/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3574 | train sum of auc:\t2.0844 \n",
      "\n",
      "val loss:\t0.9128 | val sum of auc:\t1.7978 \n",
      "\n",
      "Epoch 4/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3790 | train sum of auc:\t2.2150 \n",
      "\n",
      "val loss:\t0.9994 | val sum of auc:\t1.6244 \n",
      "\n",
      "Epoch 5/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2965 | train sum of auc:\t2.5796 \n",
      "\n",
      "val loss:\t0.5859 | val sum of auc:\t1.7002 \n",
      "\n",
      "Epoch 6/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2992 | train sum of auc:\t2.5714 \n",
      "\n",
      "val loss:\t0.5553 | val sum of auc:\t1.9018 \n",
      "\n",
      "Epoch 7/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2763 | train sum of auc:\t2.6183 \n",
      "\n",
      "val loss:\t0.5243 | val sum of auc:\t1.9584 \n",
      "\n",
      "Epoch 8/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2326 | train sum of auc:\t2.7118 \n",
      "\n",
      "val loss:\t0.7119 | val sum of auc:\t1.9096 \n",
      "\n",
      "Epoch 9/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2724 | train sum of auc:\t2.5877 \n",
      "\n",
      "val loss:\t0.6526 | val sum of auc:\t2.0335 \n",
      "\n",
      "Epoch 10/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2462 | train sum of auc:\t2.7454 \n",
      "\n",
      "val loss:\t0.6089 | val sum of auc:\t1.9893 \n",
      "\n",
      "Epoch 11/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2425 | train sum of auc:\t2.6315 \n",
      "\n",
      "val loss:\t0.9766 | val sum of auc:\t1.9315 \n",
      "\n",
      "Epoch 12/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2069 | train sum of auc:\t2.8011 \n",
      "\n",
      "val loss:\t1.3416 | val sum of auc:\t1.9780 \n",
      "\n",
      "Epoch 13/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2085 | train sum of auc:\t2.7252 \n",
      "\n",
      "val loss:\t1.0800 | val sum of auc:\t2.0160 \n",
      "\n",
      "Epoch 14/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1631 | train sum of auc:\t2.9008 \n",
      "\n",
      "val loss:\t0.6643 | val sum of auc:\t1.9752 \n",
      "\n",
      "Epoch 15/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1949 | train sum of auc:\t2.8171 \n",
      "\n",
      "val loss:\t0.5916 | val sum of auc:\t1.9372 \n",
      "\n",
      "Epoch 16/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1496 | train sum of auc:\t2.9001 \n",
      "\n",
      "val loss:\t0.6502 | val sum of auc:\t1.9080 \n",
      "\n",
      "Epoch 17/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1931 | train sum of auc:\t2.7990 \n",
      "\n",
      "val loss:\t0.7127 | val sum of auc:\t1.7601 \n",
      "\n",
      "Epoch 18/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1784 | train sum of auc:\t2.8502 \n",
      "\n",
      "val loss:\t0.6865 | val sum of auc:\t1.7326 \n",
      "\n",
      "Epoch 19/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1579 | train sum of auc:\t2.8558 \n",
      "\n",
      "val loss:\t0.6153 | val sum of auc:\t1.8457 \n",
      "\n",
      "Epoch 20/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1347 | train sum of auc:\t2.9259 \n",
      "\n",
      "val loss:\t0.5848 | val sum of auc:\t1.7262 \n",
      "\n",
      "Epoch 21/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1780 | train sum of auc:\t2.8528 \n",
      "\n",
      "val loss:\t0.6375 | val sum of auc:\t1.5781 \n",
      "\n",
      "Epoch 22/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1570 | train sum of auc:\t2.8837 \n",
      "\n",
      "val loss:\t0.8768 | val sum of auc:\t1.4038 \n",
      "\n",
      "Epoch 23/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1514 | train sum of auc:\t2.9179 \n",
      "\n",
      "val loss:\t0.8367 | val sum of auc:\t1.3840 \n",
      "\n",
      "Epoch 24/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0913 | train sum of auc:\t2.9777 \n",
      "\n",
      "val loss:\t0.7916 | val sum of auc:\t1.4684 \n",
      "\n",
      "Epoch 25/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0934 | train sum of auc:\t2.9716 \n",
      "\n",
      "val loss:\t0.8093 | val sum of auc:\t1.6134 \n",
      "\n",
      "Epoch 26/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1075 | train sum of auc:\t2.9326 \n",
      "\n",
      "val loss:\t0.8298 | val sum of auc:\t1.5995 \n",
      "\n",
      "Epoch 27/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1145 | train sum of auc:\t2.9265 \n",
      "\n",
      "val loss:\t0.8345 | val sum of auc:\t1.4878 \n",
      "\n",
      "Epoch 28/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1095 | train sum of auc:\t2.9622 \n",
      "\n",
      "val loss:\t0.7519 | val sum of auc:\t1.3628 \n",
      "\n",
      "Epoch 29/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0853 | train sum of auc:\t2.9665 \n",
      "\n",
      "val loss:\t0.7143 | val sum of auc:\t1.3013 \n",
      "\n",
      "Epoch 30/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1121 | train sum of auc:\t2.9387 \n",
      "\n",
      "val loss:\t0.6796 | val sum of auc:\t1.2596 \n",
      "\n",
      "Epoch 31/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0743 | train sum of auc:\t2.9802 \n",
      "\n",
      "{'train_loss': 0.07431469418640647, 'train_Reflux_auroc': 0.9812673450508789, 'train_Reflux_auprc': 0.9566254422459077, 'train_sum_auroc': 2.9802006783842123, 'train_sum_auprc': 2.948782304991006, 'train_Surgery_auroc': 0.9989333333333333, 'train_Surgery_auprc': 0.992156862745098, 'train_Function_auroc': 1.0, 'train_Function_auprc': 1.0, 'val_loss': 0.6874411225318908, 'val_Reflux_auroc': 0.4831656346749226, 'val_Reflux_auprc': 0.5624653952004057, 'val_sum_auroc': 1.202795264304552, 'val_sum_auprc': 1.1597494725009196, 'val_Surgery_auroc': 0.4233333333333333, 'val_Surgery_auprc': 0.04336496869522462, 'val_Function_auroc': 0.2962962962962963, 'val_Function_auprc': 0.5539191086052893, 'best_epoch': 30}\n",
      "val loss:\t0.6874 | val sum of auc:\t1.2028 \n",
      "\n",
      "Epoch 32/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0722 | train sum of auc:\t2.9873 \n",
      "\n",
      "val loss:\t0.6987 | val sum of auc:\t1.2729 \n",
      "\n",
      "Epoch 33/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0827 | train sum of auc:\t2.9668 \n",
      "\n",
      "{'train_loss': 0.08273996604340417, 'train_Reflux_auroc': 0.9780296022201665, 'train_Reflux_auprc': 0.9411610974827248, 'train_sum_auroc': 2.966829602220167, 'train_sum_auprc': 2.902272208593836, 'train_Surgery_auroc': 0.9888, 'train_Surgery_auprc': 0.961111111111111, 'train_Function_auroc': 1.0, 'train_Function_auprc': 1.0, 'val_loss': 0.6767288446426392, 'val_Reflux_auroc': 0.5457946336429309, 'val_Reflux_auprc': 0.612824426779301, 'val_sum_auroc': 1.3438810533960175, 'val_sum_auprc': 1.3477380825210592, 'val_Surgery_auroc': 0.5116666666666667, 'val_Surgery_auprc': 0.18531546099657614, 'val_Function_auroc': 0.28641975308641976, 'val_Function_auprc': 0.549598194745182, 'best_epoch': 32}\n",
      "val loss:\t0.6767 | val sum of auc:\t1.3439 \n",
      "\n",
      "Epoch 34/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0828 | train sum of auc:\t2.9733 \n",
      "\n",
      "{'train_loss': 0.08277301357260772, 'train_Reflux_auroc': 0.9877428307123035, 'train_Reflux_auprc': 0.9746933268407633, 'train_sum_auroc': 2.9733428307123035, 'train_sum_auprc': 2.8812253133727497, 'train_Surgery_auroc': 0.9856, 'train_Surgery_auprc': 0.9065319865319865, 'train_Function_auroc': 1.0, 'train_Function_auprc': 1.0, 'val_loss': 0.66660475730896, 'val_Reflux_auroc': 0.5856553147574819, 'val_Reflux_auprc': 0.5960107895137365, 'val_sum_auroc': 1.429945438214272, 'val_sum_auprc': 1.4324804028053677, 'val_Surgery_auroc': 0.5875, 'val_Surgery_auprc': 0.27767649791808546, 'val_Function_auroc': 0.2567901234567901, 'val_Function_auprc': 0.5587931153735457, 'best_epoch': 33}\n",
      "val loss:\t0.6666 | val sum of auc:\t1.4299 \n",
      "\n",
      "Epoch 35/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0781 | train sum of auc:\t2.9752 \n",
      "\n",
      "val loss:\t0.6979 | val sum of auc:\t1.4312 \n",
      "\n",
      "Epoch 36/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0707 | train sum of auc:\t2.9788 \n",
      "\n",
      "val loss:\t0.9679 | val sum of auc:\t1.3708 \n",
      "\n",
      "Epoch 37/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0894 | train sum of auc:\t2.9634 \n",
      "\n",
      "val loss:\t0.9912 | val sum of auc:\t1.4309 \n",
      "\n",
      "Epoch 38/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0715 | train sum of auc:\t2.9827 \n",
      "\n",
      "val loss:\t0.7963 | val sum of auc:\t1.5349 \n",
      "\n",
      "Epoch 39/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.0669 | train sum of auc:\t2.9759 \n",
      "\n",
      "val loss:\t0.7742 | val sum of auc:\t1.5526 \n",
      "\n",
      "Epoch 40/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1138 | train sum of auc:\t2.9361 \n",
      "\n",
      "val loss:\t0.7034 | val sum of auc:\t1.5388 \n",
      "\n",
      "Epoch 41/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0651 | train sum of auc:\t2.9739 \n",
      "\n",
      "val loss:\t0.7086 | val sum of auc:\t1.4902 \n",
      "\n",
      "Epoch 42/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0402 | train sum of auc:\t2.9932 \n",
      "\n",
      "{'train_loss': 0.04019040412136487, 'train_Reflux_auroc': 0.9983811285846439, 'train_Reflux_auprc': 0.9968111168394722, 'train_sum_auroc': 2.993218165621681, 'train_sum_auprc': 2.9815333390616945, 'train_Surgery_auroc': 0.9994666666666666, 'train_Surgery_auprc': 0.9958333333333333, 'train_Function_auroc': 0.9953703703703705, 'train_Function_auprc': 0.9888888888888889, 'val_loss': 0.6628685235977173, 'val_Reflux_auroc': 0.6591847265221877, 'val_Reflux_auprc': 0.680141286266667, 'val_sum_auroc': 1.5155736154110768, 'val_sum_auprc': 1.3861303301277483, 'val_Surgery_auroc': 0.5675, 'val_Surgery_auprc': 0.08249455508668423, 'val_Function_auroc': 0.2888888888888889, 'val_Function_auprc': 0.623494488774397, 'best_epoch': 41}\n",
      "val loss:\t0.6629 | val sum of auc:\t1.5156 \n",
      "\n",
      "Epoch 43/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0695 | train sum of auc:\t2.9623 \n",
      "\n",
      "{'train_loss': 0.06952329107693263, 'train_Reflux_auroc': 0.9854301572617946, 'train_Reflux_auprc': 0.9780909918224691, 'train_sum_auroc': 2.962282009113647, 'train_sum_auprc': 2.9384084521399294, 'train_Surgery_auroc': 1.0, 'train_Surgery_auprc': 1.0, 'train_Function_auroc': 0.9768518518518519, 'train_Function_auprc': 0.9603174603174605, 'val_loss': 0.5732188820838928, 'val_Reflux_auroc': 0.7040118679050568, 'val_Reflux_auprc': 0.7297605679241289, 'val_sum_auroc': 1.5832865592630814, 'val_sum_auprc': 1.4247290395438168, 'val_Surgery_auroc': 0.5879166666666666, 'val_Surgery_auprc': 0.08699127353415666, 'val_Function_auroc': 0.291358024691358, 'val_Function_auprc': 0.6079771980855313, 'best_epoch': 42}\n",
      "val loss:\t0.5732 | val sum of auc:\t1.5833 \n",
      "\n",
      "Epoch 44/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0578 | train sum of auc:\t2.9898 \n",
      "\n",
      "{'train_loss': 0.05778239050081798, 'train_Reflux_auroc': 0.9914431082331174, 'train_Reflux_auprc': 0.9800928463427696, 'train_sum_auroc': 2.989843108233117, 'train_sum_auprc': 2.967805264643423, 'train_Surgery_auroc': 0.9984, 'train_Surgery_auprc': 0.9877124183006536, 'train_Function_auroc': 1.0, 'train_Function_auprc': 1.0, 'val_loss': 0.5395729422569275, 'val_Reflux_auroc': 0.7220717234262126, 'val_Reflux_auprc': 0.7549447255312581, 'val_sum_auroc': 1.6153433283644842, 'val_sum_auprc': 1.42370007912823, 'val_Surgery_auroc': 0.6216666666666667, 'val_Surgery_auprc': 0.12181005392650951, 'val_Function_auroc': 0.2716049382716049, 'val_Function_auprc': 0.5469452996704622, 'best_epoch': 43}\n",
      "val loss:\t0.5396 | val sum of auc:\t1.6153 \n",
      "\n",
      "Epoch 45/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0589 | train sum of auc:\t2.9832 \n",
      "\n",
      "val loss:\t0.5922 | val sum of auc:\t1.6891 \n",
      "\n",
      "Epoch 46/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0486 | train sum of auc:\t2.9852 \n",
      "\n",
      "val loss:\t0.7581 | val sum of auc:\t1.5997 \n",
      "\n",
      "Epoch 47/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0567 | train sum of auc:\t2.9912 \n",
      "\n",
      "val loss:\t0.9861 | val sum of auc:\t1.5830 \n",
      "\n",
      "Epoch 48/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0311 | train sum of auc:\t2.9993 \n",
      "\n",
      "val loss:\t1.0315 | val sum of auc:\t1.6592 \n",
      "\n",
      "Epoch 49/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0549 | train sum of auc:\t2.9787 \n",
      "\n",
      "val loss:\t0.9021 | val sum of auc:\t1.6776 \n",
      "\n",
      "Epoch 50/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0753 | train sum of auc:\t2.9821 \n",
      "\n",
      "val loss:\t0.8105 | val sum of auc:\t1.6339 \n",
      "\n",
      "Epoch 51/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0580 | train sum of auc:\t2.9902 \n",
      "\n",
      "val loss:\t0.7912 | val sum of auc:\t1.5233 \n",
      "\n",
      "Epoch 52/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0622 | train sum of auc:\t2.9744 \n",
      "\n",
      "val loss:\t0.8203 | val sum of auc:\t1.4695 \n",
      "\n",
      "Epoch 53/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0711 | train sum of auc:\t2.9726 \n",
      "\n",
      "val loss:\t0.7907 | val sum of auc:\t1.5295 \n",
      "\n",
      "Epoch 54/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0517 | train sum of auc:\t2.9949 \n",
      "\n",
      "val loss:\t0.7666 | val sum of auc:\t1.5511 \n",
      "\n",
      "Epoch 55/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0891 | train sum of auc:\t2.9621 \n",
      "\n",
      "val loss:\t0.7495 | val sum of auc:\t1.4290 \n",
      "\n",
      "Epoch 56/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0543 | train sum of auc:\t2.9841 \n",
      "\n",
      "val loss:\t0.7694 | val sum of auc:\t1.4018 \n",
      "\n",
      "Epoch 57/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0237 | train sum of auc:\t2.9995 \n",
      "\n",
      "val loss:\t0.8221 | val sum of auc:\t1.3497 \n",
      "\n",
      "Epoch 58/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0526 | train sum of auc:\t2.9912 \n",
      "\n",
      "val loss:\t0.8584 | val sum of auc:\t1.3126 \n",
      "\n",
      "Epoch 59/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0466 | train sum of auc:\t2.9957 \n",
      "\n",
      "val loss:\t0.8621 | val sum of auc:\t1.3705 \n",
      "\n",
      "Epoch 60/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0409 | train sum of auc:\t2.9965 \n",
      "\n",
      "val loss:\t0.9091 | val sum of auc:\t1.4647 \n",
      "\n",
      "Epoch 61/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0450 | train sum of auc:\t2.9930 \n",
      "\n",
      "val loss:\t0.8755 | val sum of auc:\t1.5244 \n",
      "\n",
      "Epoch 62/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0481 | train sum of auc:\t2.9861 \n",
      "\n",
      "val loss:\t0.8966 | val sum of auc:\t1.5089 \n",
      "\n",
      "Epoch 63/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0530 | train sum of auc:\t2.9844 \n",
      "\n",
      "val loss:\t0.8735 | val sum of auc:\t1.4556 \n",
      "\n",
      "Epoch 64/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0181 | train sum of auc:\t3.0000 \n",
      "\n",
      "val loss:\t0.9379 | val sum of auc:\t1.2896 \n",
      "\n",
      "EARLY STOPPING 63\n",
      "Epoch 65/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/oma4g26f\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/oma4g26f</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.4763 | train sum of auc:\t1.3319 \n",
      "\n",
      "val loss:\t0.4306 | val sum of auc:\t1.5885 \n",
      "\n",
      "Epoch 2/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3897 | train sum of auc:\t1.8957 \n",
      "\n",
      "val loss:\t0.4137 | val sum of auc:\t1.3806 \n",
      "\n",
      "Epoch 3/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3076 | train sum of auc:\t2.2386 \n",
      "\n",
      "val loss:\t0.3947 | val sum of auc:\t1.7760 \n",
      "\n",
      "Epoch 4/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2996 | train sum of auc:\t2.3092 \n",
      "\n",
      "val loss:\t0.4124 | val sum of auc:\t1.6747 \n",
      "\n",
      "Epoch 5/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2645 | train sum of auc:\t2.3990 \n",
      "\n",
      "val loss:\t0.5315 | val sum of auc:\t1.4858 \n",
      "\n",
      "Epoch 6/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2596 | train sum of auc:\t2.5770 \n",
      "\n",
      "val loss:\t0.6736 | val sum of auc:\t1.1176 \n",
      "\n",
      "Epoch 7/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2740 | train sum of auc:\t2.5101 \n",
      "\n",
      "val loss:\t1.4354 | val sum of auc:\t1.3867 \n",
      "\n",
      "Epoch 8/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2494 | train sum of auc:\t2.5805 \n",
      "\n",
      "val loss:\t0.7537 | val sum of auc:\t1.5224 \n",
      "\n",
      "Epoch 9/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2431 | train sum of auc:\t2.6133 \n",
      "\n",
      "val loss:\t0.6241 | val sum of auc:\t1.2363 \n",
      "\n",
      "Epoch 10/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2050 | train sum of auc:\t2.7140 \n",
      "\n",
      "val loss:\t0.4304 | val sum of auc:\t1.1729 \n",
      "\n",
      "Epoch 11/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2380 | train sum of auc:\t2.6778 \n",
      "\n",
      "val loss:\t0.4962 | val sum of auc:\t1.1593 \n",
      "\n",
      "Epoch 12/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2230 | train sum of auc:\t2.6613 \n",
      "\n",
      "val loss:\t0.7084 | val sum of auc:\t0.9346 \n",
      "\n",
      "Epoch 13/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1687 | train sum of auc:\t2.8133 \n",
      "\n",
      "val loss:\t0.8479 | val sum of auc:\t0.7588 \n",
      "\n",
      "Epoch 14/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2129 | train sum of auc:\t2.7121 \n",
      "\n",
      "val loss:\t0.8034 | val sum of auc:\t0.7137 \n",
      "\n",
      "Epoch 15/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1658 | train sum of auc:\t2.7957 \n",
      "\n",
      "val loss:\t0.7709 | val sum of auc:\t0.9298 \n",
      "\n",
      "Epoch 16/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1867 | train sum of auc:\t2.7510 \n",
      "\n",
      "val loss:\t0.7842 | val sum of auc:\t1.2384 \n",
      "\n",
      "Epoch 17/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1594 | train sum of auc:\t2.8788 \n",
      "\n",
      "val loss:\t0.8503 | val sum of auc:\t1.2284 \n",
      "\n",
      "Epoch 18/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1215 | train sum of auc:\t2.9504 \n",
      "\n",
      "val loss:\t0.8556 | val sum of auc:\t0.9977 \n",
      "\n",
      "Epoch 19/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1448 | train sum of auc:\t2.9042 \n",
      "\n",
      "val loss:\t0.6972 | val sum of auc:\t0.9768 \n",
      "\n",
      "Epoch 20/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1391 | train sum of auc:\t2.9162 \n",
      "\n",
      "val loss:\t0.5236 | val sum of auc:\t1.1183 \n",
      "\n",
      "Epoch 21/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1358 | train sum of auc:\t2.9061 \n",
      "\n",
      "val loss:\t0.5472 | val sum of auc:\t1.0251 \n",
      "\n",
      "Epoch 22/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1325 | train sum of auc:\t2.8886 \n",
      "\n",
      "val loss:\t0.7812 | val sum of auc:\t0.7969 \n",
      "\n",
      "Epoch 23/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1495 | train sum of auc:\t2.8357 \n",
      "\n",
      "val loss:\t0.7581 | val sum of auc:\t0.9199 \n",
      "\n",
      "Epoch 24/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1550 | train sum of auc:\t2.8098 \n",
      "\n",
      "val loss:\t0.5929 | val sum of auc:\t1.2759 \n",
      "\n",
      "Epoch 25/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1014 | train sum of auc:\t2.9559 \n",
      "\n",
      "val loss:\t0.8024 | val sum of auc:\t1.6889 \n",
      "\n",
      "Epoch 26/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0862 | train sum of auc:\t2.9390 \n",
      "\n",
      "val loss:\t0.9812 | val sum of auc:\t1.8257 \n",
      "\n",
      "Epoch 27/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1420 | train sum of auc:\t2.8075 \n",
      "\n",
      "val loss:\t1.0062 | val sum of auc:\t1.7721 \n",
      "\n",
      "Epoch 28/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1010 | train sum of auc:\t2.9526 \n",
      "\n",
      "val loss:\t1.1646 | val sum of auc:\t1.5517 \n",
      "\n",
      "Epoch 29/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1059 | train sum of auc:\t2.9169 \n",
      "\n",
      "val loss:\t0.9247 | val sum of auc:\t1.2771 \n",
      "\n",
      "Epoch 30/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1478 | train sum of auc:\t2.8483 \n",
      "\n",
      "val loss:\t0.9562 | val sum of auc:\t1.0110 \n",
      "\n",
      "Epoch 31/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1010 | train sum of auc:\t2.9407 \n",
      "\n",
      "{'train_loss': 0.10103425581121084, 'train_Reflux_auroc': 0.9806830907054872, 'train_Reflux_auprc': 0.951745168503694, 'train_sum_auroc': 2.9406866263045504, 'train_sum_auprc': 2.821597538356064, 'train_Surgery_auroc': 0.9706987227648385, 'train_Surgery_auprc': 0.8838383838383839, 'train_Function_auroc': 0.9893048128342246, 'train_Function_auprc': 0.986013986013986, 'val_loss': 0.958850642045339, 'val_Reflux_auroc': 0.3374225150635051, 'val_Reflux_auprc': 0.11535485865721705, 'val_sum_auroc': 0.9003089676757505, 'val_sum_auprc': 0.3793955134244587, 'val_Surgery_auroc': 0.5417224314482243, 'val_Surgery_auprc': 0.22828786686802066, 'val_Function_auroc': 0.02116402116402118, 'val_Function_auprc': 0.03575278789922097, 'best_epoch': 30}\n",
      "val loss:\t0.9589 | val sum of auc:\t0.9003 \n",
      "\n",
      "Epoch 32/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1061 | train sum of auc:\t2.9445 \n",
      "\n",
      "{'train_loss': 0.10613894259387796, 'train_Reflux_auroc': 0.9720044792833146, 'train_Reflux_auprc': 0.9112888980383536, 'train_sum_auroc': 2.9445151965679743, 'train_sum_auprc': 2.875304049553505, 'train_Surgery_auroc': 0.9992486851990985, 'train_Surgery_auprc': 0.9924242424242424, 'train_Function_auroc': 0.9732620320855614, 'train_Function_auprc': 0.9715909090909091, 'val_loss': 0.7847126192516751, 'val_Reflux_auroc': 0.29832242403225107, 'val_Reflux_auprc': 0.08920683219198518, 'val_sum_auroc': 0.7183942624099813, 'val_sum_auprc': 0.3008267537288568, 'val_Surgery_auroc': 0.4081670764729683, 'val_Surgery_auprc': 0.17607525230775195, 'val_Function_auroc': 0.011904761904761894, 'val_Function_auprc': 0.0355446692291197, 'best_epoch': 31}\n",
      "val loss:\t0.7847 | val sum of auc:\t0.7184 \n",
      "\n",
      "Epoch 33/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0963 | train sum of auc:\t2.9551 \n",
      "\n",
      "{'train_loss': 0.09627328897741708, 'train_Reflux_auroc': 0.9778835386338186, 'train_Reflux_auprc': 0.9523699271342966, 'train_sum_auroc': 2.955078924677041, 'train_sum_auprc': 2.8513893420628023, 'train_Surgery_auroc': 0.9932381667918858, 'train_Surgery_auprc': 0.92395054440509, 'train_Function_auroc': 0.9839572192513368, 'train_Function_auprc': 0.9750688705234161, 'val_loss': 0.6116919782426622, 'val_Reflux_auroc': 0.33729247041484245, 'val_Reflux_auprc': 0.09329771873180806, 'val_sum_auroc': 0.7914145437827684, 'val_sum_auprc': 0.30392323701008134, 'val_Surgery_auroc': 0.4422173114631641, 'val_Surgery_auprc': 0.17508084904915358, 'val_Function_auroc': 0.011904761904761894, 'val_Function_auprc': 0.0355446692291197, 'best_epoch': 32}\n",
      "val loss:\t0.6117 | val sum of auc:\t0.7914 \n",
      "\n",
      "Epoch 34/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1134 | train sum of auc:\t2.9213 \n",
      "\n",
      "val loss:\t0.7564 | val sum of auc:\t0.8784 \n",
      "\n",
      "Epoch 35/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0801 | train sum of auc:\t2.9567 \n",
      "\n",
      "val loss:\t0.9063 | val sum of auc:\t0.9436 \n",
      "\n",
      "Epoch 36/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1080 | train sum of auc:\t2.9388 \n",
      "\n",
      "val loss:\t0.8207 | val sum of auc:\t1.0836 \n",
      "\n",
      "Epoch 37/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0974 | train sum of auc:\t2.9518 \n",
      "\n",
      "val loss:\t0.7865 | val sum of auc:\t1.2023 \n",
      "\n",
      "Epoch 38/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1139 | train sum of auc:\t2.9363 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t0.7746 | val sum of auc:\t1.3299 \n",
      "\n",
      "Epoch 39/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1166 | train sum of auc:\t2.9243 \n",
      "\n",
      "val loss:\t0.7985 | val sum of auc:\t1.4401 \n",
      "\n",
      "Epoch 40/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0636 | train sum of auc:\t2.9928 \n",
      "\n",
      "val loss:\t0.9737 | val sum of auc:\t1.5035 \n",
      "\n",
      "Epoch 41/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0915 | train sum of auc:\t2.9589 \n",
      "\n",
      "val loss:\t0.9628 | val sum of auc:\t1.5803 \n",
      "\n",
      "Epoch 42/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0681 | train sum of auc:\t2.9635 \n",
      "\n",
      "val loss:\t0.8836 | val sum of auc:\t1.6568 \n",
      "\n",
      "Epoch 43/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0808 | train sum of auc:\t2.9614 \n",
      "\n",
      "val loss:\t0.8414 | val sum of auc:\t1.7539 \n",
      "\n",
      "Epoch 44/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0892 | train sum of auc:\t2.9639 \n",
      "\n",
      "val loss:\t0.8111 | val sum of auc:\t1.8949 \n",
      "\n",
      "Epoch 45/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0758 | train sum of auc:\t2.9762 \n",
      "\n",
      "val loss:\t0.9734 | val sum of auc:\t2.0145 \n",
      "\n",
      "Epoch 46/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0870 | train sum of auc:\t2.9624 \n",
      "\n",
      "val loss:\t0.7366 | val sum of auc:\t1.7914 \n",
      "\n",
      "Epoch 47/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0726 | train sum of auc:\t2.9761 \n",
      "\n",
      "val loss:\t0.7397 | val sum of auc:\t1.4083 \n",
      "\n",
      "Epoch 48/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0694 | train sum of auc:\t2.9814 \n",
      "\n",
      "val loss:\t0.8402 | val sum of auc:\t1.2903 \n",
      "\n",
      "Epoch 49/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0828 | train sum of auc:\t2.9706 \n",
      "\n",
      "val loss:\t0.8355 | val sum of auc:\t1.5383 \n",
      "\n",
      "Epoch 50/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0602 | train sum of auc:\t2.9835 \n",
      "\n",
      "val loss:\t0.7477 | val sum of auc:\t1.7589 \n",
      "\n",
      "Epoch 51/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0630 | train sum of auc:\t2.9853 \n",
      "\n",
      "val loss:\t0.7774 | val sum of auc:\t1.4106 \n",
      "\n",
      "Epoch 52/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0618 | train sum of auc:\t2.9809 \n",
      "\n",
      "val loss:\t0.8637 | val sum of auc:\t1.1212 \n",
      "\n",
      "Epoch 53/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0640 | train sum of auc:\t2.9827 \n",
      "\n",
      "val loss:\t1.0220 | val sum of auc:\t1.0918 \n",
      "\n",
      "EARLY STOPPING 52\n",
      "Epoch 54/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/2chqqcem\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/2chqqcem</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.6034 | train sum of auc:\t1.1816 \n",
      "\n",
      "val loss:\t0.7845 | val sum of auc:\t1.2999 \n",
      "\n",
      "Epoch 2/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.4237 | train sum of auc:\t1.8908 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.4713 \n",
      "\n",
      "Epoch 3/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3833 | train sum of auc:\t2.1085 \n",
      "\n",
      "val loss:\t0.4579 | val sum of auc:\t1.5279 \n",
      "\n",
      "Epoch 4/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3173 | train sum of auc:\t2.2525 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.3173 \n",
      "\n",
      "Epoch 5/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3391 | train sum of auc:\t2.1544 \n",
      "\n",
      "val loss:\t0.3833 | val sum of auc:\t1.2805 \n",
      "\n",
      "Epoch 6/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2950 | train sum of auc:\t2.4056 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.5017 \n",
      "\n",
      "Epoch 7/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2733 | train sum of auc:\t2.5738 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.4484 \n",
      "\n",
      "Epoch 8/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2309 | train sum of auc:\t2.6805 \n",
      "\n",
      "val loss:\t0.8096 | val sum of auc:\t1.3726 \n",
      "\n",
      "Epoch 9/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2864 | train sum of auc:\t2.4486 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.3121 \n",
      "\n",
      "Epoch 10/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2396 | train sum of auc:\t2.5787 \n",
      "\n",
      "val loss:\t0.6601 | val sum of auc:\t1.1828 \n",
      "\n",
      "Epoch 11/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2161 | train sum of auc:\t2.7280 \n",
      "\n",
      "val loss:\t0.7539 | val sum of auc:\t1.1981 \n",
      "\n",
      "Epoch 12/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2277 | train sum of auc:\t2.6801 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.2459 \n",
      "\n",
      "Epoch 13/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2105 | train sum of auc:\t2.7994 \n",
      "\n",
      "val loss:\t2.3995 | val sum of auc:\t1.3528 \n",
      "\n",
      "Epoch 14/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2452 | train sum of auc:\t2.6785 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.5339 \n",
      "\n",
      "Epoch 15/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2347 | train sum of auc:\t2.6903 \n",
      "\n",
      "val loss:\t0.5145 | val sum of auc:\t1.2805 \n",
      "\n",
      "Epoch 16/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1933 | train sum of auc:\t2.6791 \n",
      "\n",
      "val loss:\t0.4981 | val sum of auc:\t1.2675 \n",
      "\n",
      "Epoch 17/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1878 | train sum of auc:\t2.7603 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.5134 \n",
      "\n",
      "Epoch 18/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1587 | train sum of auc:\t2.8389 \n",
      "\n",
      "val loss:\t0.5186 | val sum of auc:\t1.5938 \n",
      "\n",
      "Epoch 19/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2092 | train sum of auc:\t2.6627 \n",
      "\n",
      "val loss:\t0.5746 | val sum of auc:\t1.4095 \n",
      "\n",
      "Epoch 20/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1597 | train sum of auc:\t2.8509 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.4244 \n",
      "\n",
      "Epoch 21/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2239 | train sum of auc:\t2.6762 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.6567 \n",
      "\n",
      "Epoch 22/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1806 | train sum of auc:\t2.8409 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.5320 \n",
      "\n",
      "Epoch 23/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1995 | train sum of auc:\t2.7554 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.3018 \n",
      "\n",
      "Epoch 24/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1744 | train sum of auc:\t2.8340 \n",
      "\n",
      "val loss:\t0.7717 | val sum of auc:\t1.2203 \n",
      "\n",
      "Epoch 25/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2369 | train sum of auc:\t2.6806 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.2394 \n",
      "\n",
      "Epoch 26/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1683 | train sum of auc:\t2.8354 \n",
      "\n",
      "val loss:\t0.7635 | val sum of auc:\t1.3218 \n",
      "\n",
      "Epoch 27/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1428 | train sum of auc:\t2.9072 \n",
      "\n",
      "val loss:\t0.8002 | val sum of auc:\t1.4180 \n",
      "\n",
      "Epoch 28/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1383 | train sum of auc:\t2.9174 \n",
      "\n",
      "val loss:\t0.7805 | val sum of auc:\t1.5144 \n",
      "\n",
      "Epoch 29/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1230 | train sum of auc:\t2.9478 \n",
      "\n",
      "val loss:\t0.8392 | val sum of auc:\t1.6475 \n",
      "\n",
      "Epoch 30/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1346 | train sum of auc:\t2.9249 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.6161 \n",
      "\n",
      "Epoch 31/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1227 | train sum of auc:\t2.9365 \n",
      "\n",
      "{'train_loss': 0.12270052096358051, 'train_Reflux_auroc': 0.9372252076209087, 'train_Reflux_auprc': 0.8991764731936571, 'train_sum_auroc': 2.9364920697909964, 'train_sum_auprc': 2.8916007156178996, 'train_Surgery_auroc': 0.999266862170088, 'train_Surgery_auprc': 0.9924242424242424, 'train_Function_auroc': 1.0, 'train_Function_auprc': 1.0, 'val_loss': 0.8418308125426438, 'val_Reflux_auroc': 0.4048560135516658, 'val_Reflux_auprc': 0.12867075219000737, 'val_sum_auroc': 1.557716024964309, 'val_sum_auprc': 0.7593597043824916, 'val_Surgery_auroc': 0.5561204146730463, 'val_Surgery_auprc': 0.22663429715493097, 'val_Function_auroc': 0.5967395967395968, 'val_Function_auprc': 0.40405465503755317, 'best_epoch': 30}\n",
      "val loss:\t0.8418 | val sum of auc:\t1.5577 \n",
      "\n",
      "Epoch 32/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1105 | train sum of auc:\t2.9425 \n",
      "\n",
      "{'train_loss': 0.11046899275647269, 'train_Reflux_auroc': 0.9755740107474353, 'train_Reflux_auprc': 0.9615444076169541, 'train_sum_auroc': 2.9425461515098985, 'train_sum_auprc': 2.80149901319656, 'train_Surgery_auroc': 0.9794721407624634, 'train_Surgery_auprc': 0.8694684944684945, 'train_Function_auroc': 0.9874999999999999, 'train_Function_auprc': 0.9704861111111112, 'val_loss': 0.8004643750586701, 'val_Reflux_auroc': 0.3405662660321045, 'val_Reflux_auprc': 0.11728034478390967, 'val_sum_auroc': 1.5492354615434052, 'val_sum_auprc': 0.7853443857465061, 'val_Surgery_auroc': 0.5570175438596491, 'val_Surgery_auprc': 0.22492342447516084, 'val_Function_auroc': 0.6516516516516517, 'val_Function_auprc': 0.4431406164874356, 'best_epoch': 31}\n",
      "val loss:\t0.8005 | val sum of auc:\t1.5492 \n",
      "\n",
      "Epoch 33/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1359 | train sum of auc:\t2.9167 \n",
      "\n",
      "{'train_loss': 0.13592743983975164, 'train_Reflux_auroc': 0.9262335124572545, 'train_Reflux_auprc': 0.8990176935393168, 'train_sum_auroc': 2.916702720668398, 'train_sum_auprc': 2.8497752692968925, 'train_Surgery_auroc': 0.9904692082111437, 'train_Surgery_auprc': 0.9507575757575757, 'train_Function_auroc': 1.0, 'train_Function_auprc': 1.0, 'val_loss': 0.7401115384865675, 'val_Reflux_auroc': 0.4017100911510849, 'val_Reflux_auprc': 0.13010443101157265, 'val_sum_auroc': 1.4925238628859094, 'val_sum_auprc': 0.6989708680186479, 'val_Surgery_auroc': 0.5455542264752792, 'val_Surgery_auprc': 0.17587201480658343, 'val_Function_auroc': 0.5452595452595453, 'val_Function_auprc': 0.39299442220049186, 'best_epoch': 32}\n",
      "val loss:\t0.7401 | val sum of auc:\t1.4925 \n",
      "\n",
      "Epoch 34/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0915 | train sum of auc:\t2.9669 \n",
      "\n",
      "{'train_loss': 0.0915025657525769, 'train_Reflux_auroc': 0.9782608695652173, 'train_Reflux_auprc': 0.9664321756734935, 'train_sum_auroc': 2.9668789047558333, 'train_sum_auprc': 2.894242748938612, 'train_Surgery_auroc': 0.9948680351906158, 'train_Surgery_auprc': 0.9416994621540076, 'train_Function_auroc': 0.9937499999999999, 'train_Function_auprc': 0.9861111111111112, 'val_loss': 0.7149380682319995, 'val_Reflux_auroc': 0.5016536258772284, 'val_Reflux_auprc': 0.1581752241474548, 'val_sum_auroc': 1.4403776369696604, 'val_sum_auprc': 0.6153912161211811, 'val_Surgery_auroc': 0.5273125996810207, 'val_Surgery_auprc': 0.1374723272659712, 'val_Function_auroc': 0.4114114114114114, 'val_Function_auprc': 0.31974366470775506, 'best_epoch': 33}\n",
      "val loss:\t0.7149 | val sum of auc:\t1.4404 \n",
      "\n",
      "Epoch 35/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.0954 | train sum of auc:\t2.9670 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.3055 \n",
      "\n",
      "EARLY STOPPING 34\n",
      "Epoch 36/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/36tzxd6n\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/36tzxd6n</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/3k45yxvr\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/3k45yxvr</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.4770 | train sum of auc:\t1.5070 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.3100 \n",
      "\n",
      "Epoch 2/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3806 | train sum of auc:\t1.9191 \n",
      "\n",
      "val loss:\t0.4834 | val sum of auc:\t1.0434 \n",
      "\n",
      "Epoch 3/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2896 | train sum of auc:\t2.3472 \n",
      "\n",
      "val loss:\t0.5426 | val sum of auc:\t1.1643 \n",
      "\n",
      "Epoch 4/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3562 | train sum of auc:\t2.2493 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.1506 \n",
      "\n",
      "Epoch 5/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3086 | train sum of auc:\t2.4231 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.3538 \n",
      "\n",
      "Epoch 6/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3089 | train sum of auc:\t2.3220 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.5604 \n",
      "\n",
      "Epoch 7/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2549 | train sum of auc:\t2.4878 \n",
      "\n",
      "val loss:\t0.6994 | val sum of auc:\t1.6037 \n",
      "\n",
      "Epoch 8/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2139 | train sum of auc:\t2.7637 \n",
      "\n",
      "val loss:\t1.0012 | val sum of auc:\t1.6917 \n",
      "\n",
      "Epoch 9/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2083 | train sum of auc:\t2.7359 \n",
      "\n",
      "val loss:\t0.8931 | val sum of auc:\t1.6459 \n",
      "\n",
      "Epoch 10/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2199 | train sum of auc:\t2.6655 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.6086 \n",
      "\n",
      "Epoch 11/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2076 | train sum of auc:\t2.7544 \n",
      "\n",
      "val loss:\t0.7314 | val sum of auc:\t1.6813 \n",
      "\n",
      "Epoch 12/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2324 | train sum of auc:\t2.6669 \n",
      "\n",
      "val loss:\t0.9013 | val sum of auc:\t1.7580 \n",
      "\n",
      "Epoch 13/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1767 | train sum of auc:\t2.7958 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.8655 \n",
      "\n",
      "Epoch 14/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1878 | train sum of auc:\t2.7387 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.8819 \n",
      "\n",
      "Epoch 15/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1626 | train sum of auc:\t2.8689 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.8597 \n",
      "\n",
      "Epoch 16/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1559 | train sum of auc:\t2.8815 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.8397 \n",
      "\n",
      "Epoch 17/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1399 | train sum of auc:\t2.8958 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.8141 \n",
      "\n",
      "Epoch 18/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1418 | train sum of auc:\t2.8492 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.7565 \n",
      "\n",
      "Epoch 19/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1148 | train sum of auc:\t2.9280 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.7357 \n",
      "\n",
      "Epoch 20/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1433 | train sum of auc:\t2.8449 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.7018 \n",
      "\n",
      "Epoch 21/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1138 | train sum of auc:\t2.9226 \n",
      "\n",
      "val loss:\t0.6030 | val sum of auc:\t1.5815 \n",
      "\n",
      "Epoch 22/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1119 | train sum of auc:\t2.9262 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.5551 \n",
      "\n",
      "Epoch 23/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1041 | train sum of auc:\t2.9076 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.6619 \n",
      "\n",
      "Epoch 24/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1052 | train sum of auc:\t2.9120 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.6884 \n",
      "\n",
      "Epoch 25/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0868 | train sum of auc:\t2.9794 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.5610 \n",
      "\n",
      "Epoch 26/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1054 | train sum of auc:\t2.9266 \n",
      "\n",
      "val loss:\t1.4148 | val sum of auc:\t1.6306 \n",
      "\n",
      "Epoch 27/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0656 | train sum of auc:\t2.9845 \n",
      "\n",
      "val loss:\t1.1159 | val sum of auc:\t1.7952 \n",
      "\n",
      "Epoch 28/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0729 | train sum of auc:\t2.9833 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.8161 \n",
      "\n",
      "Epoch 29/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0594 | train sum of auc:\t2.9838 \n",
      "\n",
      "val loss:\t0.9378 | val sum of auc:\t1.6717 \n",
      "\n",
      "Epoch 30/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0853 | train sum of auc:\t2.9465 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.6210 \n",
      "\n",
      "Epoch 31/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0612 | train sum of auc:\t2.9722 \n",
      "\n",
      "{'train_loss': 0.06121602045622335, 'train_Reflux_auroc': 0.9965986394557823, 'train_Reflux_auprc': 0.9911031983400405, 'train_sum_auroc': 2.972194182124496, 'train_sum_auprc': 2.9293856526652755, 'train_Surgery_auroc': 0.9948263118994826, 'train_Surgery_auprc': 0.9596499756927565, 'train_Function_auroc': 0.9807692307692308, 'train_Function_auprc': 0.9786324786324785, 'val_loss': 1.0332937073392836, 'val_Reflux_auroc': 0.4306645735217163, 'val_Reflux_auprc': 0.4913754001088556, 'val_sum_auroc': 1.6232653476943446, 'val_sum_auprc': 0.9989524897405788, 'val_Surgery_auroc': 0.41300893743793443, 'val_Surgery_auprc': 0.1036315114004307, 'val_Function_auroc': 0.7795918367346939, 'val_Function_auprc': 0.4039455782312925, 'best_epoch': 30}\n",
      "val loss:\t1.0333 | val sum of auc:\t1.6233 \n",
      "\n",
      "Epoch 32/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0607 | train sum of auc:\t2.9835 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.6236 \n",
      "\n",
      "EARLY STOPPING 31\n",
      "Epoch 33/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/2cv4ad89\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/2cv4ad89</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.5367 | train sum of auc:\t1.7134 \n",
      "\n",
      "val loss:\t0.3706 | val sum of auc:\t1.1820 \n",
      "\n",
      "Epoch 2/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3855 | train sum of auc:\t2.1320 \n",
      "\n",
      "val loss:\t0.3585 | val sum of auc:\t1.6385 \n",
      "\n",
      "Epoch 3/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3808 | train sum of auc:\t1.9449 \n",
      "\n",
      "val loss:\t0.4376 | val sum of auc:\t1.2677 \n",
      "\n",
      "Epoch 4/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3020 | train sum of auc:\t2.4303 \n",
      "\n",
      "val loss:\t0.4203 | val sum of auc:\t1.5561 \n",
      "\n",
      "Epoch 5/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2964 | train sum of auc:\t2.3464 \n",
      "\n",
      "val loss:\t0.3545 | val sum of auc:\t1.4906 \n",
      "\n",
      "Epoch 6/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2949 | train sum of auc:\t2.2974 \n",
      "\n",
      "val loss:\t0.3422 | val sum of auc:\t1.4876 \n",
      "\n",
      "Epoch 7/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2059 | train sum of auc:\t2.7851 \n",
      "\n",
      "val loss:\t0.3633 | val sum of auc:\t1.4820 \n",
      "\n",
      "Epoch 8/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2426 | train sum of auc:\t2.6855 \n",
      "\n",
      "val loss:\t0.4456 | val sum of auc:\t1.4149 \n",
      "\n",
      "Epoch 9/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1889 | train sum of auc:\t2.8323 \n",
      "\n",
      "val loss:\t0.3708 | val sum of auc:\t1.4861 \n",
      "\n",
      "Epoch 10/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1665 | train sum of auc:\t2.8443 \n",
      "\n",
      "val loss:\t0.3834 | val sum of auc:\t1.6343 \n",
      "\n",
      "Epoch 11/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2042 | train sum of auc:\t2.7616 \n",
      "\n",
      "val loss:\t0.4284 | val sum of auc:\t1.7166 \n",
      "\n",
      "Epoch 12/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1869 | train sum of auc:\t2.8505 \n",
      "\n",
      "val loss:\t0.4925 | val sum of auc:\t1.8080 \n",
      "\n",
      "Epoch 13/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1544 | train sum of auc:\t2.8277 \n",
      "\n",
      "val loss:\t0.5164 | val sum of auc:\t1.6257 \n",
      "\n",
      "Epoch 14/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1620 | train sum of auc:\t2.8325 \n",
      "\n",
      "val loss:\t0.5548 | val sum of auc:\t1.5761 \n",
      "\n",
      "Epoch 15/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1493 | train sum of auc:\t2.8657 \n",
      "\n",
      "val loss:\t0.5590 | val sum of auc:\t1.6671 \n",
      "\n",
      "Epoch 16/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1300 | train sum of auc:\t2.9348 \n",
      "\n",
      "val loss:\t0.5727 | val sum of auc:\t1.7818 \n",
      "\n",
      "Epoch 17/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1373 | train sum of auc:\t2.8385 \n",
      "\n",
      "val loss:\t0.5414 | val sum of auc:\t1.5921 \n",
      "\n",
      "Epoch 18/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1520 | train sum of auc:\t2.8518 \n",
      "\n",
      "val loss:\t0.3575 | val sum of auc:\t1.5901 \n",
      "\n",
      "Epoch 19/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1390 | train sum of auc:\t2.8830 \n",
      "\n",
      "val loss:\t0.5172 | val sum of auc:\t1.3617 \n",
      "\n",
      "Epoch 20/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1689 | train sum of auc:\t2.8364 \n",
      "\n",
      "val loss:\t0.6117 | val sum of auc:\t1.3563 \n",
      "\n",
      "Epoch 21/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1242 | train sum of auc:\t2.8935 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.4892 \n",
      "\n",
      "Epoch 22/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1017 | train sum of auc:\t2.9369 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.5371 \n",
      "\n",
      "Epoch 23/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1460 | train sum of auc:\t2.8571 \n",
      "\n",
      "val loss:\t0.9000 | val sum of auc:\t1.5515 \n",
      "\n",
      "Epoch 24/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1456 | train sum of auc:\t2.8752 \n",
      "\n",
      "val loss:\t0.7396 | val sum of auc:\t1.6713 \n",
      "\n",
      "Epoch 25/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1005 | train sum of auc:\t2.9490 \n",
      "\n",
      "val loss:\t0.5460 | val sum of auc:\t1.6891 \n",
      "\n",
      "Epoch 26/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0895 | train sum of auc:\t2.9636 \n",
      "\n",
      "val loss:\t0.4931 | val sum of auc:\t1.7112 \n",
      "\n",
      "Epoch 27/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1026 | train sum of auc:\t2.9228 \n",
      "\n",
      "val loss:\t0.4633 | val sum of auc:\t1.8502 \n",
      "\n",
      "Epoch 28/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0931 | train sum of auc:\t2.9607 \n",
      "\n",
      "val loss:\t0.4410 | val sum of auc:\t1.9734 \n",
      "\n",
      "Epoch 29/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0761 | train sum of auc:\t2.9832 \n",
      "\n",
      "val loss:\t0.4670 | val sum of auc:\t1.9262 \n",
      "\n",
      "Epoch 30/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0973 | train sum of auc:\t2.8406 \n",
      "\n",
      "val loss:\t0.4675 | val sum of auc:\t1.9074 \n",
      "\n",
      "Epoch 31/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0647 | train sum of auc:\t2.9749 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.8453 \n",
      "\n",
      "Epoch 32/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0592 | train sum of auc:\t2.9785 \n",
      "\n",
      "{'train_loss': 0.059190409970195854, 'train_Reflux_auroc': 0.9919979994998749, 'train_Reflux_auprc': 0.9872909016727868, 'train_sum_auroc': 2.9785059360078114, 'train_sum_auprc': 2.9243279387098235, 'train_Surgery_auroc': 0.9865079365079366, 'train_Surgery_auprc': 0.937037037037037, 'train_Function_auroc': 1.0, 'train_Function_auprc': 1.0, 'val_loss': 0.4124342561191857, 'val_Reflux_auroc': 0.6622210125204138, 'val_Reflux_auprc': 0.3409193158699876, 'val_sum_auroc': 1.81636160631618, 'val_sum_auprc': 0.7307024587757451, 'val_Surgery_auroc': 0.7352216748768473, 'val_Surgery_auprc': 0.10830798708233483, 'val_Function_auroc': 0.4189189189189189, 'val_Function_auprc': 0.28147515582342275, 'best_epoch': 31}\n",
      "val loss:\t0.4124 | val sum of auc:\t1.8164 \n",
      "\n",
      "Epoch 33/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0941 | train sum of auc:\t2.9590 \n",
      "\n",
      "{'train_loss': 0.09412537109764184, 'train_Reflux_auroc': 0.9637409352338084, 'train_Reflux_auprc': 0.9381551424221589, 'train_sum_auroc': 2.9589790304719035, 'train_sum_auprc': 2.8948218090888256, 'train_Surgery_auroc': 0.9952380952380953, 'train_Surgery_auprc': 0.9566666666666667, 'train_Function_auroc': 1.0, 'train_Function_auprc': 1.0, 'val_loss': 0.36385425939379146, 'val_Reflux_auroc': 0.7606151333696244, 'val_Reflux_auprc': 0.5106774927831954, 'val_sum_auroc': 1.9014696578448385, 'val_sum_auprc': 0.9035539742801317, 'val_Surgery_auroc': 0.7444581280788177, 'val_Surgery_auprc': 0.11075462315868018, 'val_Function_auroc': 0.3963963963963964, 'val_Function_auprc': 0.2821218583382561, 'best_epoch': 32}\n",
      "val loss:\t0.3639 | val sum of auc:\t1.9015 \n",
      "\n",
      "Epoch 34/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0520 | train sum of auc:\t2.9871 \n",
      "\n",
      "{'train_loss': 0.051961989689837486, 'train_Reflux_auroc': 0.9949987496874217, 'train_Reflux_auprc': 0.9918000604047117, 'train_sum_auroc': 2.987062241750914, 'train_sum_auprc': 2.9418000604047116, 'train_Surgery_auroc': 0.9920634920634921, 'train_Surgery_auprc': 0.9500000000000001, 'train_Function_auroc': 1.0, 'train_Function_auprc': 1.0, 'val_loss': 0.35474634777878133, 'val_Reflux_auroc': 0.7900108873162766, 'val_Reflux_auprc': 0.5446121834555517, 'val_sum_auroc': 2.0140785510391126, 'val_sum_auprc': 0.9444082206471586, 'val_Surgery_auroc': 0.8066502463054188, 'val_Surgery_auprc': 0.11934668129151402, 'val_Function_auroc': 0.4174174174174174, 'val_Function_auprc': 0.2804493559000928, 'best_epoch': 33}\n",
      "val loss:\t0.3547 | val sum of auc:\t2.0141 \n",
      "\n",
      "Epoch 35/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0583 | train sum of auc:\t2.9925 \n",
      "\n",
      "val loss:\t0.3848 | val sum of auc:\t2.0738 \n",
      "\n",
      "Epoch 36/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0480 | train sum of auc:\t2.9920 \n",
      "\n",
      "val loss:\t0.3759 | val sum of auc:\t2.2548 \n",
      "\n",
      "Epoch 37/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0862 | train sum of auc:\t2.9734 \n",
      "\n",
      "val loss:\t0.4096 | val sum of auc:\t2.1640 \n",
      "\n",
      "Epoch 38/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0423 | train sum of auc:\t2.9953 \n",
      "\n",
      "val loss:\t0.4943 | val sum of auc:\t1.9332 \n",
      "\n",
      "Epoch 39/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.0555 | train sum of auc:\t2.9502 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.8483 \n",
      "\n",
      "EARLY STOPPING 38\n",
      "Epoch 40/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/1xtcv4sb\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/1xtcv4sb</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.4822 | train sum of auc:\t1.3201 \n",
      "\n",
      "val loss:\t0.4774 | val sum of auc:\t1.2688 \n",
      "\n",
      "Epoch 2/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.4259 | train sum of auc:\t1.7148 \n",
      "\n",
      "val loss:\t0.4978 | val sum of auc:\t1.5948 \n",
      "\n",
      "Epoch 3/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3257 | train sum of auc:\t2.3207 \n",
      "\n",
      "val loss:\t0.5284 | val sum of auc:\t1.2103 \n",
      "\n",
      "Epoch 4/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3180 | train sum of auc:\t2.4111 \n",
      "\n",
      "val loss:\t0.5911 | val sum of auc:\t0.9938 \n",
      "\n",
      "Epoch 5/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2691 | train sum of auc:\t2.6707 \n",
      "\n",
      "val loss:\t0.5834 | val sum of auc:\t1.5667 \n",
      "\n",
      "Epoch 6/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2746 | train sum of auc:\t2.5233 \n",
      "\n",
      "val loss:\t0.6854 | val sum of auc:\t1.9880 \n",
      "\n",
      "Epoch 7/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2720 | train sum of auc:\t2.5538 \n",
      "\n",
      "val loss:\t0.4761 | val sum of auc:\t1.9670 \n",
      "\n",
      "Epoch 8/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2514 | train sum of auc:\t2.5618 \n",
      "\n",
      "val loss:\t0.4489 | val sum of auc:\t1.9423 \n",
      "\n",
      "Epoch 9/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2328 | train sum of auc:\t2.6984 \n",
      "\n",
      "val loss:\t0.5067 | val sum of auc:\t1.7420 \n",
      "\n",
      "Epoch 10/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2007 | train sum of auc:\t2.7485 \n",
      "\n",
      "val loss:\t0.5436 | val sum of auc:\t1.6895 \n",
      "\n",
      "Epoch 11/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2257 | train sum of auc:\t2.6897 \n",
      "\n",
      "val loss:\t0.8588 | val sum of auc:\t1.6877 \n",
      "\n",
      "Epoch 12/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1866 | train sum of auc:\t2.8330 \n",
      "\n",
      "val loss:\t1.0860 | val sum of auc:\t1.6447 \n",
      "\n",
      "Epoch 13/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1922 | train sum of auc:\t2.7880 \n",
      "\n",
      "val loss:\t0.7496 | val sum of auc:\t1.5048 \n",
      "\n",
      "Epoch 14/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1615 | train sum of auc:\t2.8799 \n",
      "\n",
      "val loss:\t0.7183 | val sum of auc:\t1.3621 \n",
      "\n",
      "Epoch 15/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1617 | train sum of auc:\t2.8770 \n",
      "\n",
      "val loss:\t0.6872 | val sum of auc:\t1.3791 \n",
      "\n",
      "Epoch 16/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1684 | train sum of auc:\t2.8522 \n",
      "\n",
      "val loss:\t0.7409 | val sum of auc:\t1.4972 \n",
      "\n",
      "Epoch 17/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1474 | train sum of auc:\t2.8426 \n",
      "\n",
      "val loss:\t0.8763 | val sum of auc:\t1.6293 \n",
      "\n",
      "Epoch 18/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1494 | train sum of auc:\t2.8771 \n",
      "\n",
      "val loss:\t0.7929 | val sum of auc:\t1.6809 \n",
      "\n",
      "Epoch 19/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2159 | train sum of auc:\t2.7452 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.5789 \n",
      "\n",
      "Epoch 20/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1335 | train sum of auc:\t2.9179 \n",
      "\n",
      "val loss:\t0.7557 | val sum of auc:\t1.6017 \n",
      "\n",
      "Epoch 21/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1124 | train sum of auc:\t2.9486 \n",
      "\n",
      "val loss:\t0.7398 | val sum of auc:\t1.6998 \n",
      "\n",
      "Epoch 22/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1399 | train sum of auc:\t2.9219 \n",
      "\n",
      "val loss:\t0.6026 | val sum of auc:\t1.6777 \n",
      "\n",
      "Epoch 23/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1492 | train sum of auc:\t2.9174 \n",
      "\n",
      "val loss:\t0.6338 | val sum of auc:\t1.5828 \n",
      "\n",
      "Epoch 24/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1200 | train sum of auc:\t2.9525 \n",
      "\n",
      "val loss:\t0.6525 | val sum of auc:\t1.5847 \n",
      "\n",
      "Epoch 25/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1094 | train sum of auc:\t2.9575 \n",
      "\n",
      "val loss:\t0.6322 | val sum of auc:\t1.7073 \n",
      "\n",
      "Epoch 26/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1118 | train sum of auc:\t2.9502 \n",
      "\n",
      "val loss:\t0.6601 | val sum of auc:\t1.6996 \n",
      "\n",
      "Epoch 27/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0811 | train sum of auc:\t2.9738 \n",
      "\n",
      "val loss:\t0.6705 | val sum of auc:\t1.6925 \n",
      "\n",
      "Epoch 28/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1021 | train sum of auc:\t2.9517 \n",
      "\n",
      "val loss:\t0.6697 | val sum of auc:\t1.6293 \n",
      "\n",
      "Epoch 29/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0839 | train sum of auc:\t2.9599 \n",
      "\n",
      "val loss:\t0.6938 | val sum of auc:\t1.7057 \n",
      "\n",
      "Epoch 30/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1070 | train sum of auc:\t2.9563 \n",
      "\n",
      "val loss:\t0.7338 | val sum of auc:\t1.7879 \n",
      "\n",
      "Epoch 31/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0647 | train sum of auc:\t2.9856 \n",
      "\n",
      "{'train_loss': 0.06469738760786335, 'train_Reflux_auroc': 0.9861442904921165, 'train_Reflux_auprc': 0.9863586372513291, 'train_sum_auroc': 2.985563570399201, 'train_sum_auprc': 2.9815967324894244, 'train_Surgery_auroc': 0.9994192799070848, 'train_Surgery_auprc': 0.995238095238095, 'train_Function_auroc': 1.0, 'train_Function_auprc': 1.0, 'val_loss': 0.852808389641823, 'val_Reflux_auroc': 0.5585016835016835, 'val_Reflux_auprc': 0.5875837823302694, 'val_sum_auroc': 1.7439103924398043, 'val_sum_auprc': 1.5204200352922754, 'val_Surgery_auroc': 0.5672268907563025, 'val_Surgery_auprc': 0.12225047542370905, 'val_Function_auroc': 0.6181818181818182, 'val_Function_auprc': 0.8105857775382971, 'best_epoch': 30}\n",
      "val loss:\t0.8528 | val sum of auc:\t1.7439 \n",
      "\n",
      "Epoch 32/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0658 | train sum of auc:\t2.9813 \n",
      "\n",
      "{'train_loss': 0.06583334769319443, 'train_Reflux_auroc': 0.9964166268514094, 'train_Reflux_auprc': 0.9933891343098503, 'train_sum_auroc': 2.9813179044356137, 'train_sum_auprc': 2.946960562881279, 'train_Surgery_auroc': 0.9849012775842044, 'train_Surgery_auprc': 0.9535714285714286, 'train_Function_auroc': 1.0, 'train_Function_auprc': 1.0, 'val_loss': 0.8476587364433008, 'val_Reflux_auroc': 0.519023569023569, 'val_Reflux_auprc': 0.5712453567121427, 'val_sum_auroc': 1.669774778598308, 'val_sum_auprc': 1.522630330503202, 'val_Surgery_auroc': 0.5371148459383753, 'val_Surgery_auprc': 0.12537886305712231, 'val_Function_auroc': 0.6136363636363636, 'val_Function_auprc': 0.8260061107339371, 'best_epoch': 31}\n",
      "val loss:\t0.8477 | val sum of auc:\t1.6698 \n",
      "\n",
      "Epoch 33/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0893 | train sum of auc:\t2.9627 \n",
      "\n",
      "{'train_loss': 0.08930148108162149, 'train_Reflux_auroc': 0.9789775441949355, 'train_Reflux_auprc': 0.9651103674105073, 'train_sum_auroc': 2.96266458885759, 'train_sum_auprc': 2.928404018204158, 'train_Surgery_auroc': 0.9988385598141696, 'train_Surgery_auprc': 0.9910714285714286, 'train_Function_auroc': 0.9848484848484849, 'train_Function_auprc': 0.9722222222222223, 'val_loss': 0.7425849585358156, 'val_Reflux_auroc': 0.5819023569023568, 'val_Reflux_auprc': 0.5477429337949327, 'val_sum_auroc': 1.7123772741419798, 'val_sum_auprc': 1.449077857310443, 'val_Surgery_auroc': 0.5395658263305322, 'val_Surgery_auprc': 0.10849303650661703, 'val_Function_auroc': 0.5909090909090908, 'val_Function_auprc': 0.7928418870088934, 'best_epoch': 32}\n",
      "val loss:\t0.7426 | val sum of auc:\t1.7124 \n",
      "\n",
      "Epoch 34/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0810 | train sum of auc:\t2.9769 \n",
      "\n",
      "val loss:\t0.7777 | val sum of auc:\t1.6118 \n",
      "\n",
      "Epoch 35/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0547 | train sum of auc:\t2.9938 \n",
      "\n",
      "val loss:\t0.9068 | val sum of auc:\t1.3897 \n",
      "\n",
      "Epoch 36/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0723 | train sum of auc:\t2.9783 \n",
      "\n",
      "val loss:\t1.0627 | val sum of auc:\t1.3601 \n",
      "\n",
      "Epoch 37/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0729 | train sum of auc:\t2.9813 \n",
      "\n",
      "val loss:\t0.9234 | val sum of auc:\t1.5101 \n",
      "\n",
      "Epoch 38/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0744 | train sum of auc:\t2.9722 \n",
      "\n",
      "val loss:\t0.8001 | val sum of auc:\t1.8306 \n",
      "\n",
      "Epoch 39/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.0467 | train sum of auc:\t2.9936 \n",
      "\n",
      "val loss:\t0.8594 | val sum of auc:\t1.9481 \n",
      "\n",
      "Epoch 40/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0580 | train sum of auc:\t2.9814 \n",
      "\n",
      "val loss:\t0.9254 | val sum of auc:\t1.8245 \n",
      "\n",
      "Epoch 41/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0430 | train sum of auc:\t2.9989 \n",
      "\n",
      "val loss:\t0.9475 | val sum of auc:\t1.6343 \n",
      "\n",
      "Epoch 42/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0607 | train sum of auc:\t2.9830 \n",
      "\n",
      "val loss:\t0.8618 | val sum of auc:\t1.5566 \n",
      "\n",
      "Epoch 43/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0516 | train sum of auc:\t2.9926 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.5856 \n",
      "\n",
      "EARLY STOPPING 42\n",
      "Epoch 44/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/5ijvyoln\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/5ijvyoln</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.4808 | train sum of auc:\t1.5402 \n",
      "\n",
      "val loss:\t0.5198 | val sum of auc:\t1.6762 \n",
      "\n",
      "Epoch 2/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3642 | train sum of auc:\t2.0317 \n",
      "\n",
      "val loss:\t0.6644 | val sum of auc:\t2.2551 \n",
      "\n",
      "Epoch 3/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3185 | train sum of auc:\t2.3871 \n",
      "\n",
      "val loss:\t1.3200 | val sum of auc:\t2.1677 \n",
      "\n",
      "Epoch 4/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3421 | train sum of auc:\t2.1386 \n",
      "\n",
      "val loss:\t1.1403 | val sum of auc:\t1.9038 \n",
      "\n",
      "Epoch 5/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3292 | train sum of auc:\t2.3320 \n",
      "\n",
      "val loss:\t1.2921 | val sum of auc:\t1.8083 \n",
      "\n",
      "Epoch 6/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2839 | train sum of auc:\t2.4881 \n",
      "\n",
      "val loss:\t0.8368 | val sum of auc:\t1.7560 \n",
      "\n",
      "Epoch 7/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1994 | train sum of auc:\t2.7585 \n",
      "\n",
      "val loss:\t1.0917 | val sum of auc:\t1.6651 \n",
      "\n",
      "Epoch 8/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2427 | train sum of auc:\t2.6486 \n",
      "\n",
      "val loss:\t1.1216 | val sum of auc:\t1.6495 \n",
      "\n",
      "Epoch 9/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2358 | train sum of auc:\t2.6109 \n",
      "\n",
      "val loss:\t0.9844 | val sum of auc:\t1.7511 \n",
      "\n",
      "Epoch 10/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2280 | train sum of auc:\t2.7300 \n",
      "\n",
      "val loss:\t0.6879 | val sum of auc:\t1.6843 \n",
      "\n",
      "Epoch 11/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2094 | train sum of auc:\t2.6995 \n",
      "\n",
      "val loss:\t0.7434 | val sum of auc:\t1.6820 \n",
      "\n",
      "Epoch 12/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1616 | train sum of auc:\t2.8942 \n",
      "\n",
      "val loss:\t0.7292 | val sum of auc:\t1.6785 \n",
      "\n",
      "Epoch 13/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1589 | train sum of auc:\t2.8885 \n",
      "\n",
      "val loss:\t0.6950 | val sum of auc:\t1.7028 \n",
      "\n",
      "Epoch 14/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1966 | train sum of auc:\t2.7488 \n",
      "\n",
      "val loss:\t0.7056 | val sum of auc:\t1.8318 \n",
      "\n",
      "Epoch 15/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1405 | train sum of auc:\t2.9237 \n",
      "\n",
      "val loss:\t0.6612 | val sum of auc:\t1.8926 \n",
      "\n",
      "Epoch 16/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1607 | train sum of auc:\t2.8176 \n",
      "\n",
      "val loss:\t0.6675 | val sum of auc:\t1.7578 \n",
      "\n",
      "Epoch 17/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1631 | train sum of auc:\t2.8567 \n",
      "\n",
      "val loss:\t0.6795 | val sum of auc:\t1.7523 \n",
      "\n",
      "Epoch 18/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1247 | train sum of auc:\t2.9061 \n",
      "\n",
      "val loss:\t0.6544 | val sum of auc:\t1.8850 \n",
      "\n",
      "Epoch 19/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1351 | train sum of auc:\t2.9334 \n",
      "\n",
      "val loss:\t0.6137 | val sum of auc:\t1.6821 \n",
      "\n",
      "Epoch 20/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1214 | train sum of auc:\t2.9290 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.5258 \n",
      "\n",
      "Epoch 21/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1275 | train sum of auc:\t2.9026 \n",
      "\n",
      "val loss:\t0.4807 | val sum of auc:\t1.6133 \n",
      "\n",
      "Epoch 22/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1209 | train sum of auc:\t2.9000 \n",
      "\n",
      "val loss:\t0.5597 | val sum of auc:\t1.5542 \n",
      "\n",
      "Epoch 23/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1466 | train sum of auc:\t2.8361 \n",
      "\n",
      "val loss:\t0.6262 | val sum of auc:\t1.4423 \n",
      "\n",
      "Epoch 24/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1087 | train sum of auc:\t2.9569 \n",
      "\n",
      "val loss:\t0.5891 | val sum of auc:\t1.4591 \n",
      "\n",
      "Epoch 25/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1415 | train sum of auc:\t2.9074 \n",
      "\n",
      "val loss:\t0.5398 | val sum of auc:\t1.3952 \n",
      "\n",
      "Epoch 26/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1127 | train sum of auc:\t2.9598 \n",
      "\n",
      "val loss:\t0.6219 | val sum of auc:\t1.4060 \n",
      "\n",
      "Epoch 27/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1009 | train sum of auc:\t2.9491 \n",
      "\n",
      "val loss:\t0.7489 | val sum of auc:\t1.7473 \n",
      "\n",
      "Epoch 28/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0837 | train sum of auc:\t2.9441 \n",
      "\n",
      "val loss:\t1.5937 | val sum of auc:\t1.7674 \n",
      "\n",
      "Epoch 29/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1135 | train sum of auc:\t2.9517 \n",
      "\n",
      "val loss:\t1.3002 | val sum of auc:\t1.6289 \n",
      "\n",
      "Epoch 30/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0846 | train sum of auc:\t2.9728 \n",
      "\n",
      "val loss:\t1.0091 | val sum of auc:\t1.5641 \n",
      "\n",
      "Epoch 31/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0751 | train sum of auc:\t2.9807 \n",
      "\n",
      "{'train_loss': 0.0750721863094177, 'train_Reflux_auroc': 0.982967032967033, 'train_Reflux_auprc': 0.9687320094710068, 'train_sum_auroc': 2.9806943056943056, 'train_sum_auprc': 2.949251489990487, 'train_Surgery_auroc': 0.9977272727272727, 'train_Surgery_auprc': 0.9805194805194806, 'train_Function_auroc': 1.0, 'train_Function_auprc': 0.9999999999999998, 'val_loss': 0.8356112682707257, 'val_Reflux_auroc': 0.6016326530612245, 'val_Reflux_auprc': 0.13195321795441883, 'val_sum_auroc': 1.5363774209957115, 'val_sum_auprc': 0.5491650662203948, 'val_Surgery_auroc': 0.6625075711689885, 'val_Surgery_auprc': 0.3296497364647337, 'val_Function_auroc': 0.2722371967654986, 'val_Function_auprc': 0.08756211180124224, 'best_epoch': 30}\n",
      "val loss:\t0.8356 | val sum of auc:\t1.5364 \n",
      "\n",
      "Epoch 32/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0725 | train sum of auc:\t2.9681 \n",
      "\n",
      "val loss:\t0.8585 | val sum of auc:\t1.5201 \n",
      "\n",
      "Epoch 33/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0836 | train sum of auc:\t2.9504 \n",
      "\n",
      "val loss:\t0.9121 | val sum of auc:\t1.5647 \n",
      "\n",
      "Epoch 34/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0601 | train sum of auc:\t2.9874 \n",
      "\n",
      "val loss:\t0.8634 | val sum of auc:\t1.5023 \n",
      "\n",
      "Epoch 35/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0573 | train sum of auc:\t2.9892 \n",
      "\n",
      "{'train_loss': 0.057315992802836516, 'train_Reflux_auroc': 0.9914835164835165, 'train_Reflux_auprc': 0.9832667929765453, 'train_sum_auroc': 2.989210789210789, 'train_sum_auprc': 2.9637862734960256, 'train_Surgery_auroc': 0.9977272727272727, 'train_Surgery_auprc': 0.9805194805194806, 'train_Function_auroc': 1.0, 'train_Function_auprc': 0.9999999999999998, 'val_loss': 0.7520481073370547, 'val_Reflux_auroc': 0.44789115646258504, 'val_Reflux_auprc': 0.07476932856075713, 'val_sum_auroc': 1.3389316269117613, 'val_sum_auprc': 0.5467409827661381, 'val_Surgery_auroc': 0.7347062386432466, 'val_Surgery_auprc': 0.3942918490837154, 'val_Function_auroc': 0.15633423180592987, 'val_Function_auprc': 0.07767980512166558, 'best_epoch': 34}\n",
      "val loss:\t0.7520 | val sum of auc:\t1.3389 \n",
      "\n",
      "Epoch 36/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0670 | train sum of auc:\t2.9784 \n",
      "\n",
      "{'train_loss': 0.0669619694693398, 'train_Reflux_auroc': 0.9920329670329671, 'train_Reflux_auprc': 0.9838833637632598, 'train_sum_auroc': 2.9783966033966034, 'train_sum_auprc': 2.899912071897231, 'train_Surgery_auroc': 0.9863636363636364, 'train_Surgery_auprc': 0.9160287081339713, 'train_Function_auroc': 1.0, 'train_Function_auprc': 0.9999999999999998, 'val_loss': 0.6230850735428192, 'val_Reflux_auroc': 0.41006802721088437, 'val_Reflux_auprc': 0.06818891385697459, 'val_sum_auroc': 1.2931138329873395, 'val_sum_auprc': 0.5110660733504924, 'val_Surgery_auroc': 0.7105390672319806, 'val_Surgery_auprc': 0.363893449081755, 'val_Function_auroc': 0.1725067385444744, 'val_Function_auprc': 0.07898371041176283, 'best_epoch': 35}\n",
      "val loss:\t0.6231 | val sum of auc:\t1.2931 \n",
      "\n",
      "Epoch 37/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0455 | train sum of auc:\t2.9970 \n",
      "\n",
      "{'train_loss': 0.04554394342517125, 'train_Reflux_auroc': 0.9978021978021978, 'train_Reflux_auprc': 0.9951711792899064, 'train_sum_auroc': 2.997044622044622, 'train_sum_auprc': 2.9875954217141487, 'train_Surgery_auroc': 0.9992424242424243, 'train_Surgery_auprc': 0.9924242424242424, 'train_Function_auroc': 1.0, 'train_Function_auprc': 0.9999999999999998, 'val_loss': 0.588183392364777, 'val_Reflux_auroc': 0.4253061224489796, 'val_Reflux_auprc': 0.07081998263482699, 'val_sum_auroc': 1.2896970576169164, 'val_sum_auprc': 0.49085189552774705, 'val_Surgery_auroc': 0.6757116898849181, 'val_Surgery_auprc': 0.3402740767441065, 'val_Function_auroc': 0.18867924528301885, 'val_Function_auprc': 0.07975783614881359, 'best_epoch': 36}\n",
      "val loss:\t0.5882 | val sum of auc:\t1.2897 \n",
      "\n",
      "Epoch 38/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.0479 | train sum of auc:\t2.9942 \n",
      "\n",
      "val loss:\t0.6373 | val sum of auc:\t1.2584 \n",
      "\n",
      "Epoch 39/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0597 | train sum of auc:\t2.9863 \n",
      "\n",
      "val loss:\t0.7135 | val sum of auc:\t1.2514 \n",
      "\n",
      "Epoch 40/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0716 | train sum of auc:\t2.9484 \n",
      "\n",
      "val loss:\t0.7194 | val sum of auc:\t1.2420 \n",
      "\n",
      "Epoch 41/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0475 | train sum of auc:\t2.9818 \n",
      "\n",
      "val loss:\t0.7171 | val sum of auc:\t1.2310 \n",
      "\n",
      "Epoch 42/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0419 | train sum of auc:\t2.9948 \n",
      "\n",
      "val loss:\t0.6562 | val sum of auc:\t1.2029 \n",
      "\n",
      "Epoch 43/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0593 | train sum of auc:\t2.9544 \n",
      "\n",
      "{'train_loss': 0.05931912550261913, 'train_Reflux_auroc': 0.9928571428571429, 'train_Reflux_auprc': 0.9788250151229568, 'train_sum_auroc': 2.9543956043956046, 'train_sum_auprc': 2.9310599596655935, 'train_Surgery_auroc': 1.0, 'train_Surgery_auprc': 1.0, 'train_Function_auroc': 0.9615384615384616, 'train_Function_auprc': 0.9522349445426366, 'val_loss': 0.5198361362596291, 'val_Reflux_auroc': 0.3574149659863946, 'val_Reflux_auprc': 0.09837113985012508, 'val_sum_auroc': 1.2963680794306685, 'val_sum_auprc': 0.5870008668513661, 'val_Surgery_auroc': 0.7071471835251363, 'val_Surgery_auprc': 0.4047828438909345, 'val_Function_auroc': 0.23180592991913743, 'val_Function_auprc': 0.08384688311030651, 'best_epoch': 42}\n",
      "val loss:\t0.5198 | val sum of auc:\t1.2964 \n",
      "\n",
      "Epoch 44/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0351 | train sum of auc:\t2.9981 \n",
      "\n",
      "{'train_loss': 0.03513135709835373, 'train_Reflux_auroc': 0.9980769230769231, 'train_Reflux_auprc': 0.9958290740267485, 'train_sum_auroc': 2.998076923076923, 'train_sum_auprc': 2.995829074026748, 'train_Surgery_auroc': 1.0, 'train_Surgery_auprc': 1.0, 'train_Function_auroc': 1.0, 'train_Function_auprc': 0.9999999999999998, 'val_loss': 0.4833229993204338, 'val_Reflux_auroc': 0.395374149659864, 'val_Reflux_auprc': 0.08207257501442769, 'val_sum_auroc': 1.4806677151049672, 'val_sum_auprc': 0.5809148493415413, 'val_Surgery_auroc': 0.691762568140521, 'val_Surgery_auprc': 0.39563119514995837, 'val_Function_auroc': 0.3935309973045822, 'val_Function_auprc': 0.10321107917715526, 'best_epoch': 43}\n",
      "val loss:\t0.4833 | val sum of auc:\t1.4807 \n",
      "\n",
      "Epoch 45/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0436 | train sum of auc:\t2.9948 \n",
      "\n",
      "{'train_loss': 0.04361704313231789, 'train_Reflux_auroc': 0.9947802197802198, 'train_Reflux_auprc': 0.9891839279621234, 'train_sum_auroc': 2.99478021978022, 'train_sum_auprc': 2.989183927962123, 'train_Surgery_auroc': 1.0, 'train_Surgery_auprc': 1.0, 'train_Function_auroc': 1.0, 'train_Function_auprc': 0.9999999999999998, 'val_loss': 0.47985402048568365, 'val_Reflux_auroc': 0.4461224489795918, 'val_Reflux_auprc': 0.07919283630011292, 'val_sum_auroc': 1.503733861486265, 'val_sum_auprc': 0.6312801012048906, 'val_Surgery_auroc': 0.728770442156269, 'val_Surgery_auprc': 0.4559092669780973, 'val_Function_auroc': 0.32884097035040427, 'val_Function_auprc': 0.09617799792668044, 'best_epoch': 44}\n",
      "val loss:\t0.4799 | val sum of auc:\t1.5037 \n",
      "\n",
      "Epoch 46/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0571 | train sum of auc:\t2.9883 \n",
      "\n",
      "val loss:\t0.6378 | val sum of auc:\t1.5098 \n",
      "\n",
      "Epoch 47/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0515 | train sum of auc:\t2.9879 \n",
      "\n",
      "val loss:\t0.7912 | val sum of auc:\t1.4387 \n",
      "\n",
      "Epoch 48/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0361 | train sum of auc:\t2.9959 \n",
      "\n",
      "val loss:\t1.1027 | val sum of auc:\t1.3754 \n",
      "\n",
      "Epoch 49/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0217 | train sum of auc:\t2.9986 \n",
      "\n",
      "val loss:\t1.1746 | val sum of auc:\t1.3819 \n",
      "\n",
      "Epoch 50/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0518 | train sum of auc:\t2.9755 \n",
      "\n",
      "val loss:\t0.9606 | val sum of auc:\t1.3675 \n",
      "\n",
      "Epoch 51/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0345 | train sum of auc:\t2.9968 \n",
      "\n",
      "val loss:\t0.7895 | val sum of auc:\t1.2687 \n",
      "\n",
      "Epoch 52/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0292 | train sum of auc:\t2.9973 \n",
      "\n",
      "val loss:\t0.7625 | val sum of auc:\t1.1693 \n",
      "\n",
      "Epoch 53/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0488 | train sum of auc:\t2.9913 \n",
      "\n",
      "val loss:\t0.7464 | val sum of auc:\t1.1990 \n",
      "\n",
      "Epoch 54/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0441 | train sum of auc:\t2.9891 \n",
      "\n",
      "val loss:\t0.7112 | val sum of auc:\t1.4238 \n",
      "\n",
      "Epoch 55/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0387 | train sum of auc:\t2.9931 \n",
      "\n",
      "val loss:\t0.7701 | val sum of auc:\t1.6096 \n",
      "\n",
      "Epoch 56/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0231 | train sum of auc:\t2.9989 \n",
      "\n",
      "val loss:\t0.6775 | val sum of auc:\t1.8521 \n",
      "\n",
      "Epoch 57/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0371 | train sum of auc:\t2.9973 \n",
      "\n",
      "val loss:\t0.5894 | val sum of auc:\t1.9383 \n",
      "\n",
      "Epoch 58/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0277 | train sum of auc:\t2.9964 \n",
      "\n",
      "val loss:\t0.6238 | val sum of auc:\t1.8993 \n",
      "\n",
      "Epoch 59/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0651 | train sum of auc:\t2.9736 \n",
      "\n",
      "val loss:\t0.8046 | val sum of auc:\t1.6962 \n",
      "\n",
      "Epoch 60/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0598 | train sum of auc:\t2.9615 \n",
      "\n",
      "val loss:\t1.2581 | val sum of auc:\t1.6415 \n",
      "\n",
      "Epoch 61/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0389 | train sum of auc:\t2.9913 \n",
      "\n",
      "val loss:\t1.9244 | val sum of auc:\t1.7017 \n",
      "\n",
      "Epoch 62/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0663 | train sum of auc:\t2.9801 \n",
      "\n",
      "val loss:\t1.7923 | val sum of auc:\t1.7151 \n",
      "\n",
      "Epoch 63/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0346 | train sum of auc:\t2.9984 \n",
      "\n",
      "val loss:\t0.9186 | val sum of auc:\t1.6237 \n",
      "\n",
      "Epoch 64/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0228 | train sum of auc:\t2.9992 \n",
      "\n",
      "val loss:\t0.8615 | val sum of auc:\t1.6860 \n",
      "\n",
      "Epoch 65/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0306 | train sum of auc:\t2.9981 \n",
      "\n",
      "val loss:\t0.8448 | val sum of auc:\t1.7917 \n",
      "\n",
      "EARLY STOPPING 64\n",
      "Epoch 66/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/cf744tee\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/cf744tee</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.4744 | train sum of auc:\t1.5312 \n",
      "\n",
      "val loss:\t0.6764 | val sum of auc:\t1.4343 \n",
      "\n",
      "Epoch 2/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3911 | train sum of auc:\t1.9609 \n",
      "\n",
      "val loss:\t0.6059 | val sum of auc:\t1.5102 \n",
      "\n",
      "Epoch 3/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3261 | train sum of auc:\t2.2942 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.5509 \n",
      "\n",
      "Epoch 4/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2642 | train sum of auc:\t2.5094 \n",
      "\n",
      "val loss:\t0.6475 | val sum of auc:\t1.5715 \n",
      "\n",
      "Epoch 5/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2566 | train sum of auc:\t2.6315 \n",
      "\n",
      "val loss:\t0.5959 | val sum of auc:\t1.5447 \n",
      "\n",
      "Epoch 6/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2499 | train sum of auc:\t2.6773 \n",
      "\n",
      "val loss:\t0.6451 | val sum of auc:\t1.6271 \n",
      "\n",
      "Epoch 7/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2167 | train sum of auc:\t2.6784 \n",
      "\n",
      "val loss:\t0.6756 | val sum of auc:\t1.6364 \n",
      "\n",
      "Epoch 8/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2068 | train sum of auc:\t2.6649 \n",
      "\n",
      "val loss:\t0.6603 | val sum of auc:\t1.6744 \n",
      "\n",
      "Epoch 9/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2031 | train sum of auc:\t2.7543 \n",
      "\n",
      "val loss:\t0.7585 | val sum of auc:\t1.7760 \n",
      "\n",
      "Epoch 10/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2248 | train sum of auc:\t2.6554 \n",
      "\n",
      "val loss:\t0.8938 | val sum of auc:\t1.7239 \n",
      "\n",
      "Epoch 11/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1847 | train sum of auc:\t2.7964 \n",
      "\n",
      "val loss:\t0.8252 | val sum of auc:\t1.6468 \n",
      "\n",
      "Epoch 12/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1609 | train sum of auc:\t2.8561 \n",
      "\n",
      "val loss:\t0.8143 | val sum of auc:\t1.5144 \n",
      "\n",
      "Epoch 13/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1722 | train sum of auc:\t2.7522 \n",
      "\n",
      "val loss:\t1.2439 | val sum of auc:\t1.3158 \n",
      "\n",
      "Epoch 14/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1474 | train sum of auc:\t2.8336 \n",
      "\n",
      "val loss:\t1.1696 | val sum of auc:\t1.2686 \n",
      "\n",
      "Epoch 15/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1845 | train sum of auc:\t2.8490 \n",
      "\n",
      "val loss:\t0.8065 | val sum of auc:\t1.3606 \n",
      "\n",
      "Epoch 16/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2008 | train sum of auc:\t2.7823 \n",
      "\n",
      "val loss:\t0.8767 | val sum of auc:\t1.5436 \n",
      "\n",
      "Epoch 17/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1408 | train sum of auc:\t2.8860 \n",
      "\n",
      "val loss:\t0.8064 | val sum of auc:\t1.6212 \n",
      "\n",
      "Epoch 18/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1180 | train sum of auc:\t2.9483 \n",
      "\n",
      "val loss:\t0.8882 | val sum of auc:\t1.3807 \n",
      "\n",
      "Epoch 19/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1439 | train sum of auc:\t2.9140 \n",
      "\n",
      "val loss:\t1.0656 | val sum of auc:\t1.2691 \n",
      "\n",
      "Epoch 20/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0994 | train sum of auc:\t2.9575 \n",
      "\n",
      "val loss:\t0.9485 | val sum of auc:\t1.3957 \n",
      "\n",
      "Epoch 21/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1232 | train sum of auc:\t2.9216 \n",
      "\n",
      "val loss:\t0.7932 | val sum of auc:\t1.5421 \n",
      "\n",
      "Epoch 22/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1022 | train sum of auc:\t2.9402 \n",
      "\n",
      "val loss:\t0.8697 | val sum of auc:\t1.6154 \n",
      "\n",
      "Epoch 23/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0890 | train sum of auc:\t2.9619 \n",
      "\n",
      "val loss:\t0.8961 | val sum of auc:\t1.6328 \n",
      "\n",
      "Epoch 24/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1074 | train sum of auc:\t2.9577 \n",
      "\n",
      "val loss:\t0.7633 | val sum of auc:\t1.5605 \n",
      "\n",
      "Epoch 25/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.1144 | train sum of auc:\t2.9353 \n",
      "\n",
      "val loss:\t0.7732 | val sum of auc:\t1.5314 \n",
      "\n",
      "Epoch 26/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0801 | train sum of auc:\t2.9579 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.5400 \n",
      "\n",
      "Epoch 27/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0864 | train sum of auc:\t2.9709 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.7296 \n",
      "\n",
      "Epoch 28/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0805 | train sum of auc:\t2.9550 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.7282 \n",
      "\n",
      "Epoch 29/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0709 | train sum of auc:\t2.9709 \n",
      "\n",
      "val loss:\t0.7371 | val sum of auc:\t1.6357 \n",
      "\n",
      "Epoch 30/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0887 | train sum of auc:\t2.9267 \n",
      "\n",
      "val loss:\t0.7942 | val sum of auc:\t1.4454 \n",
      "\n",
      "Epoch 31/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0618 | train sum of auc:\t2.9870 \n",
      "\n",
      "{'train_loss': 0.061756184463625526, 'train_Reflux_auroc': 0.987772071411103, 'train_Reflux_auprc': 0.9824175762331575, 'train_sum_auroc': 2.9869656197982, 'train_sum_auprc': 2.9733266671422482, 'train_Surgery_auroc': 0.9991935483870967, 'train_Surgery_auprc': 0.9909090909090909, 'train_Function_auroc': 1.0, 'train_Function_auprc': 1.0, 'val_loss': 0.803889924758359, 'val_Reflux_auroc': 0.42133674266638255, 'val_Reflux_auprc': 0.1828555854802438, 'val_sum_auroc': 1.415830077014146, 'val_sum_auprc': 0.703515165918236, 'val_Surgery_auroc': 0.5885132679025808, 'val_Surgery_auprc': 0.19385789299180267, 'val_Function_auroc': 0.4059800664451827, 'val_Function_auprc': 0.32680168744618954, 'best_epoch': 30}\n",
      "val loss:\t0.8039 | val sum of auc:\t1.4158 \n",
      "\n",
      "Epoch 32/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0649 | train sum of auc:\t2.9748 \n",
      "\n",
      "val loss:\t0.8691 | val sum of auc:\t1.3737 \n",
      "\n",
      "Epoch 33/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.0492 | train sum of auc:\t2.9944 \n",
      "\n",
      "val loss:\tnan | val sum of auc:\t1.3294 \n",
      "\n",
      "EARLY STOPPING 32\n",
      "Epoch 34/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/2uzdcm5f\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/2uzdcm5f</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/d8920rtq\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/d8920rtq</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.5626 | train sum of auc:\t1.5053 \n",
      "\n",
      "val loss:\t0.4659 | val sum of auc:\t1.8125 \n",
      "\n",
      "Epoch 2/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.4110 | train sum of auc:\t1.5594 \n",
      "\n",
      "val loss:\t0.4420 | val sum of auc:\t1.8048 \n",
      "\n",
      "Epoch 3/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3646 | train sum of auc:\t1.4928 \n",
      "\n",
      "val loss:\t0.4364 | val sum of auc:\t1.7116 \n",
      "\n",
      "Epoch 4/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3570 | train sum of auc:\t1.7569 \n",
      "\n",
      "val loss:\t0.4429 | val sum of auc:\t1.6220 \n",
      "\n",
      "Epoch 5/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3660 | train sum of auc:\t1.8309 \n",
      "\n",
      "val loss:\t0.4524 | val sum of auc:\t1.5912 \n",
      "\n",
      "Epoch 6/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3843 | train sum of auc:\t1.6321 \n",
      "\n",
      "val loss:\t0.4642 | val sum of auc:\t1.6718 \n",
      "\n",
      "Epoch 7/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3739 | train sum of auc:\t1.9219 \n",
      "\n",
      "val loss:\t0.4733 | val sum of auc:\t1.6673 \n",
      "\n",
      "Epoch 8/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3824 | train sum of auc:\t1.5368 \n",
      "\n",
      "val loss:\t0.4776 | val sum of auc:\t1.5976 \n",
      "\n",
      "Epoch 9/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3545 | train sum of auc:\t1.8529 \n",
      "\n",
      "val loss:\t0.4763 | val sum of auc:\t1.4992 \n",
      "\n",
      "Epoch 10/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3597 | train sum of auc:\t1.7340 \n",
      "\n",
      "val loss:\t0.4740 | val sum of auc:\t1.4148 \n",
      "\n",
      "Epoch 11/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3668 | train sum of auc:\t1.6125 \n",
      "\n",
      "val loss:\t0.4712 | val sum of auc:\t1.3540 \n",
      "\n",
      "Epoch 12/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3607 | train sum of auc:\t1.6906 \n",
      "\n",
      "val loss:\t0.4704 | val sum of auc:\t1.2821 \n",
      "\n",
      "Epoch 13/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3598 | train sum of auc:\t1.6688 \n",
      "\n",
      "val loss:\t0.4689 | val sum of auc:\t1.2432 \n",
      "\n",
      "Epoch 14/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3445 | train sum of auc:\t1.7517 \n",
      "\n",
      "val loss:\t0.4689 | val sum of auc:\t1.2125 \n",
      "\n",
      "Epoch 15/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3454 | train sum of auc:\t1.7448 \n",
      "\n",
      "val loss:\t0.4705 | val sum of auc:\t1.2013 \n",
      "\n",
      "Epoch 16/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3323 | train sum of auc:\t1.9065 \n",
      "\n",
      "val loss:\t0.4771 | val sum of auc:\t1.2167 \n",
      "\n",
      "Epoch 17/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3518 | train sum of auc:\t1.7448 \n",
      "\n",
      "val loss:\t0.4861 | val sum of auc:\t1.1848 \n",
      "\n",
      "Epoch 18/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3589 | train sum of auc:\t1.6776 \n",
      "\n",
      "val loss:\t0.4893 | val sum of auc:\t1.1991 \n",
      "\n",
      "Epoch 19/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3318 | train sum of auc:\t1.9306 \n",
      "\n",
      "val loss:\t0.4862 | val sum of auc:\t1.2575 \n",
      "\n",
      "Epoch 20/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3294 | train sum of auc:\t1.8642 \n",
      "\n",
      "val loss:\t0.4835 | val sum of auc:\t1.2758 \n",
      "\n",
      "Epoch 21/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3413 | train sum of auc:\t1.7108 \n",
      "\n",
      "val loss:\t0.4832 | val sum of auc:\t1.3049 \n",
      "\n",
      "Epoch 22/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3281 | train sum of auc:\t2.0898 \n",
      "\n",
      "val loss:\t0.4852 | val sum of auc:\t1.3308 \n",
      "\n",
      "Epoch 23/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3203 | train sum of auc:\t2.1933 \n",
      "\n",
      "val loss:\t0.4886 | val sum of auc:\t1.3536 \n",
      "\n",
      "Epoch 24/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3393 | train sum of auc:\t1.8796 \n",
      "\n",
      "val loss:\t0.4977 | val sum of auc:\t1.3788 \n",
      "\n",
      "Epoch 25/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3312 | train sum of auc:\t2.0653 \n",
      "\n",
      "val loss:\t0.5121 | val sum of auc:\t1.3274 \n",
      "\n",
      "Epoch 26/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3203 | train sum of auc:\t2.1778 \n",
      "\n",
      "val loss:\t0.5240 | val sum of auc:\t1.3152 \n",
      "\n",
      "Epoch 27/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3302 | train sum of auc:\t2.0547 \n",
      "\n",
      "val loss:\t0.5314 | val sum of auc:\t1.3475 \n",
      "\n",
      "Epoch 28/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3214 | train sum of auc:\t2.1572 \n",
      "\n",
      "val loss:\t0.5402 | val sum of auc:\t1.3758 \n",
      "\n",
      "Epoch 29/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3183 | train sum of auc:\t2.1496 \n",
      "\n",
      "val loss:\t0.5447 | val sum of auc:\t1.3799 \n",
      "\n",
      "Epoch 30/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3231 | train sum of auc:\t2.0886 \n",
      "\n",
      "val loss:\t0.5421 | val sum of auc:\t1.3512 \n",
      "\n",
      "Epoch 31/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3230 | train sum of auc:\t2.0974 \n",
      "\n",
      "{'train_loss': 0.323001589573605, 'train_Reflux_auroc': 0.6965250965250965, 'train_Reflux_auprc': 0.5074594350730153, 'train_sum_auroc': 2.0974330308299773, 'train_sum_auprc': 1.278687725095165, 'train_Surgery_auroc': 0.6016655100624566, 'train_Surgery_auprc': 0.09986759662198258, 'train_Function_auroc': 0.7992424242424243, 'train_Function_auprc': 0.6713606934001671, 'val_loss': 0.5396542125354612, 'val_Reflux_auroc': 0.5030364372469636, 'val_Reflux_auprc': 0.4948202832452958, 'val_sum_auroc': 1.3319453689251506, 'val_sum_auprc': 0.7766883214684598, 'val_Surgery_auroc': 0.37404782056707575, 'val_Surgery_auprc': 0.1506356230512786, 'val_Function_auroc': 0.45486111111111116, 'val_Function_auprc': 0.1312324151718854, 'best_epoch': 30}\n",
      "val loss:\t0.5397 | val sum of auc:\t1.3319 \n",
      "\n",
      "Epoch 32/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3111 | train sum of auc:\t2.1312 \n",
      "\n",
      "{'train_loss': 0.3111456783724503, 'train_Reflux_auroc': 0.7853281853281853, 'train_Reflux_auprc': 0.6252205757631355, 'train_sum_auroc': 2.1311817592351945, 'train_sum_auprc': 1.4123160009126385, 'train_Surgery_auroc': 0.6072172102706453, 'train_Surgery_auprc': 0.16043086217201108, 'train_Function_auroc': 0.7386363636363635, 'train_Function_auprc': 0.6266645629774921, 'val_loss': 0.53373637433686, 'val_Reflux_auroc': 0.5063765182186234, 'val_Reflux_auprc': 0.5060309250855002, 'val_sum_auroc': 1.3025822007314367, 'val_sum_auprc': 0.7753804410887227, 'val_Surgery_auroc': 0.3969001269572577, 'val_Surgery_auprc': 0.1546416699120784, 'val_Function_auroc': 0.3993055555555556, 'val_Function_auprc': 0.11470784609114415, 'best_epoch': 31}\n",
      "val loss:\t0.5337 | val sum of auc:\t1.3026 \n",
      "\n",
      "Epoch 33/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3195 | train sum of auc:\t2.1078 \n",
      "\n",
      "{'train_loss': 0.3194728348876389, 'train_Reflux_auroc': 0.6967824967824968, 'train_Reflux_auprc': 0.4616505727655501, 'train_sum_auroc': 2.10775288771472, 'train_sum_auprc': 1.398226595558425, 'train_Surgery_auroc': 0.6571825121443442, 'train_Surgery_auprc': 0.21514893270636465, 'train_Function_auroc': 0.7537878787878788, 'train_Function_auprc': 0.7214270900865104, 'val_loss': 0.5272665549220378, 'val_Reflux_auroc': 0.5084345479082322, 'val_Reflux_auprc': 0.5096588516244877, 'val_sum_auroc': 1.3248857986723266, 'val_sum_auprc': 0.7814921592702743, 'val_Surgery_auroc': 0.4240901396529835, 'val_Surgery_auprc': 0.1610334452640632, 'val_Function_auroc': 0.39236111111111105, 'val_Function_auprc': 0.11079986238172337, 'best_epoch': 32}\n",
      "val loss:\t0.5273 | val sum of auc:\t1.3249 \n",
      "\n",
      "Epoch 34/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3288 | train sum of auc:\t2.0391 \n",
      "\n",
      "{'train_loss': 0.328757976352329, 'train_Reflux_auroc': 0.6574002574002574, 'train_Reflux_auprc': 0.42498089644702364, 'train_sum_auroc': 2.0390507316079836, 'train_sum_auprc': 1.3145333428738857, 'train_Surgery_auroc': 0.6960444136016655, 'train_Surgery_auprc': 0.2282340265572743, 'train_Function_auroc': 0.6856060606060607, 'train_Function_auprc': 0.6613184198695878, 'val_loss': 0.5196750849657665, 'val_Reflux_auroc': 0.5076248313090419, 'val_Reflux_auprc': 0.5082363232176627, 'val_sum_auroc': 1.3341408243969246, 'val_sum_auprc': 0.7871602776894282, 'val_Surgery_auroc': 0.44630765975454934, 'val_Surgery_auprc': 0.16767403363137623, 'val_Function_auroc': 0.3802083333333333, 'val_Function_auprc': 0.11124992084038936, 'best_epoch': 33}\n",
      "val loss:\t0.5197 | val sum of auc:\t1.3341 \n",
      "\n",
      "Epoch 35/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.3288 | train sum of auc:\t1.9290 \n",
      "\n",
      "{'train_loss': 0.3287970350241997, 'train_Reflux_auroc': 0.6604890604890605, 'train_Reflux_auprc': 0.4587329901038899, 'train_sum_auroc': 1.9289658127444387, 'train_sum_auprc': 1.2496001469106797, 'train_Surgery_auroc': 0.4843858431644691, 'train_Surgery_auprc': 0.08564387132034634, 'train_Function_auroc': 0.7840909090909091, 'train_Function_auprc': 0.7052232854864435, 'val_loss': 0.511986594324167, 'val_Reflux_auroc': 0.5114035087719297, 'val_Reflux_auprc': 0.5099039762555853, 'val_sum_auroc': 1.2967149654889092, 'val_sum_auprc': 0.77677232561223, 'val_Surgery_auroc': 0.4346170122725349, 'val_Surgery_auprc': 0.16215881435552917, 'val_Function_auroc': 0.3506944444444445, 'val_Function_auprc': 0.1047095350011156, 'best_epoch': 34}\n",
      "val loss:\t0.5120 | val sum of auc:\t1.2967 \n",
      "\n",
      "Epoch 36/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3158 | train sum of auc:\t2.2103 \n",
      "\n",
      "{'train_loss': 0.3158454735514144, 'train_Reflux_auroc': 0.7042471042471043, 'train_Reflux_auprc': 0.42080967994285645, 'train_sum_auroc': 2.2102614461393086, 'train_sum_auprc': 1.3766259839204211, 'train_Surgery_auroc': 0.7029840388619015, 'train_Surgery_auprc': 0.2475292669405276, 'train_Function_auroc': 0.803030303030303, 'train_Function_auprc': 0.7082870370370371, 'val_loss': 0.5094545252750374, 'val_Reflux_auroc': 0.5164642375168691, 'val_Reflux_auprc': 0.5159654617269582, 'val_sum_auroc': 1.3053977965519938, 'val_sum_auprc': 0.7851896181891668, 'val_Surgery_auroc': 0.4139335590351248, 'val_Surgery_auprc': 0.15696699198253108, 'val_Function_auroc': 0.375, 'val_Function_auprc': 0.11225716447967751, 'best_epoch': 35}\n",
      "val loss:\t0.5095 | val sum of auc:\t1.3054 \n",
      "\n",
      "Epoch 37/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3247 | train sum of auc:\t2.0322 \n",
      "\n",
      "{'train_loss': 0.3246536930681954, 'train_Reflux_auroc': 0.6435006435006435, 'train_Reflux_auprc': 0.4507982496408287, 'train_sum_auroc': 2.0322064033896092, 'train_sum_auprc': 1.2377549704737623, 'train_Surgery_auroc': 0.7182512144344205, 'train_Surgery_auprc': 0.17983733957562126, 'train_Function_auroc': 0.6704545454545454, 'train_Function_auprc': 0.6071193812573124, 'val_loss': 0.504442963641503, 'val_Reflux_auroc': 0.5095479082321187, 'val_Reflux_auprc': 0.5180425689285939, 'val_sum_auroc': 1.3506074135690258, 'val_sum_auprc': 0.8168880178822413, 'val_Surgery_auroc': 0.4313372831146847, 'val_Surgery_auprc': 0.1634756428644829, 'val_Function_auroc': 0.4097222222222222, 'val_Function_auprc': 0.13536980608916446, 'best_epoch': 36}\n",
      "val loss:\t0.5044 | val sum of auc:\t1.3506 \n",
      "\n",
      "Epoch 38/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3080 | train sum of auc:\t2.2690 \n",
      "\n",
      "{'train_loss': 0.30796780489699943, 'train_Reflux_auroc': 0.7438867438867439, 'train_Reflux_auprc': 0.49267275390908616, 'train_sum_auroc': 2.269013970349848, 'train_sum_auprc': 1.4152031962927223, 'train_Surgery_auroc': 0.7258848022206801, 'train_Surgery_auprc': 0.19462476682349833, 'train_Function_auroc': 0.7992424242424243, 'train_Function_auprc': 0.7279056755601379, 'val_loss': 0.49629727402174406, 'val_Reflux_auroc': 0.5107962213225371, 'val_Reflux_auprc': 0.5151746880125827, 'val_sum_auroc': 1.4094919706054636, 'val_sum_auprc': 0.8318740302455624, 'val_Surgery_auroc': 0.4750846381718154, 'val_Surgery_auprc': 0.1791108845694351, 'val_Function_auroc': 0.4236111111111111, 'val_Function_auprc': 0.13758845766354458, 'best_epoch': 37}\n",
      "val loss:\t0.4963 | val sum of auc:\t1.4095 \n",
      "\n",
      "Epoch 39/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2975 | train sum of auc:\t2.4453 \n",
      "\n",
      "{'train_loss': 0.2975014401993281, 'train_Reflux_auroc': 0.7580437580437581, 'train_Reflux_auprc': 0.5099019887844765, 'train_sum_auroc': 2.445297979649125, 'train_sum_auprc': 1.7557454724533605, 'train_Surgery_auroc': 0.8084663428174879, 'train_Surgery_auprc': 0.43663992850944655, 'train_Function_auroc': 0.8787878787878788, 'train_Function_auprc': 0.8092035551594375, 'val_loss': 0.49470265580050515, 'val_Reflux_auroc': 0.5077935222672065, 'val_Reflux_auprc': 0.503870208441205, 'val_sum_auroc': 1.4344000311777252, 'val_sum_auprc': 0.8357969309683341, 'val_Surgery_auroc': 0.5186203977994075, 'val_Surgery_auprc': 0.19692084132871907, 'val_Function_auroc': 0.4079861111111111, 'val_Function_auprc': 0.13500588119840992, 'best_epoch': 38}\n",
      "val loss:\t0.4947 | val sum of auc:\t1.4344 \n",
      "\n",
      "Epoch 40/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3173 | train sum of auc:\t2.0998 \n",
      "\n",
      "val loss:\t0.4990 | val sum of auc:\t1.4051 \n",
      "\n",
      "Epoch 41/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3129 | train sum of auc:\t2.2602 \n",
      "\n",
      "val loss:\t0.5061 | val sum of auc:\t1.3957 \n",
      "\n",
      "Epoch 42/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3149 | train sum of auc:\t2.2612 \n",
      "\n",
      "val loss:\t0.5097 | val sum of auc:\t1.4089 \n",
      "\n",
      "Epoch 43/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3155 | train sum of auc:\t2.0845 \n",
      "\n",
      "val loss:\t0.5177 | val sum of auc:\t1.3904 \n",
      "\n",
      "Epoch 44/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3120 | train sum of auc:\t2.2754 \n",
      "\n",
      "val loss:\t0.5229 | val sum of auc:\t1.4274 \n",
      "\n",
      "Epoch 45/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3100 | train sum of auc:\t2.2530 \n",
      "\n",
      "val loss:\t0.5235 | val sum of auc:\t1.4324 \n",
      "\n",
      "Epoch 46/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3071 | train sum of auc:\t2.3575 \n",
      "\n",
      "val loss:\t0.5176 | val sum of auc:\t1.4765 \n",
      "\n",
      "Epoch 47/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3048 | train sum of auc:\t2.4028 \n",
      "\n",
      "val loss:\t0.5133 | val sum of auc:\t1.4882 \n",
      "\n",
      "Epoch 48/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3187 | train sum of auc:\t2.0872 \n",
      "\n",
      "val loss:\t0.5135 | val sum of auc:\t1.4774 \n",
      "\n",
      "Epoch 49/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2942 | train sum of auc:\t2.4731 \n",
      "\n",
      "val loss:\t0.5087 | val sum of auc:\t1.5071 \n",
      "\n",
      "Epoch 50/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3232 | train sum of auc:\t2.0259 \n",
      "\n",
      "val loss:\t0.5045 | val sum of auc:\t1.5395 \n",
      "\n",
      "Epoch 51/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3048 | train sum of auc:\t2.3389 \n",
      "\n",
      "val loss:\t0.4999 | val sum of auc:\t1.5937 \n",
      "\n",
      "Epoch 52/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3118 | train sum of auc:\t2.2040 \n",
      "\n",
      "val loss:\t0.5118 | val sum of auc:\t1.5323 \n",
      "\n",
      "Epoch 53/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3142 | train sum of auc:\t2.2186 \n",
      "\n",
      "val loss:\t0.5291 | val sum of auc:\t1.4719 \n",
      "\n",
      "Epoch 54/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2987 | train sum of auc:\t2.4176 \n",
      "\n",
      "val loss:\t0.5423 | val sum of auc:\t1.4205 \n",
      "\n",
      "Epoch 55/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3032 | train sum of auc:\t2.2335 \n",
      "\n",
      "val loss:\t0.5454 | val sum of auc:\t1.4371 \n",
      "\n",
      "Epoch 56/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2864 | train sum of auc:\t2.5513 \n",
      "\n",
      "val loss:\t0.5484 | val sum of auc:\t1.4344 \n",
      "\n",
      "Epoch 57/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.2934 | train sum of auc:\t2.4673 \n",
      "\n",
      "val loss:\t0.5481 | val sum of auc:\t1.3910 \n",
      "\n",
      "Epoch 58/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3084 | train sum of auc:\t2.2055 \n",
      "\n",
      "val loss:\t0.5401 | val sum of auc:\t1.4075 \n",
      "\n",
      "Epoch 59/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3087 | train sum of auc:\t2.3216 \n",
      "\n",
      "val loss:\t0.5262 | val sum of auc:\t1.4585 \n",
      "\n",
      "EARLY STOPPING 58\n",
      "Epoch 60/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/17mjjxp1\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/17mjjxp1</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.4817 | train sum of auc:\t1.7177 \n",
      "\n",
      "val loss:\t0.6320 | val sum of auc:\t1.3288 \n",
      "\n",
      "Epoch 2/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3893 | train sum of auc:\t1.6744 \n",
      "\n",
      "val loss:\t0.5698 | val sum of auc:\t1.4799 \n",
      "\n",
      "Epoch 3/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3748 | train sum of auc:\t1.5042 \n",
      "\n",
      "val loss:\t0.4900 | val sum of auc:\t1.5600 \n",
      "\n",
      "Epoch 4/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3595 | train sum of auc:\t1.7781 \n",
      "\n",
      "val loss:\t0.4280 | val sum of auc:\t1.6151 \n",
      "\n",
      "Epoch 5/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3677 | train sum of auc:\t1.7272 \n",
      "\n",
      "val loss:\t0.3869 | val sum of auc:\t1.6864 \n",
      "\n",
      "Epoch 6/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3635 | train sum of auc:\t1.8164 \n",
      "\n",
      "val loss:\t0.3655 | val sum of auc:\t1.7664 \n",
      "\n",
      "Epoch 7/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3521 | train sum of auc:\t1.9967 \n",
      "\n",
      "val loss:\t0.3540 | val sum of auc:\t1.8680 \n",
      "\n",
      "Epoch 8/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3654 | train sum of auc:\t1.8447 \n",
      "\n",
      "val loss:\t0.3496 | val sum of auc:\t1.9445 \n",
      "\n",
      "Epoch 9/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3600 | train sum of auc:\t1.8718 \n",
      "\n",
      "val loss:\t0.3486 | val sum of auc:\t1.9879 \n",
      "\n",
      "Epoch 10/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3652 | train sum of auc:\t1.7710 \n",
      "\n",
      "val loss:\t0.3496 | val sum of auc:\t1.9837 \n",
      "\n",
      "Epoch 11/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3678 | train sum of auc:\t1.6701 \n",
      "\n",
      "val loss:\t0.3505 | val sum of auc:\t1.9751 \n",
      "\n",
      "Epoch 12/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3590 | train sum of auc:\t1.7365 \n",
      "\n",
      "val loss:\t0.3532 | val sum of auc:\t1.9419 \n",
      "\n",
      "Epoch 13/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3522 | train sum of auc:\t1.9275 \n",
      "\n",
      "val loss:\t0.3591 | val sum of auc:\t1.9030 \n",
      "\n",
      "Epoch 14/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3485 | train sum of auc:\t1.8965 \n",
      "\n",
      "val loss:\t0.3654 | val sum of auc:\t1.8673 \n",
      "\n",
      "Epoch 15/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3405 | train sum of auc:\t2.0450 \n",
      "\n",
      "val loss:\t0.3677 | val sum of auc:\t1.8769 \n",
      "\n",
      "Epoch 16/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3515 | train sum of auc:\t1.8511 \n",
      "\n",
      "val loss:\t0.3688 | val sum of auc:\t1.8791 \n",
      "\n",
      "Epoch 17/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3520 | train sum of auc:\t1.9581 \n",
      "\n",
      "val loss:\t0.3669 | val sum of auc:\t1.9031 \n",
      "\n",
      "Epoch 18/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3399 | train sum of auc:\t2.0890 \n",
      "\n",
      "val loss:\t0.3650 | val sum of auc:\t1.9174 \n",
      "\n",
      "Epoch 19/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3456 | train sum of auc:\t2.0147 \n",
      "\n",
      "val loss:\t0.3626 | val sum of auc:\t1.9139 \n",
      "\n",
      "Epoch 20/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3519 | train sum of auc:\t1.8444 \n",
      "\n",
      "val loss:\t0.3636 | val sum of auc:\t1.8939 \n",
      "\n",
      "Epoch 21/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3526 | train sum of auc:\t1.8482 \n",
      "\n",
      "val loss:\t0.3631 | val sum of auc:\t1.8725 \n",
      "\n",
      "Epoch 22/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3315 | train sum of auc:\t2.1452 \n",
      "\n",
      "val loss:\t0.3629 | val sum of auc:\t1.8455 \n",
      "\n",
      "Epoch 23/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3302 | train sum of auc:\t2.1340 \n",
      "\n",
      "val loss:\t0.3647 | val sum of auc:\t1.7870 \n",
      "\n",
      "Epoch 24/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3484 | train sum of auc:\t1.8989 \n",
      "\n",
      "val loss:\t0.3682 | val sum of auc:\t1.7717 \n",
      "\n",
      "Epoch 25/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3325 | train sum of auc:\t2.0686 \n",
      "\n",
      "val loss:\t0.3731 | val sum of auc:\t1.6873 \n",
      "\n",
      "Epoch 26/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3450 | train sum of auc:\t2.0256 \n",
      "\n",
      "val loss:\t0.3732 | val sum of auc:\t1.6555 \n",
      "\n",
      "Epoch 27/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3327 | train sum of auc:\t2.1739 \n",
      "\n",
      "val loss:\t0.3705 | val sum of auc:\t1.6678 \n",
      "\n",
      "Epoch 28/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3361 | train sum of auc:\t2.1246 \n",
      "\n",
      "val loss:\t0.3710 | val sum of auc:\t1.6772 \n",
      "\n",
      "Epoch 29/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3406 | train sum of auc:\t2.0266 \n",
      "\n",
      "val loss:\t0.3717 | val sum of auc:\t1.6706 \n",
      "\n",
      "Epoch 30/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3164 | train sum of auc:\t2.3202 \n",
      "\n",
      "val loss:\t0.3711 | val sum of auc:\t1.6974 \n",
      "\n",
      "Epoch 31/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3219 | train sum of auc:\t2.2662 \n",
      "\n",
      "{'train_loss': 0.3219067649446803, 'train_Reflux_auroc': 0.7515151515151515, 'train_Reflux_auprc': 0.5573961880980242, 'train_sum_auroc': 2.2661724102668983, 'train_sum_auprc': 1.428209753648432, 'train_Surgery_auroc': 0.7440944881889764, 'train_Surgery_auprc': 0.233612892126824, 'train_Function_auroc': 0.7705627705627706, 'train_Function_auprc': 0.6372006734235837, 'val_loss': 0.37063107062393513, 'val_Reflux_auroc': 0.4378646662120179, 'val_Reflux_auprc': 0.2787707463916642, 'val_sum_auroc': 1.669023829629246, 'val_sum_auprc': 1.1783115114959601, 'val_Surgery_auroc': 0.7527472527472528, 'val_Surgery_auprc': 0.5213623794193571, 'val_Function_auroc': 0.47841191066997524, 'val_Function_auprc': 0.3781783856849389, 'best_epoch': 30}\n",
      "val loss:\t0.3706 | val sum of auc:\t1.6690 \n",
      "\n",
      "Epoch 32/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3309 | train sum of auc:\t2.1819 \n",
      "\n",
      "{'train_loss': 0.33088699562086477, 'train_Reflux_auroc': 0.6744949494949494, 'train_Reflux_auprc': 0.45638428979897405, 'train_sum_auroc': 2.1819087954914727, 'train_sum_auprc': 1.4524271589668432, 'train_Surgery_auroc': 0.6719160104986877, 'train_Surgery_auprc': 0.24915734728234729, 'train_Function_auroc': 0.8354978354978355, 'train_Function_auprc': 0.7468855218855218, 'val_loss': 0.3700952487932125, 'val_Reflux_auroc': 0.44328256421914075, 'val_Reflux_auprc': 0.27697939565606433, 'val_sum_auroc': 1.614429056710863, 'val_sum_auprc': 1.1596457694296698, 'val_Surgery_auroc': 0.7339256488192658, 'val_Surgery_auprc': 0.5285705041398918, 'val_Function_auroc': 0.4372208436724566, 'val_Function_auprc': 0.35409586963371364, 'best_epoch': 31}\n",
      "val loss:\t0.3701 | val sum of auc:\t1.6144 \n",
      "\n",
      "Epoch 33/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3304 | train sum of auc:\t2.1770 \n",
      "\n",
      "{'train_loss': 0.33041470985618426, 'train_Reflux_auroc': 0.6676767676767678, 'train_Reflux_auprc': 0.5249842777156368, 'train_sum_auroc': 2.176956857665519, 'train_sum_auprc': 1.3989310326514859, 'train_Surgery_auroc': 0.7473753280839895, 'train_Surgery_auprc': 0.23378552727462143, 'train_Function_auroc': 0.7619047619047619, 'train_Function_auprc': 0.6401612276612276, 'val_loss': 0.366679353613249, 'val_Reflux_auroc': 0.46074865499734785, 'val_Reflux_auprc': 0.27997113541450047, 'val_sum_auroc': 1.6192091375484274, 'val_sum_auprc': 1.1251402514410769, 'val_Surgery_auroc': 0.7708674304418985, 'val_Surgery_auprc': 0.550775407483744, 'val_Function_auroc': 0.38759305210918116, 'val_Function_auprc': 0.2943937085428324, 'best_epoch': 32}\n",
      "val loss:\t0.3667 | val sum of auc:\t1.6192 \n",
      "\n",
      "Epoch 34/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3293 | train sum of auc:\t2.2607 \n",
      "\n",
      "{'train_loss': 0.32929350467894575, 'train_Reflux_auroc': 0.6550505050505051, 'train_Reflux_auprc': 0.4220273925647069, 'train_sum_auroc': 2.2606594630216676, 'train_sum_auprc': 1.469428240756022, 'train_Surgery_auroc': 0.8523622047244094, 'train_Surgery_auprc': 0.3576760127127774, 'train_Function_auroc': 0.7532467532467532, 'train_Function_auprc': 0.6897248354785376, 'val_loss': 0.36345611029947306, 'val_Reflux_auroc': 0.4695764188830795, 'val_Reflux_auprc': 0.2871386866338709, 'val_sum_auroc': 1.6581190361834182, 'val_sum_auprc': 1.1157479088229474, 'val_Surgery_auroc': 0.8083937339256488, 'val_Surgery_auprc': 0.5542636756057362, 'val_Function_auroc': 0.38014888337468977, 'val_Function_auprc': 0.2743455465833404, 'best_epoch': 33}\n",
      "val loss:\t0.3635 | val sum of auc:\t1.6581 \n",
      "\n",
      "Epoch 35/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.3197 | train sum of auc:\t2.2612 \n",
      "\n",
      "{'train_loss': 0.3196687089453498, 'train_Reflux_auroc': 0.7219696969696969, 'train_Reflux_auprc': 0.5135824319536283, 'train_sum_auroc': 2.261169342468555, 'train_sum_auprc': 1.63601281794568, 'train_Surgery_auroc': 0.7729658792650919, 'train_Surgery_auprc': 0.41235238927612383, 'train_Function_auroc': 0.7662337662337663, 'train_Function_auprc': 0.7100779967159279, 'val_loss': 0.35957625970034535, 'val_Reflux_auroc': 0.5035614154732136, 'val_Reflux_auprc': 0.311040791764515, 'val_sum_auroc': 1.7369236606405378, 'val_sum_auprc': 1.1467134550754665, 'val_Surgery_auroc': 0.8159925181201777, 'val_Surgery_auprc': 0.5510058089207774, 'val_Function_auroc': 0.41736972704714637, 'val_Function_auprc': 0.28466685439017403, 'best_epoch': 34}\n",
      "val loss:\t0.3596 | val sum of auc:\t1.7369 \n",
      "\n",
      "Epoch 36/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3359 | train sum of auc:\t2.0552 \n",
      "\n",
      "{'train_loss': 0.33588353721357933, 'train_Reflux_auroc': 0.6722222222222223, 'train_Reflux_auprc': 0.4601027425901284, 'train_sum_auroc': 2.055202758746066, 'train_sum_auprc': 1.1781141380297417, 'train_Surgery_auroc': 0.8202099737532809, 'train_Surgery_auprc': 0.32121250190158934, 'train_Function_auroc': 0.5627705627705627, 'train_Function_auprc': 0.39679889353802394, 'val_loss': 0.35448531407705497, 'val_Reflux_auroc': 0.5333030234144124, 'val_Reflux_auprc': 0.35319020683192703, 'val_sum_auroc': 1.8093165088994145, 'val_sum_auprc': 1.2058931057091677, 'val_Surgery_auroc': 0.8164601356090717, 'val_Surgery_auprc': 0.5505038835884205, 'val_Function_auroc': 0.45955334987593055, 'val_Function_auprc': 0.3021990152888201, 'best_epoch': 35}\n",
      "val loss:\t0.3545 | val sum of auc:\t1.8093 \n",
      "\n",
      "Epoch 37/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3227 | train sum of auc:\t2.2537 \n",
      "\n",
      "{'train_loss': 0.32267072689619, 'train_Reflux_auroc': 0.7037878787878787, 'train_Reflux_auprc': 0.5336588584805303, 'train_sum_auroc': 2.253716296826533, 'train_sum_auprc': 1.478302230180685, 'train_Surgery_auroc': 0.7014435695538058, 'train_Surgery_auprc': 0.2086863993795458, 'train_Function_auroc': 0.8484848484848484, 'train_Function_auprc': 0.7359569723206087, 'val_loss': 0.3535118262532731, 'val_Reflux_auroc': 0.5241342729408198, 'val_Reflux_auprc': 0.34033761466985724, 'val_sum_auroc': 1.8437485639346578, 'val_sum_auprc': 1.285985237571965, 'val_Surgery_auroc': 0.8560907177928454, 'val_Surgery_auprc': 0.6266533242594116, 'val_Function_auroc': 0.46352357320099247, 'val_Function_auprc': 0.31899429864269624, 'best_epoch': 36}\n",
      "val loss:\t0.3535 | val sum of auc:\t1.8437 \n",
      "\n",
      "Epoch 38/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3341 | train sum of auc:\t2.1710 \n",
      "\n",
      "val loss:\t0.3575 | val sum of auc:\t1.8076 \n",
      "\n",
      "Epoch 39/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3299 | train sum of auc:\t2.0900 \n",
      "\n",
      "val loss:\t0.3617 | val sum of auc:\t1.7411 \n",
      "\n",
      "Epoch 40/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3134 | train sum of auc:\t2.3718 \n",
      "\n",
      "val loss:\t0.3656 | val sum of auc:\t1.7022 \n",
      "\n",
      "Epoch 41/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3106 | train sum of auc:\t2.3969 \n",
      "\n",
      "val loss:\t0.3656 | val sum of auc:\t1.6983 \n",
      "\n",
      "Epoch 42/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3153 | train sum of auc:\t2.3257 \n",
      "\n",
      "val loss:\t0.3643 | val sum of auc:\t1.7325 \n",
      "\n",
      "Epoch 43/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3223 | train sum of auc:\t2.2913 \n",
      "\n",
      "val loss:\t0.3603 | val sum of auc:\t1.8054 \n",
      "\n",
      "Epoch 44/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3197 | train sum of auc:\t2.2313 \n",
      "\n",
      "val loss:\t0.3606 | val sum of auc:\t1.8170 \n",
      "\n",
      "Epoch 45/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3205 | train sum of auc:\t2.2595 \n",
      "\n",
      "val loss:\t0.3566 | val sum of auc:\t1.8500 \n",
      "\n",
      "Epoch 46/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3276 | train sum of auc:\t2.2503 \n",
      "\n",
      "val loss:\t0.3565 | val sum of auc:\t1.8795 \n",
      "\n",
      "Epoch 47/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3070 | train sum of auc:\t2.3929 \n",
      "\n",
      "val loss:\t0.3616 | val sum of auc:\t1.8786 \n",
      "\n",
      "Epoch 48/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3142 | train sum of auc:\t2.2549 \n",
      "\n",
      "val loss:\t0.3636 | val sum of auc:\t1.8551 \n",
      "\n",
      "Epoch 49/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3203 | train sum of auc:\t2.2410 \n",
      "\n",
      "val loss:\t0.3643 | val sum of auc:\t1.8314 \n",
      "\n",
      "Epoch 50/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3276 | train sum of auc:\t2.1741 \n",
      "\n",
      "val loss:\t0.3655 | val sum of auc:\t1.8836 \n",
      "\n",
      "Epoch 51/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3063 | train sum of auc:\t2.4100 \n",
      "\n",
      "val loss:\t0.3662 | val sum of auc:\t1.9512 \n",
      "\n",
      "Epoch 52/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3121 | train sum of auc:\t2.3506 \n",
      "\n",
      "val loss:\t0.3629 | val sum of auc:\t1.8962 \n",
      "\n",
      "Epoch 53/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3148 | train sum of auc:\t2.2384 \n",
      "\n",
      "val loss:\t0.3599 | val sum of auc:\t1.8318 \n",
      "\n",
      "Epoch 54/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3082 | train sum of auc:\t2.3509 \n",
      "\n",
      "val loss:\t0.3622 | val sum of auc:\t1.7624 \n",
      "\n",
      "Epoch 55/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3155 | train sum of auc:\t2.2735 \n",
      "\n",
      "val loss:\t0.3620 | val sum of auc:\t1.7652 \n",
      "\n",
      "Epoch 56/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3168 | train sum of auc:\t2.2349 \n",
      "\n",
      "val loss:\t0.3646 | val sum of auc:\t1.7680 \n",
      "\n",
      "Epoch 57/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3139 | train sum of auc:\t2.2390 \n",
      "\n",
      "val loss:\t0.3742 | val sum of auc:\t1.6948 \n",
      "\n",
      "EARLY STOPPING 56\n",
      "Epoch 58/75\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/1p2epkzl\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_weigh_function_0.25_no_data_augment_min30_epochs/runs/1p2epkzl</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.5572 | train sum of auc:\t1.4830 \n",
      "\n",
      "val loss:\t0.4428 | val sum of auc:\t1.2634 \n",
      "\n",
      "Epoch 2/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.4390 | train sum of auc:\t1.3209 \n",
      "\n",
      "val loss:\t0.3927 | val sum of auc:\t1.3646 \n",
      "\n",
      "Epoch 3/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3961 | train sum of auc:\t1.6415 \n",
      "\n",
      "val loss:\t0.3600 | val sum of auc:\t1.4708 \n",
      "\n",
      "Epoch 4/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3768 | train sum of auc:\t1.5837 \n",
      "\n",
      "val loss:\t0.3391 | val sum of auc:\t1.5336 \n",
      "\n",
      "Epoch 5/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3916 | train sum of auc:\t1.5634 \n",
      "\n",
      "val loss:\t0.3259 | val sum of auc:\t1.5419 \n",
      "\n",
      "Epoch 6/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3885 | train sum of auc:\t1.7838 \n",
      "\n",
      "val loss:\t0.3208 | val sum of auc:\t1.5361 \n",
      "\n",
      "Epoch 7/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3966 | train sum of auc:\t1.5402 \n",
      "\n",
      "val loss:\t0.3177 | val sum of auc:\t1.5702 \n",
      "\n",
      "Epoch 8/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3854 | train sum of auc:\t1.7780 \n",
      "\n",
      "val loss:\t0.3115 | val sum of auc:\t1.5610 \n",
      "\n",
      "Epoch 9/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3925 | train sum of auc:\t1.6173 \n",
      "\n",
      "val loss:\t0.3077 | val sum of auc:\t1.5342 \n",
      "\n",
      "Epoch 10/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3795 | train sum of auc:\t1.8186 \n",
      "\n",
      "val loss:\t0.3041 | val sum of auc:\t1.5376 \n",
      "\n",
      "Epoch 11/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3761 | train sum of auc:\t1.8343 \n",
      "\n",
      "val loss:\t0.3078 | val sum of auc:\t1.5390 \n",
      "\n",
      "Epoch 12/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3803 | train sum of auc:\t1.8542 \n",
      "\n",
      "val loss:\t0.3166 | val sum of auc:\t1.5050 \n",
      "\n",
      "Epoch 13/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3691 | train sum of auc:\t1.8750 \n",
      "\n",
      "val loss:\t0.3318 | val sum of auc:\t1.4504 \n",
      "\n",
      "Epoch 14/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3760 | train sum of auc:\t1.6571 \n",
      "\n",
      "val loss:\t0.3524 | val sum of auc:\t1.3428 \n",
      "\n",
      "Epoch 15/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3595 | train sum of auc:\t1.9803 \n",
      "\n",
      "val loss:\t0.3683 | val sum of auc:\t1.2843 \n",
      "\n",
      "Epoch 16/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3585 | train sum of auc:\t2.0268 \n",
      "\n",
      "val loss:\t0.3804 | val sum of auc:\t1.2325 \n",
      "\n",
      "Epoch 17/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3724 | train sum of auc:\t1.8002 \n",
      "\n",
      "val loss:\t0.3853 | val sum of auc:\t1.2190 \n",
      "\n",
      "Epoch 18/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3683 | train sum of auc:\t1.8477 \n",
      "\n",
      "val loss:\t0.3833 | val sum of auc:\t1.1865 \n",
      "\n",
      "Epoch 19/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3554 | train sum of auc:\t1.9955 \n",
      "\n",
      "val loss:\t0.3756 | val sum of auc:\t1.1219 \n",
      "\n",
      "Epoch 20/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3494 | train sum of auc:\t2.0829 \n",
      "\n",
      "val loss:\t0.3607 | val sum of auc:\t1.0732 \n",
      "\n",
      "Epoch 21/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3592 | train sum of auc:\t1.9679 \n",
      "\n",
      "val loss:\t0.3415 | val sum of auc:\t1.0099 \n",
      "\n",
      "Epoch 22/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3447 | train sum of auc:\t2.1254 \n",
      "\n",
      "val loss:\t0.3258 | val sum of auc:\t1.0132 \n",
      "\n",
      "Epoch 23/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3552 | train sum of auc:\t2.1020 \n",
      "\n",
      "val loss:\t0.3190 | val sum of auc:\t1.0273 \n",
      "\n",
      "Epoch 24/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3501 | train sum of auc:\t2.0058 \n",
      "\n",
      "val loss:\t0.3160 | val sum of auc:\t1.0618 \n",
      "\n",
      "Epoch 25/75\n",
      "------------------------------------------------------\n",
      "train loss:\t0.3562 | train sum of auc:\t2.0910 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b50eede47d4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mview_to_use\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Saggital_Right'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Transverse_Right'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Saggital_Left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Transverse_Left'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                     \u001b[0mtrain5fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview_to_use\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'feature_extract'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature_extract\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'resnet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-cb288dc5c4f3>\u001b[0m in \u001b[0;36mtrain5fold\u001b[0;34m(view_to_use, network_configs, model_name, lr, wd, amsgrad, feature_extract, i)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Train & Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mmetrics_from_best_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mview_to_use\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"inception\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_testing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mbest_metrics_per_fold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_from_best_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-fe951b4f5501>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, view_to_use, num_epochs, is_inception, final_testing)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;31m#                 labels = labels.type(torch.long)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m# Only use the image for the specified view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-4f06f5f67b6e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, ind)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_image_path_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mempty_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mempty_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty_im\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0moutput_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mempty_im\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _init_jupyter.<locals>.cleanup at 0x7f1f8cade510> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# shutdown async logger because _user_process_finished isn't called in jupyter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mshutdown_async_log_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post_run_cell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36m_stop_jupyter_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jupyter_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmirror_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cloud\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stopping streaming files and file change observer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_file_syncing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36m_end_file_syncing\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_observer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_observer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopped_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_observer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _init_jupyter.<locals>.cleanup at 0x7f1f8ca1b158> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# shutdown async logger because _user_process_finished isn't called in jupyter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mshutdown_async_log_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post_run_cell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36m_stop_jupyter_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jupyter_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmirror_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cloud\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stopping streaming files and file change observer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_file_syncing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36m_end_file_syncing\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;31m# TODO: there was a case where _file_event_handlers was getting modified in the loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_event_handlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m             \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_pusher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tailer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tailer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tailer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _init_jupyter.<locals>.cleanup at 0x7f1f8c7a0f28> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# shutdown async logger because _user_process_finished isn't called in jupyter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mshutdown_async_log_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post_run_cell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36m_stop_jupyter_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jupyter_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmirror_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;34m\"\"\"Stops system stats, streaming handlers, and uploads files without output, used by wandb.monitor\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shutting down system stats and metadata service\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mwatcher\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensorboard_watchers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/stats.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Incase we never start it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _init_jupyter.<locals>.cleanup at 0x7f1f8c7022f0> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# shutdown async logger because _user_process_finished isn't called in jupyter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mshutdown_async_log_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post_run_cell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36m_stop_jupyter_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jupyter_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmirror_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shutting down system stats and metadata service\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mwatcher\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensorboard_watchers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mwatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/meta.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;31m# Incase we never start it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _init_jupyter.<locals>.cleanup at 0x7f1f8c717510> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# shutdown async logger because _user_process_finished isn't called in jupyter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mshutdown_async_log_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post_run_cell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36m_stop_jupyter_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jupyter_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmirror_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shutting down system stats and metadata service\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mwatcher\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensorboard_watchers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mwatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/meta.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;31m# Incase we never start it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _init_jupyter.<locals>.cleanup at 0x7f1f81ee6f28> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# shutdown async logger because _user_process_finished isn't called in jupyter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mshutdown_async_log_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post_run_cell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36m_stop_jupyter_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jupyter_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmirror_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shutting down system stats and metadata service\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mwatcher\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensorboard_watchers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mwatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/meta.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;31m# Incase we never start it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _init_jupyter.<locals>.cleanup at 0x7f1f80a38598> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# shutdown async logger because _user_process_finished isn't called in jupyter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mshutdown_async_log_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post_run_cell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36m_stop_jupyter_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jupyter_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmirror_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cloud\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stopping streaming files and file change observer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_file_syncing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36m_end_file_syncing\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;31m# TODO: there was a case where _file_event_handlers was getting modified in the loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_event_handlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m             \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_pusher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tailer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tailer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tailer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _init_jupyter.<locals>.cleanup at 0x7f1f8095eb70> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# shutdown async logger because _user_process_finished isn't called in jupyter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mshutdown_async_log_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post_run_cell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36m_stop_jupyter_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jupyter_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmirror_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shutting down system stats and metadata service\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mwatcher\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensorboard_watchers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mwatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/meta.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;31m# Incase we never start it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _init_jupyter.<locals>.cleanup at 0x7f1f808c6f28> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# shutdown async logger because _user_process_finished isn't called in jupyter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mshutdown_async_log_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post_run_cell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36m_stop_jupyter_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jupyter_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmirror_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cloud\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stopping streaming files and file change observer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_file_syncing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36m_end_file_syncing\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_observer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_observer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopped_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_observer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "wd = 0.001\n",
    "layers_to_train = [2, 4, 8, 12, 16]\n",
    "amsgrad = False\n",
    "i = 0\n",
    "lrs = [1e-3, 1e-4, 5e-4, 1e-3]\n",
    "weight_decays = [1e-4, 5e-4, 1e-5]\n",
    "feature_extracts = [False, True]\n",
    "for lr in lrs:\n",
    "    for wd in weight_decays:\n",
    "        for feature_extract in feature_extracts:\n",
    "            for layer in layers_to_train:\n",
    "                for view_to_use in ['Saggital_Right', 'Transverse_Right', 'Saggital_Left', 'Transverse_Left']:\n",
    "                    if i >= 2:\n",
    "                        train5fold(view_to_use, {'feature_extract': feature_extract}, 'resnet', lr, wd, amsgrad, feature_extract,layer, i)\n",
    "                    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for visualizing images, \n",
    "# MAKE SURE THIS IS COMMENTED OUT WHEN COMMITTING!!\n",
    "# image_return_order = ['Saggital_Right', 'Transverse_Right', 'Saggital_Left', 'Transverse_Left', 'Bladder']\n",
    "\n",
    "# for inputs, labs in training_generator:\n",
    "#     plt.figure(figsize=(20,10)) \n",
    "#     first_scan = inputs[0]\n",
    "#     for i in range(5):\n",
    "#         im = first_scan[i,:, :, :]\n",
    "\n",
    "#         im_np = np.asarray(im).squeeze()\n",
    "#         plt.subplot(2,5,i+ 1)\n",
    "#         plt.imshow(im_np, cmap='gray')\n",
    "#         frame1 = plt.gca()\n",
    "#         frame1.axes.get_xaxis().set_visible(False)\n",
    "#         frame1.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "#         plt.title(image_return_order[i])\n",
    "        \n",
    "        \n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "image_label.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
