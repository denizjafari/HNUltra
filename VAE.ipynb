{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqA1aA9sw4lQ"
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cbxBJbXs3XGM"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'batch_size':32,\n",
    "    'epochs':20,\n",
    "    'no_cuda':False,\n",
    "    'seed':1,\n",
    "    'log_interval':10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NaSBBp0Rw4lY"
   },
   "outputs": [],
   "source": [
    "# root directory\n",
    "root_dir = \"/home/andreasabo/Documents/HNProject/\"\n",
    "\n",
    "# data directory on current machine: abhishekmoturu, andreasabo, denizjafari, navidkorhani\n",
    "data_dir = \"/home/navidkorhani/Documents/HNProject/all_label_img/\"\n",
    "\n",
    "# read target df\n",
    "csv_path = os.path.join(root_dir, \"all_splits_100000.csv\")\n",
    "data_df = pd.read_csv(csv_path, usecols=['image_ids', 'view_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LVbQIjvZw4le"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of images in train = 67592, # of images in test = 4867\n"
     ]
    }
   ],
   "source": [
    "train_df = data_df[data_df.view_train != 0]\n",
    "test_df = data_df[data_df.view_train == 0]\n",
    "\n",
    "print(\"# of images in train = {}, # of images in test = {}\".format(len(train_df), len(test_df)))\n",
    "train_ids = []\n",
    "test_ids = []\n",
    "\n",
    "for ind, row in train_df.iterrows():\n",
    "    train_ids.append(row['image_ids'])\n",
    "\n",
    "for ind, row in test_df.iterrows():\n",
    "    test_ids.append(row['image_ids'])\n",
    "\n",
    "partition = {'train':train_ids, 'test':test_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Ylr6TUew4li"
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, list_IDs):\n",
    "        'Initialization'\n",
    "        self.list_IDs = list_IDs\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        img_path = data_dir + ID + '.jpg'\n",
    "        image = Image.open(img_path)#.convert('L')\n",
    "        image = ToTensor()(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda:1\n"
     ]
    }
   ],
   "source": [
    "args[\"cuda\"]= not args['no_cuda'] and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "device = torch.device(\"cuda:1\" if args['cuda'] else \"cpu\")\n",
    "\n",
    "print(\"device is\", device)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args['cuda'] else {}\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': args['batch_size'],\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "\n",
    "if args['cuda']:\n",
    "    params['num_workers']=1\n",
    "    params['pin_memory']=True\n",
    "\n",
    "\n",
    "# Data Loader\n",
    "training_set = Dataset(partition['train'])\n",
    "train_loader = data.DataLoader(training_set, **params)\n",
    "\n",
    "test_set = Dataset(partition['test'])\n",
    "test_loader = data.DataLoader(test_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aGGaperRw4lp"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(65536, 400)\n",
    "        self.fc21 = nn.Linear(400, 40)\n",
    "        self.fc22 = nn.Linear(400, 40)\n",
    "        self.fc3 = nn.Linear(40, 400)\n",
    "        self.fc4 = nn.Linear(400, 65536)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 65536))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 65536), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args['log_interval'] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(args['batch_size'], 1, 256, 256)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OV9WoQYX6lWR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/67592 (0%)]\tLoss: 137737.203125\n",
      "Train Epoch: 1 [320/67592 (0%)]\tLoss: 150652.484375\n",
      "Train Epoch: 1 [640/67592 (1%)]\tLoss: 148804.671875\n",
      "Train Epoch: 1 [960/67592 (1%)]\tLoss: 141267.562500\n",
      "Train Epoch: 1 [1280/67592 (2%)]\tLoss: 140122.484375\n",
      "Train Epoch: 1 [1600/67592 (2%)]\tLoss: 139597.234375\n",
      "Train Epoch: 1 [1920/67592 (3%)]\tLoss: 139256.187500\n",
      "Train Epoch: 1 [2240/67592 (3%)]\tLoss: 138825.515625\n",
      "Train Epoch: 1 [2560/67592 (4%)]\tLoss: 138438.078125\n",
      "Train Epoch: 1 [2880/67592 (4%)]\tLoss: 138172.640625\n",
      "Train Epoch: 1 [3200/67592 (5%)]\tLoss: 138006.765625\n",
      "Train Epoch: 1 [3520/67592 (5%)]\tLoss: 137259.765625\n",
      "Train Epoch: 1 [3840/67592 (6%)]\tLoss: 136758.468750\n",
      "Train Epoch: 1 [4160/67592 (6%)]\tLoss: 136526.171875\n",
      "Train Epoch: 1 [4480/67592 (7%)]\tLoss: 135477.875000\n",
      "Train Epoch: 1 [4800/67592 (7%)]\tLoss: 136075.015625\n",
      "Train Epoch: 1 [5120/67592 (8%)]\tLoss: 137377.562500\n",
      "Train Epoch: 1 [5440/67592 (8%)]\tLoss: 136989.390625\n",
      "Train Epoch: 1 [5760/67592 (9%)]\tLoss: 135155.765625\n",
      "Train Epoch: 1 [6080/67592 (9%)]\tLoss: 132425.984375\n",
      "Train Epoch: 1 [6400/67592 (9%)]\tLoss: 134611.687500\n",
      "Train Epoch: 1 [6720/67592 (10%)]\tLoss: 134605.593750\n",
      "Train Epoch: 1 [7040/67592 (10%)]\tLoss: 135252.109375\n",
      "Train Epoch: 1 [7360/67592 (11%)]\tLoss: 134790.671875\n",
      "Train Epoch: 1 [7680/67592 (11%)]\tLoss: 132899.750000\n",
      "Train Epoch: 1 [8000/67592 (12%)]\tLoss: 132772.406250\n",
      "Train Epoch: 1 [8320/67592 (12%)]\tLoss: 133136.687500\n",
      "Train Epoch: 1 [8640/67592 (13%)]\tLoss: 133448.593750\n",
      "Train Epoch: 1 [8960/67592 (13%)]\tLoss: 130859.257812\n",
      "Train Epoch: 1 [9280/67592 (14%)]\tLoss: 133741.718750\n",
      "Train Epoch: 1 [9600/67592 (14%)]\tLoss: 134540.015625\n",
      "Train Epoch: 1 [9920/67592 (15%)]\tLoss: 130763.531250\n",
      "Train Epoch: 1 [10240/67592 (15%)]\tLoss: 131659.406250\n",
      "Train Epoch: 1 [10560/67592 (16%)]\tLoss: 130884.382812\n",
      "Train Epoch: 1 [10880/67592 (16%)]\tLoss: 132136.906250\n",
      "Train Epoch: 1 [11200/67592 (17%)]\tLoss: 134227.984375\n",
      "Train Epoch: 1 [11520/67592 (17%)]\tLoss: 136197.625000\n",
      "Train Epoch: 1 [11840/67592 (18%)]\tLoss: 131421.234375\n",
      "Train Epoch: 1 [12160/67592 (18%)]\tLoss: 131174.625000\n",
      "Train Epoch: 1 [12480/67592 (18%)]\tLoss: 131608.218750\n",
      "Train Epoch: 1 [12800/67592 (19%)]\tLoss: 129957.234375\n",
      "Train Epoch: 1 [13120/67592 (19%)]\tLoss: 128135.875000\n",
      "Train Epoch: 1 [13440/67592 (20%)]\tLoss: 128320.617188\n",
      "Train Epoch: 1 [13760/67592 (20%)]\tLoss: 131179.734375\n",
      "Train Epoch: 1 [14080/67592 (21%)]\tLoss: 127841.875000\n",
      "Train Epoch: 1 [14400/67592 (21%)]\tLoss: 128084.375000\n",
      "Train Epoch: 1 [14720/67592 (22%)]\tLoss: 128574.062500\n",
      "Train Epoch: 1 [15040/67592 (22%)]\tLoss: 127015.515625\n",
      "Train Epoch: 1 [15360/67592 (23%)]\tLoss: 129009.343750\n",
      "Train Epoch: 1 [15680/67592 (23%)]\tLoss: 128512.234375\n",
      "Train Epoch: 1 [16000/67592 (24%)]\tLoss: 124891.234375\n",
      "Train Epoch: 1 [16320/67592 (24%)]\tLoss: 129271.593750\n",
      "Train Epoch: 1 [16640/67592 (25%)]\tLoss: 129483.218750\n",
      "Train Epoch: 1 [16960/67592 (25%)]\tLoss: 126075.515625\n",
      "Train Epoch: 1 [17280/67592 (26%)]\tLoss: 126081.882812\n",
      "Train Epoch: 1 [17600/67592 (26%)]\tLoss: 126200.953125\n",
      "Train Epoch: 1 [17920/67592 (27%)]\tLoss: 125016.398438\n",
      "Train Epoch: 1 [18240/67592 (27%)]\tLoss: 125407.140625\n",
      "Train Epoch: 1 [18560/67592 (27%)]\tLoss: 125886.835938\n",
      "Train Epoch: 1 [18880/67592 (28%)]\tLoss: 127100.414062\n",
      "Train Epoch: 1 [19200/67592 (28%)]\tLoss: 126254.492188\n",
      "Train Epoch: 1 [19520/67592 (29%)]\tLoss: 132215.875000\n",
      "Train Epoch: 1 [19840/67592 (29%)]\tLoss: 127718.570312\n",
      "Train Epoch: 1 [20160/67592 (30%)]\tLoss: 125775.156250\n",
      "Train Epoch: 1 [20480/67592 (30%)]\tLoss: 127642.585938\n",
      "Train Epoch: 1 [20800/67592 (31%)]\tLoss: 124548.187500\n",
      "Train Epoch: 1 [21120/67592 (31%)]\tLoss: 125159.757812\n",
      "Train Epoch: 1 [21440/67592 (32%)]\tLoss: 125850.875000\n",
      "Train Epoch: 1 [21760/67592 (32%)]\tLoss: 126361.906250\n",
      "Train Epoch: 1 [22080/67592 (33%)]\tLoss: 127793.164062\n",
      "Train Epoch: 1 [22400/67592 (33%)]\tLoss: 126426.796875\n",
      "Train Epoch: 1 [22720/67592 (34%)]\tLoss: 125003.304688\n",
      "Train Epoch: 1 [23040/67592 (34%)]\tLoss: 125701.234375\n",
      "Train Epoch: 1 [23360/67592 (35%)]\tLoss: 126130.039062\n",
      "Train Epoch: 1 [23680/67592 (35%)]\tLoss: 125712.937500\n",
      "Train Epoch: 1 [24000/67592 (35%)]\tLoss: 125598.984375\n",
      "Train Epoch: 1 [24320/67592 (36%)]\tLoss: 125325.875000\n",
      "Train Epoch: 1 [24640/67592 (36%)]\tLoss: 123165.312500\n",
      "Train Epoch: 1 [24960/67592 (37%)]\tLoss: 124911.460938\n",
      "Train Epoch: 1 [25280/67592 (37%)]\tLoss: 123915.773438\n",
      "Train Epoch: 1 [25600/67592 (38%)]\tLoss: 123368.648438\n",
      "Train Epoch: 1 [25920/67592 (38%)]\tLoss: 123647.492188\n",
      "Train Epoch: 1 [26240/67592 (39%)]\tLoss: 124506.781250\n",
      "Train Epoch: 1 [26560/67592 (39%)]\tLoss: 123810.140625\n",
      "Train Epoch: 1 [26880/67592 (40%)]\tLoss: 123222.507812\n",
      "Train Epoch: 1 [27200/67592 (40%)]\tLoss: 122388.304688\n",
      "Train Epoch: 1 [27520/67592 (41%)]\tLoss: 122841.960938\n",
      "Train Epoch: 1 [27840/67592 (41%)]\tLoss: 121942.242188\n",
      "Train Epoch: 1 [28160/67592 (42%)]\tLoss: 124067.156250\n",
      "Train Epoch: 1 [28480/67592 (42%)]\tLoss: 124236.445312\n",
      "Train Epoch: 1 [28800/67592 (43%)]\tLoss: 121815.125000\n",
      "Train Epoch: 1 [29120/67592 (43%)]\tLoss: 120239.171875\n",
      "Train Epoch: 1 [29440/67592 (44%)]\tLoss: 121919.750000\n",
      "Train Epoch: 1 [29760/67592 (44%)]\tLoss: 123338.914062\n",
      "Train Epoch: 1 [30080/67592 (44%)]\tLoss: 122082.164062\n",
      "Train Epoch: 1 [30400/67592 (45%)]\tLoss: 123500.453125\n",
      "Train Epoch: 1 [30720/67592 (45%)]\tLoss: 122348.484375\n",
      "Train Epoch: 1 [31040/67592 (46%)]\tLoss: 123197.367188\n",
      "Train Epoch: 1 [31360/67592 (46%)]\tLoss: 120995.117188\n",
      "Train Epoch: 1 [31680/67592 (47%)]\tLoss: 121899.187500\n",
      "Train Epoch: 1 [32000/67592 (47%)]\tLoss: 124324.156250\n",
      "Train Epoch: 1 [32320/67592 (48%)]\tLoss: 123544.648438\n",
      "Train Epoch: 1 [32640/67592 (48%)]\tLoss: 123088.539062\n",
      "Train Epoch: 1 [32960/67592 (49%)]\tLoss: 121194.312500\n",
      "Train Epoch: 1 [33280/67592 (49%)]\tLoss: 121500.656250\n",
      "Train Epoch: 1 [33600/67592 (50%)]\tLoss: 122371.234375\n",
      "Train Epoch: 1 [33920/67592 (50%)]\tLoss: 122613.156250\n",
      "Train Epoch: 1 [34240/67592 (51%)]\tLoss: 121054.617188\n",
      "Train Epoch: 1 [34560/67592 (51%)]\tLoss: 120715.242188\n",
      "Train Epoch: 1 [34880/67592 (52%)]\tLoss: 122202.078125\n",
      "Train Epoch: 1 [35200/67592 (52%)]\tLoss: 119812.648438\n",
      "Train Epoch: 1 [35520/67592 (53%)]\tLoss: 124480.929688\n",
      "Train Epoch: 1 [35840/67592 (53%)]\tLoss: 121251.007812\n",
      "Train Epoch: 1 [36160/67592 (53%)]\tLoss: 122607.992188\n",
      "Train Epoch: 1 [36480/67592 (54%)]\tLoss: 121273.468750\n",
      "Train Epoch: 1 [36800/67592 (54%)]\tLoss: 121614.781250\n",
      "Train Epoch: 1 [37120/67592 (55%)]\tLoss: 121400.421875\n",
      "Train Epoch: 1 [37440/67592 (55%)]\tLoss: 121996.359375\n",
      "Train Epoch: 1 [37760/67592 (56%)]\tLoss: 122929.609375\n",
      "Train Epoch: 1 [38080/67592 (56%)]\tLoss: 122893.710938\n",
      "Train Epoch: 1 [38400/67592 (57%)]\tLoss: 122029.468750\n",
      "Train Epoch: 1 [38720/67592 (57%)]\tLoss: 122162.679688\n",
      "Train Epoch: 1 [39040/67592 (58%)]\tLoss: 121738.390625\n",
      "Train Epoch: 1 [39360/67592 (58%)]\tLoss: 121511.757812\n",
      "Train Epoch: 1 [39680/67592 (59%)]\tLoss: 121572.101562\n",
      "Train Epoch: 1 [40000/67592 (59%)]\tLoss: 122496.617188\n",
      "Train Epoch: 1 [40320/67592 (60%)]\tLoss: 121201.515625\n",
      "Train Epoch: 1 [40640/67592 (60%)]\tLoss: 119091.070312\n",
      "Train Epoch: 1 [40960/67592 (61%)]\tLoss: 120816.968750\n",
      "Train Epoch: 1 [41280/67592 (61%)]\tLoss: 119517.078125\n",
      "Train Epoch: 1 [41600/67592 (62%)]\tLoss: 120019.781250\n",
      "Train Epoch: 1 [41920/67592 (62%)]\tLoss: 122153.601562\n",
      "Train Epoch: 1 [42240/67592 (62%)]\tLoss: 121395.554688\n",
      "Train Epoch: 1 [42560/67592 (63%)]\tLoss: 120222.015625\n",
      "Train Epoch: 1 [42880/67592 (63%)]\tLoss: 122780.945312\n",
      "Train Epoch: 1 [43200/67592 (64%)]\tLoss: 119376.867188\n",
      "Train Epoch: 1 [43520/67592 (64%)]\tLoss: 122518.062500\n",
      "Train Epoch: 1 [43840/67592 (65%)]\tLoss: 123234.835938\n",
      "Train Epoch: 1 [44160/67592 (65%)]\tLoss: 122039.476562\n",
      "Train Epoch: 1 [44480/67592 (66%)]\tLoss: 119736.421875\n",
      "Train Epoch: 1 [44800/67592 (66%)]\tLoss: 120952.679688\n",
      "Train Epoch: 1 [45120/67592 (67%)]\tLoss: 119773.242188\n",
      "Train Epoch: 1 [45440/67592 (67%)]\tLoss: 122565.890625\n",
      "Train Epoch: 1 [45760/67592 (68%)]\tLoss: 120104.429688\n",
      "Train Epoch: 1 [46080/67592 (68%)]\tLoss: 120190.039062\n",
      "Train Epoch: 1 [46400/67592 (69%)]\tLoss: 120445.945312\n",
      "Train Epoch: 1 [46720/67592 (69%)]\tLoss: 120420.773438\n",
      "Train Epoch: 1 [47040/67592 (70%)]\tLoss: 119733.562500\n",
      "Train Epoch: 1 [47360/67592 (70%)]\tLoss: 122749.453125\n",
      "Train Epoch: 1 [47680/67592 (71%)]\tLoss: 119597.398438\n",
      "Train Epoch: 1 [48000/67592 (71%)]\tLoss: 121632.734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [48320/67592 (71%)]\tLoss: 119899.171875\n",
      "Train Epoch: 1 [48640/67592 (72%)]\tLoss: 121582.929688\n",
      "Train Epoch: 1 [48960/67592 (72%)]\tLoss: 120475.125000\n",
      "Train Epoch: 1 [49280/67592 (73%)]\tLoss: 121033.851562\n",
      "Train Epoch: 1 [49600/67592 (73%)]\tLoss: 121039.320312\n",
      "Train Epoch: 1 [49920/67592 (74%)]\tLoss: 120612.570312\n",
      "Train Epoch: 1 [50240/67592 (74%)]\tLoss: 118576.679688\n",
      "Train Epoch: 1 [50560/67592 (75%)]\tLoss: 120556.484375\n",
      "Train Epoch: 1 [50880/67592 (75%)]\tLoss: 120784.062500\n",
      "Train Epoch: 1 [51200/67592 (76%)]\tLoss: 119478.265625\n",
      "Train Epoch: 1 [51520/67592 (76%)]\tLoss: 120571.476562\n",
      "Train Epoch: 1 [51840/67592 (77%)]\tLoss: 120285.046875\n",
      "Train Epoch: 1 [52160/67592 (77%)]\tLoss: 119740.406250\n",
      "Train Epoch: 1 [52480/67592 (78%)]\tLoss: 124912.945312\n",
      "Train Epoch: 1 [52800/67592 (78%)]\tLoss: 122765.960938\n",
      "Train Epoch: 1 [53120/67592 (79%)]\tLoss: 123973.937500\n",
      "Train Epoch: 1 [53440/67592 (79%)]\tLoss: 121704.820312\n",
      "Train Epoch: 1 [53760/67592 (80%)]\tLoss: 121727.492188\n",
      "Train Epoch: 1 [54080/67592 (80%)]\tLoss: 122335.921875\n",
      "Train Epoch: 1 [54400/67592 (80%)]\tLoss: 122454.414062\n",
      "Train Epoch: 1 [54720/67592 (81%)]\tLoss: 122215.093750\n",
      "Train Epoch: 1 [55040/67592 (81%)]\tLoss: 121142.304688\n",
      "Train Epoch: 1 [55360/67592 (82%)]\tLoss: 120786.765625\n",
      "Train Epoch: 1 [55680/67592 (82%)]\tLoss: 121527.218750\n",
      "Train Epoch: 1 [56000/67592 (83%)]\tLoss: 122051.187500\n",
      "Train Epoch: 1 [56320/67592 (83%)]\tLoss: 120534.460938\n",
      "Train Epoch: 1 [56640/67592 (84%)]\tLoss: 122035.976562\n",
      "Train Epoch: 1 [56960/67592 (84%)]\tLoss: 121900.773438\n",
      "Train Epoch: 1 [57280/67592 (85%)]\tLoss: 120504.757812\n",
      "Train Epoch: 1 [57600/67592 (85%)]\tLoss: 120145.359375\n",
      "Train Epoch: 1 [57920/67592 (86%)]\tLoss: 120385.890625\n",
      "Train Epoch: 1 [58240/67592 (86%)]\tLoss: 120535.578125\n",
      "Train Epoch: 1 [58560/67592 (87%)]\tLoss: 120354.570312\n",
      "Train Epoch: 1 [58880/67592 (87%)]\tLoss: 119203.851562\n",
      "Train Epoch: 1 [59200/67592 (88%)]\tLoss: 118935.929688\n",
      "Train Epoch: 1 [59520/67592 (88%)]\tLoss: 120102.625000\n",
      "Train Epoch: 1 [59840/67592 (88%)]\tLoss: 120604.945312\n",
      "Train Epoch: 1 [60160/67592 (89%)]\tLoss: 121417.351562\n",
      "Train Epoch: 1 [60480/67592 (89%)]\tLoss: 121737.164062\n",
      "Train Epoch: 1 [60800/67592 (90%)]\tLoss: 120647.531250\n",
      "Train Epoch: 1 [61120/67592 (90%)]\tLoss: 120626.710938\n",
      "Train Epoch: 1 [61440/67592 (91%)]\tLoss: 121192.250000\n",
      "Train Epoch: 1 [61760/67592 (91%)]\tLoss: 120787.679688\n",
      "Train Epoch: 1 [62080/67592 (92%)]\tLoss: 119703.937500\n",
      "Train Epoch: 1 [62400/67592 (92%)]\tLoss: 119172.375000\n",
      "Train Epoch: 1 [62720/67592 (93%)]\tLoss: 119282.882812\n",
      "Train Epoch: 1 [63040/67592 (93%)]\tLoss: 120474.781250\n",
      "Train Epoch: 1 [63360/67592 (94%)]\tLoss: 120031.390625\n",
      "Train Epoch: 1 [63680/67592 (94%)]\tLoss: 120014.953125\n",
      "Train Epoch: 1 [64000/67592 (95%)]\tLoss: 120875.828125\n",
      "Train Epoch: 1 [64320/67592 (95%)]\tLoss: 118866.031250\n",
      "Train Epoch: 1 [64640/67592 (96%)]\tLoss: 120521.539062\n",
      "Train Epoch: 1 [64960/67592 (96%)]\tLoss: 121754.757812\n",
      "Train Epoch: 1 [65280/67592 (97%)]\tLoss: 122241.851562\n",
      "Train Epoch: 1 [65600/67592 (97%)]\tLoss: 120810.820312\n",
      "Train Epoch: 1 [65920/67592 (97%)]\tLoss: 119843.421875\n",
      "Train Epoch: 1 [66240/67592 (98%)]\tLoss: 121896.929688\n",
      "Train Epoch: 1 [66560/67592 (98%)]\tLoss: 121163.929688\n",
      "Train Epoch: 1 [66880/67592 (99%)]\tLoss: 121326.007812\n",
      "Train Epoch: 1 [67200/67592 (99%)]\tLoss: 120444.382812\n",
      "Train Epoch: 1 [67520/67592 (100%)]\tLoss: 119092.117188\n",
      "====> Epoch: 1 Average loss: 126055.7238\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6b18af9d08e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-97e2df220da9>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_DataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_std_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for epoch in range(1, args['epochs'] + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 40).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 256, 256),\n",
    "                       'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "VAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
