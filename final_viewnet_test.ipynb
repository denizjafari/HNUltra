{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZbZRovgc-In"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from torch.utils import data\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "import io\n",
    "import uuid\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import wandb\n",
    "wandb_username = 'denizjafari'\n",
    "local_username = 'denizjafari'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2aVSXNyc-Iv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:1') \n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylfRtoN9c-Iy"
   },
   "outputs": [],
   "source": [
    "# root directory\n",
    "root_dir = \"/home/andreasabo/Documents/HNProject/\"\n",
    "split_file_base = \"/home/andreasabo/Documents/HNUltra/\"\n",
    "\n",
    "# data directory on current machine: abhishekmoturu, andreasabo, denizjafari, navidkorhani\n",
    "data_dir = \"/home/\" + local_username + \"/Documents/HNProject/all_label_img/\"\n",
    "\n",
    "# read target df\n",
    "csv_path = os.path.join(root_dir, \"all_splits_1000000.csv\")\n",
    "data_df = pd.read_csv(csv_path, usecols=['subj_id', 'image_ids', 'view_label', 'view_train'])\n",
    "\n",
    "# Are we doing the final test?\n",
    "test_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_OCR_7uy52w"
   },
   "source": [
    "### **Reading Data Indicies and Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nnwavxcqGBv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading splits from file\n"
     ]
    }
   ],
   "source": [
    "label_mapping = {'Other':0, 'Saggital_Right':1, 'Transverse_Right':2, \n",
    "                 'Saggital_Left':3, 'Transverse_Left':4, 'Bladder':5}\n",
    "label_unmapping = {0: 'Other', 1:'Saggital_Right', 2: 'Transverse_Right', \n",
    "                   3:'Saggital_Left', 4:'Transverse_Left', 5: 'Bladder'}\n",
    "\n",
    "data_df['view_label'] = data_df['view_label'].map(label_mapping)\n",
    "\n",
    "train_df = data_df[data_df.view_train == 1]\n",
    "test_df = data_df[data_df.view_train == 0]\n",
    "\n",
    "unique_subj = train_df.subj_id.unique()\n",
    "\n",
    "# Create the splits for 5-fold cross validation based on subj_id\n",
    "data_split_file = split_file_base + 'data_splits.json'\n",
    "# just load from file\n",
    "print(\"Reading splits from file\")\n",
    "with open(data_split_file, 'r') as f:\n",
    "    all_folds = json.load(f)\n",
    "\n",
    "# If we're testing, overwrite the training data with the entire train/test data\n",
    "if test_data:\n",
    "    train_images = train_df.image_ids.tolist()\n",
    "    test_images = test_df.image_ids.tolist()\n",
    "    train_labels = train_df.view_label.tolist()\n",
    "    test_labels = test_df.view_label.tolist()\n",
    "\n",
    "    cur_fold = {'train_ids': train_images, 'test_ids': test_images, 'train_labels': train_labels, 'test_labels': test_labels}\n",
    "\n",
    "    \n",
    "    all_folds['test'] = cur_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13958\n",
      "11081\n",
      "5070\n",
      "2877\n"
     ]
    }
   ],
   "source": [
    "print(len(all_folds['test']['train_ids']))\n",
    "print(len(all_folds['0']['train_ids']))\n",
    "\n",
    "print(len(all_folds['test']['test_ids']))\n",
    "print(len(all_folds['0']['valid_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DIRECTORIES \n",
    "\n",
    "\n",
    "vae50_dir = '/home/navidkorhani/Documents/HNProject/HNUltra/saved models/vae_model_h800_l50.pt'\n",
    "vae100_dir = \"/home/navidkorhani/Documents/HNProject/HNUltra/results/h800_l100_e30/vae_model.pt\"\n",
    "vae200_dir = \"/home/navidkorhani/Documents/HNProject/HNUltra/results/h800_l200_e30/vae_model.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://scikit-learn.org/0.16/auto_examples/model_selection/plot_confusion_matrix.html#example-model-selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm,rep, classnames, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classnames))\n",
    "    plt.xticks(tick_marks, classnames, rotation=45)\n",
    "    plt.yticks(tick_marks, classnames)\n",
    "    \n",
    "    \n",
    "    thresh = cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=18)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('ViewNetConfMtrx' + str(rep) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BbgEWoqKc-JO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 256\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == 'vae50':\n",
    "        \n",
    "        model_ft = VAE_50()\n",
    "        input_size = 256\n",
    "        \n",
    "    elif model_name == 'vae100':\n",
    "        \n",
    "        model_ft = VAE_100()\n",
    "        input_size = 256\n",
    "        \n",
    "    elif model_name == 'vae200':\n",
    "        \n",
    "        model_ft = VAE_200()\n",
    "        input_size = 256\n",
    "        \n",
    "        \n",
    "    elif model_name == 'viewnet':\n",
    "        conv1_filters = 8\n",
    "        conv2_filters = 16\n",
    "        conv3_filters = 32\n",
    "        linear1_size = 512\n",
    "        dropout = 0.25\n",
    "        model_ft = ViewNet(num_classes, conv1_filters, conv2_filters, conv3_filters, linear1_size, dropout)\n",
    "        input_size = 256\n",
    "        \n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Architectures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom view labeller \n",
    "class ViewNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, conv1_filters, conv2_filters, conv3_filters, linear1_size, dropout):\n",
    "        super(ViewNet, self).__init__()\n",
    "        self.conv1_filters = conv1_filters\n",
    "        self.conv2_filters = conv2_filters\n",
    "        self.conv3_filters = conv3_filters\n",
    "        self.linear1_size = linear1_size\n",
    "        self.drop_percent = dropout\n",
    "        self.max_pool = 4\n",
    "        self.conv_output = int(self.conv3_filters*(256/(self.max_pool**3))*(256/(self.max_pool**3)))\n",
    "        print(\"conv_output: \", self.conv_output)\n",
    "\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, self.conv1_filters, 4, padding=2)\n",
    "        self.conv2 = nn.Conv2d(self.conv1_filters, self.conv2_filters, 4, padding=2)\n",
    "        self.conv3 = nn.Conv2d(self.conv2_filters, self.conv3_filters, 4, padding=2)\n",
    "        self.pool = nn.MaxPool2d(self.max_pool, self.max_pool)\n",
    "        self.dropout = nn.Dropout(self.drop_percent)\n",
    "        self.linear1 = nn.Linear(self.conv_output, self.linear1_size)\n",
    "        self.linear2 = nn.Linear(self.linear1_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.dropout(F.relu(self.conv1(x))))\n",
    "        x = self.pool(self.dropout(F.relu(self.conv2(x))))\n",
    "        x = self.pool(self.dropout(F.relu(self.conv3(x))))\n",
    "        x = x.view(-1, self.conv_output) \n",
    "        x = self.dropout(F.relu((self.linear1(x))))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class VAE_50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_50, self).__init__()\n",
    "        \n",
    "        hidden_dim = 800\n",
    "        latent_dim = 50\n",
    "        self.fc1 = nn.Linear(65536, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 65536)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        #print(\"z.size() =\", z.size())\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        #print(\"h3.size() =\", h3.size())\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 65536))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    \n",
    "class VAE_100(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_100, self).__init__()\n",
    "        \n",
    "        hidden_dim = 800\n",
    "        latent_dim = 100\n",
    "        self.fc1 = nn.Linear(65536, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 65536)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        #print(\"z.size() =\", z.size())\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        #print(\"h3.size() =\", h3.size())\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 65536))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "class VAE_200(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_200, self).__init__()\n",
    "        \n",
    "        hidden_dim = 800\n",
    "        latent_dim = 200\n",
    "        self.fc1 = nn.Linear(65536, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 65536)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        #print(\"z.size() =\", z.size())\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        #print(\"h3.size() =\", h3.size())\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 65536))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from: https://gist.github.com/stefanonardo/693d96ceb2f531fa05db530f3e21517d\n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=True):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if np.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DdSxc4Dhc-JT"
   },
   "outputs": [],
   "source": [
    "def train5fold(network_configs,criterion_used,  model_ft, lr, wd, amsgrad, repetitions):\n",
    "    fold = 'test'\n",
    "\n",
    "    random_str = str(uuid.uuid4()).split(\"-\")[0]\n",
    "    best_metrics_per_fold = []\n",
    "    final_df = pd.DataFrame()\n",
    "    model_base = copy.deepcopy(model_ft)\n",
    "    \n",
    "    for i in range(repetitions):\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%d-%m-%Y.%H:%M:%S\")\n",
    "        wandb.init(project='hnultra_test', entity=wandb_username, name=local_username + '_rep_' + i, group=random_str)\n",
    "        partition = all_folds[fold]\n",
    "\n",
    "        model_ft = copy.deepcopy(model_base)\n",
    "        model_ft = model_ft.to(device)\n",
    "        wandb.watch(model_ft)\n",
    "\n",
    "        # Gather the parameters to be optimized/updated in this run. If we are\n",
    "        #  finetuning we will be updating all parameters. However, if we are\n",
    "        #  doing feature extract method, we will only update the parameters\n",
    "        #  that we have just initialized, i.e. the parameters with requires_grad\n",
    "        #  is True.\n",
    "        params_to_update = model_ft.parameters()\n",
    "        #print(\"Params to learn:\")\n",
    "        if feature_extract:\n",
    "            params_to_update = []\n",
    "            for name,param in model_ft.named_parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    params_to_update.append(param)\n",
    "                    print(\"\\t\",name)\n",
    "        else:\n",
    "            for name,param in model_ft.named_parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    print(\"\\t\",name)\n",
    "\n",
    "        # Observe that all parameters are being optimized\n",
    "        optimizer_ft = optim.Adam(params_to_update, lr=lr, weight_decay=wd, amsgrad=amsgrad)\n",
    "\n",
    "        # Setup the loss fxn\n",
    "        criterion = criterion_used\n",
    "\n",
    "        shuffle = True\n",
    "        num_workers = 0\n",
    "        params = {'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                  'num_workers': num_workers}\n",
    "\n",
    "        config_dict = {'i': i, 'batch_size': batch_size, 'shuffle': shuffle, 'num_workers': num_workers, 'fold': fold,\n",
    "                       'lr': lr, 'wd': wd, 'amsgrad': amsgrad, 'model_name': model_name,'criterion': criterion, 'num_classes': num_classes, \n",
    "                       'num_epochs': num_epochs, 'feature_extract': feature_extract, \"pretrain\": pretrain }\n",
    "\n",
    "        wandb.config.update(config_dict)\n",
    "        wandb.config.update(network_configs)\n",
    "        # Tranforms\n",
    "        trans = transforms.Compose([transforms.RandomAffine(degrees=8, translate=(0.1, 0.1), scale=(0.95,1.25))])\n",
    "\n",
    "        # Generators\n",
    "        training_set = Dataset(partition['train_ids'], partition['train_labels'], transformations=trans)\n",
    "        training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "        validation_set = Dataset(partition['test_ids'], partition['test_labels'])\n",
    "        validation_generator = data.DataLoader(validation_set, **params)\n",
    "\n",
    "        dataloaders_dict = {'train':training_generator, 'test':validation_generator}\n",
    "\n",
    "        # Train & Evaluate\n",
    "        model_ft, metrics_from_best_epoch, rep_ids, rep_labels, rep_preds = train_model(model_ft, \n",
    "                    dataloaders_dict, criterion, optimizer_ft, num_epochs, is_inception=(model_name==\"inception\"))\n",
    "        best_metrics_per_fold.append(metrics_from_best_epoch)\n",
    "        final_df['IDs_iter_' + str(i)] = rep_ids\n",
    "        final_df['labels_iter_' + str(i)] = rep_labels\n",
    "        final_df['preds_iter_' + str(i)] = rep_preds\n",
    "       \n",
    "\n",
    "    # Calculate the performance metrics on the best model in each fold\n",
    "    wandb.init(project='hnultra', entity=wandb_username, name=local_username + '_ALL', group=random_str)\n",
    "    config_dict['fold'] = -1\n",
    "    wandb.config.update(config_dict)\n",
    "    wandb.config.update(network_configs)\n",
    "\n",
    "\n",
    "    metrics_all = {}\n",
    "    for fold in best_metrics_per_fold:\n",
    "        for key in fold:\n",
    "            if key not in metrics_all:\n",
    "                metrics_all[key] = [fold[key]]\n",
    "            else:\n",
    "                metrics_all[key].append(fold[key]) \n",
    "\n",
    "    metrics_to_log = {}\n",
    "    for m in metrics_all:\n",
    "        metric_list = np.asarray(metrics_all[m])\n",
    "\n",
    "        metrics_to_log[m + '_mean'] = metric_list.mean()    \n",
    "        metrics_to_log[m + '_stdev'] = metric_list.std()\n",
    "\n",
    "    wandb.config.update(metrics_to_log)\n",
    "    return final_df\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iikM7_G3c-JR"
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, list_IDs, labels, transformations=None):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.transformations = transformations\n",
    "        \n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        img_path = data_dir + ID + '.jpg'\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        \n",
    "        if self.transformations:\n",
    "            image = self.transformations(image)\n",
    "        \n",
    "        image = ToTensor()(image)\n",
    "        \n",
    "        y = torch.FloatTensor([0]*6)        \n",
    "        y[int(self.labels[index])] = 1\n",
    "\n",
    "        return image, y, ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False, final_testing=True):\n",
    "    es = EarlyStopping(patience = 15)\n",
    "    stop_now = 0\n",
    "\n",
    "    since = time.time()\n",
    "    classnames = ['Other', 'Saggital_Right', 'Transverse_Right', 'Saggital_Left','Transverse_Left', 'Bladder']\n",
    "    #val_acc_history = []\n",
    "    \n",
    "    #val_metrics_list = []\n",
    "    #train_metrics_list = []\n",
    "    y_test_last = []\n",
    "    y_pred_last = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    epoch_with_best_val_acc = num_epochs -1\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 54)\n",
    "\n",
    "        if stop_now:\n",
    "            break\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            running_preds = []\n",
    "            running_labels = []\n",
    "\n",
    "            # save the needed data \n",
    "            new_col_ids = []\n",
    "            #new_col_labels = []\n",
    "            #new_col_preds = []\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for inputs, labels, ids in dataloaders[phase]:\n",
    "                labels = labels.type(torch.long)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                if epoch == (num_epochs -1):\n",
    "                    new_col_ids += ids.tolist()\n",
    "                    \n",
    "                    \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        labels = torch.argmax(labels, 1)\n",
    "                        running_preds += torch.argmax(outputs, 1).tolist()\n",
    "                        running_labels += labels.tolist()\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    preds = torch.argmax(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print('{} loss:\\t{:.4f} | {} acc:\\t{:.4f}\\n'.format(phase, epoch_loss, phase, epoch_acc))\n",
    "\n",
    "            if phase == 'train':\n",
    "                wandb.log({'epoch': epoch, 'train_acc':epoch_acc, 'train_loss':epoch_loss})\n",
    "                \n",
    "                cur_train_metrics = {}\n",
    "                                # compute and log f1, precision, recall for each class\n",
    "                for c in range(6):\n",
    "                    running_labels = np.asarray(running_labels)\n",
    "                    running_preds = np.asarray(running_preds)\n",
    "\n",
    "                    cur_c_labs_bin = np.asarray([0] *len(running_labels))\n",
    "                    cur_c_preds_bin = np.asarray([0] *len(running_labels))\n",
    "\n",
    "                    # Need to binarize\n",
    "                    cur_c_preds_bin[running_preds == c] = 1\n",
    "                    cur_c_labs_bin[running_labels == c] = 1\n",
    "                    f1 = f1_score(cur_c_labs_bin, cur_c_preds_bin)\n",
    "                    precision = precision_score(cur_c_labs_bin, cur_c_preds_bin)\n",
    "                    recall = recall_score(cur_c_labs_bin, cur_c_preds_bin)\n",
    "                    \n",
    "                    cur_train_metrics['train_' + label_unmapping[c] + '_f1'] = f1\n",
    "                    cur_train_metrics['train_' + label_unmapping[c] + '_precision'] = precision\n",
    "                    cur_train_metrics['train_' + label_unmapping[c] + '_recall'] = recall\n",
    "                    \n",
    "                \n",
    "                #train_metrics_list.append(cur_train_metrics)\n",
    "                \n",
    "                average_types = ['macro', 'micro', 'weighted']\n",
    "                average_metrics_to_log = ['precision', 'recall', 'f1score', 'support']\n",
    "                average_dict = {'epoch': epoch}\n",
    "                for av in average_types:\n",
    "                    results_tuple = precision_recall_fscore_support(running_labels, running_preds, average=av)\n",
    "                    for m in range(len(average_metrics_to_log)):      \n",
    "                        average_dict[phase + '_'+ average_metrics_to_log[m] +'_average_' + av] = results_tuple[m]\n",
    "                        cur_train_metrics[phase + '_'+ average_metrics_to_log[m] +'_average_' + av] = results_tuple[m]\n",
    "                cur_train_metrics[phase + '_acc_average'] = accuracy_score(running_labels, running_preds)                  \n",
    "                average_dict[phase + '_acc_average'] = accuracy_score(running_labels, running_preds)     \n",
    "                wandb.log(cur_train_metrics)\n",
    "                \n",
    "            if phase == 'test':\n",
    "                wandb.log({'test_loss':epoch_loss, 'test_acc':epoch_acc, 'epoch': epoch})\n",
    "                if epoch == (num_epochs-1):\n",
    "            \n",
    "                    cur_val_metrics = {}\n",
    "                    # compute and log f1, precision, recall for each class\n",
    "                    for c in range(6):\n",
    "                        running_labels = np.asarray(running_labels)\n",
    "                        running_preds = np.asarray(running_preds)\n",
    "                        new_col_ids = np.asarray(new_col_ids)\n",
    "                        # confusion matrix of the last epoch\n",
    "                        conf_mx = confusion_matrix(running_labels, running_preds)\n",
    "\n",
    "                        cur_c_labs_bin = np.asarray([0] *len(running_labels))\n",
    "                        cur_c_preds_bin = np.asarray([0] *len(running_labels))\n",
    "\n",
    "                        # Need to binarize\n",
    "                        cur_c_preds_bin[running_preds == c] = 1\n",
    "                        cur_c_labs_bin[running_labels == c] = 1\n",
    "                        f1 = f1_score(cur_c_labs_bin, cur_c_preds_bin)\n",
    "                        precision = precision_score(cur_c_labs_bin, cur_c_preds_bin)\n",
    "                        recall = recall_score(cur_c_labs_bin, cur_c_preds_bin)\n",
    "\n",
    "                        wandb.log({'test_' + label_unmapping[c] + '_f1': f1})\n",
    "                        wandb.log({'test_' + label_unmapping[c] + '_precision': precision})\n",
    "                        wandb.log({'test_' + label_unmapping[c] + '_recall': recall})\n",
    "                        #wandb.log({'test_' + '_confusion_matrix': conf_mx})\n",
    "\n",
    "                        cur_val_metrics['val_' + label_unmapping[c] + '_f1'] = f1\n",
    "                        cur_val_metrics['val_' + label_unmapping[c] + '_precision'] = precision\n",
    "                        cur_val_metrics['val_' + label_unmapping[c] + '_recall'] = recall\n",
    "\n",
    "                    average_types = ['macro', 'micro', 'weighted']\n",
    "                    average_metrics_to_log = ['precision', 'recall', 'f1score']\n",
    "                    average_dict = {'epoch': epoch}\n",
    "                    for av in average_types:\n",
    "                        results_tuple = precision_recall_fscore_support(running_labels, running_preds, average=av)\n",
    "                        for m in range(len(average_metrics_to_log)):      \n",
    "                            average_dict[phase + '_'+ average_metrics_to_log[m] +'_average_' + av] = results_tuple[m]\n",
    "                            cur_val_metrics[phase + '_'+ average_metrics_to_log[m] +'_average_' + av] = results_tuple[m]\n",
    "                    cur_val_metrics[phase + '_acc_average'] = accuracy_score(running_labels, running_preds)                  \n",
    "                    average_dict[phase + '_acc_average'] = accuracy_score(running_labels, running_preds)     \n",
    "                    print(cur_val_metrics)\n",
    "                    wandb.log(cur_val_metrics)\n",
    "\n",
    "\n",
    "                    #val_metrics_list.append(cur_val_metrics)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                print(classification_report(running_labels, running_preds))\n",
    "                train_acc = epoch_acc\n",
    "                if epoch == (num_epochs-1):\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    torch.save(model.state_dict(), os.path.join(wandb.run.dir, \"ViewNetFinalTrainModel.pt\"))\n",
    "                \n",
    "            if phase == 'test' and epoch == (num_epochs-1):\n",
    "                best_acc = epoch_acc\n",
    "                best_acc_train = train_acc\n",
    "                print(classification_report(running_labels, running_preds))\n",
    "                print()\n",
    "                #print(conf_mx)\n",
    "                plot_confusion_matrix(conf_mx,classnames, title='Test Data Confusion matrix - ViewNet', \n",
    "                                      cmap=plt.cm.Blues)\n",
    "                \n",
    "           # if phase == 'test' and epoch_acc > best_acc:\n",
    "            #    best_acc = epoch_acc\n",
    "            #    best_acc_train = train_acc\n",
    "            #    epoch_with_best_val_acc = epoch\n",
    "            #    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            #    torch.save(model.state_dict(), os.path.join(wandb.run.dir, \"model.pt\"))\n",
    "            #    print(classification_report(running_labels, running_preds))\n",
    "\n",
    "            #if phase == 'test':\n",
    "            #    val_acc_history.append(epoch_acc)\n",
    "            #    if es.step(epoch_loss) and not final_testing:\n",
    "            #        stop_now = 1\n",
    "            #        print(\"EARLY STOPPING \" + str(epoch))\n",
    "            #        break\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val acc: {:4f}\\n'.format(best_acc))\n",
    "    \n",
    "    # Directly save the best results in this fold\n",
    "    wandb.config.best_acc = best_acc\n",
    "    wandb.config.best_epoch = epoch_with_best_val_acc\n",
    "\n",
    "    #wandb.config.val_acc_history = val_acc_history\n",
    "    wandb.config.best_epoch = epoch_with_best_val_acc\n",
    "    \n",
    "    #wandb.config.update(val_metrics_list[epoch_with_best_val_acc])\n",
    "    wandb.config.update(cur_val_metrics)\n",
    "    #wandb.config.update(train_metrics_list[epoch_with_best_val_acc])\n",
    "    wandb.config.update(cur_train_metrics)\n",
    "    \n",
    "    metrics_from_best_epoch = {'best_epoch': epoch_with_best_val_acc, 'last_epoch': epoch}\n",
    "    metrics_from_best_epoch.update( cur_val_metrics )\n",
    "    #metrics_from_best_epoch.update( train_metrics_list[epoch_with_best_val_acc] )\n",
    "    metrics_from_best_epoch.update(cur_train_metrics)\n",
    "    metrics_from_best_epoch.update( {'val_acc': best_acc.cpu(), 'train_acc': best_acc_train.cpu()} )    \n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    #return model, val_acc_history, metrics_from_best_epoch\n",
    "    return model, metrics_from_best_epoch, new_col_ids, running_labels, running_preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception, viewnet]\n",
    "model_name = \"viewnet\"\n",
    "\n",
    "# Number of classes in the dataset: right_sag, right_trav, left_sag, left_trav, bladder, other\n",
    "num_classes = 6\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 100\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 46\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model; when True we only update the reshaped layer params\n",
    "feature_extract = False\n",
    "\n",
    "# Flag for whether or not to use pretrained model\n",
    "pretrain = False\n",
    "\n",
    "criterion_used = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_output:  512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/denizjafari/hnultra_test\" target=\"_blank\">https://app.wandb.ai/denizjafari/hnultra_test</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/denizjafari/hnultra_test/runs/9l21d294\" target=\"_blank\">https://app.wandb.ai/denizjafari/hnultra_test/runs/9l21d294</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t conv1.weight\n",
      "\t conv1.bias\n",
      "\t conv2.weight\n",
      "\t conv2.bias\n",
      "\t conv3.weight\n",
      "\t conv3.bias\n",
      "\t linear1.weight\n",
      "\t linear1.bias\n",
      "\t linear2.weight\n",
      "\t linear2.bias\n",
      "Epoch 1/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.6010 | train acc:\t0.3667\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.93      0.53      4943\n",
      "           1       0.27      0.03      0.05      2225\n",
      "           2       0.16      0.01      0.02      1539\n",
      "           3       0.32      0.08      0.13      2284\n",
      "           4       0.00      0.00      0.00      1669\n",
      "           5       0.56      0.18      0.28      1298\n",
      "\n",
      "    accuracy                           0.37     13958\n",
      "   macro avg       0.28      0.21      0.17     13958\n",
      "weighted avg       0.29      0.37      0.24     13958\n",
      "\n",
      "test loss:\t1.5514 | test acc:\t0.4337\n",
      "\n",
      "Epoch 2/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.4237 | train acc:\t0.4363\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.81      0.57      4943\n",
      "           1       0.39      0.21      0.27      2225\n",
      "           2       0.41      0.01      0.01      1539\n",
      "           3       0.37      0.33      0.35      2284\n",
      "           4       0.35      0.02      0.03      1669\n",
      "           5       0.58      0.65      0.61      1298\n",
      "\n",
      "    accuracy                           0.44     13958\n",
      "   macro avg       0.42      0.34      0.31     13958\n",
      "weighted avg       0.42      0.44      0.36     13958\n",
      "\n",
      "test loss:\t1.4985 | test acc:\t0.4584\n",
      "\n",
      "Epoch 3/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3435 | train acc:\t0.4763\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.77      0.59      4943\n",
      "           1       0.43      0.33      0.37      2225\n",
      "           2       0.52      0.05      0.10      1539\n",
      "           3       0.41      0.44      0.42      2284\n",
      "           4       0.43      0.10      0.16      1669\n",
      "           5       0.64      0.67      0.65      1298\n",
      "\n",
      "    accuracy                           0.48     13958\n",
      "   macro avg       0.48      0.39      0.38     13958\n",
      "weighted avg       0.47      0.48      0.43     13958\n",
      "\n",
      "test loss:\t1.4268 | test acc:\t0.5174\n",
      "\n",
      "Epoch 4/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2934 | train acc:\t0.4963\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.76      0.60      4943\n",
      "           1       0.45      0.40      0.42      2225\n",
      "           2       0.50      0.12      0.19      1539\n",
      "           3       0.43      0.42      0.42      2284\n",
      "           4       0.42      0.16      0.23      1669\n",
      "           5       0.69      0.70      0.69      1298\n",
      "\n",
      "    accuracy                           0.50     13958\n",
      "   macro avg       0.50      0.42      0.43     13958\n",
      "weighted avg       0.49      0.50      0.46     13958\n",
      "\n",
      "test loss:\t1.3848 | test acc:\t0.5193\n",
      "\n",
      "Epoch 5/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2637 | train acc:\t0.5078\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.75      0.61      4943\n",
      "           1       0.49      0.40      0.44      2225\n",
      "           2       0.47      0.14      0.22      1539\n",
      "           3       0.43      0.45      0.44      2284\n",
      "           4       0.42      0.20      0.27      1669\n",
      "           5       0.71      0.71      0.71      1298\n",
      "\n",
      "    accuracy                           0.51     13958\n",
      "   macro avg       0.51      0.44      0.45     13958\n",
      "weighted avg       0.50      0.51      0.48     13958\n",
      "\n",
      "test loss:\t1.3511 | test acc:\t0.5168\n",
      "\n",
      "Epoch 6/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2419 | train acc:\t0.5241\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.75      0.62      4943\n",
      "           1       0.52      0.43      0.47      2225\n",
      "           2       0.49      0.19      0.27      1539\n",
      "           3       0.46      0.46      0.46      2284\n",
      "           4       0.44      0.23      0.30      1669\n",
      "           5       0.71      0.71      0.71      1298\n",
      "\n",
      "    accuracy                           0.52     13958\n",
      "   macro avg       0.52      0.46      0.47     13958\n",
      "weighted avg       0.52      0.52      0.50     13958\n",
      "\n",
      "test loss:\t1.3920 | test acc:\t0.5065\n",
      "\n",
      "Epoch 7/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2242 | train acc:\t0.5269\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.75      0.62      4943\n",
      "           1       0.54      0.42      0.47      2225\n",
      "           2       0.51      0.21      0.30      1539\n",
      "           3       0.46      0.47      0.47      2284\n",
      "           4       0.42      0.23      0.30      1669\n",
      "           5       0.72      0.72      0.72      1298\n",
      "\n",
      "    accuracy                           0.53     13958\n",
      "   macro avg       0.53      0.47      0.48     13958\n",
      "weighted avg       0.52      0.53      0.51     13958\n",
      "\n",
      "test loss:\t1.3324 | test acc:\t0.5306\n",
      "\n",
      "Epoch 8/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2091 | train acc:\t0.5322\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.62      4943\n",
      "           1       0.55      0.41      0.47      2225\n",
      "           2       0.48      0.24      0.32      1539\n",
      "           3       0.46      0.47      0.46      2284\n",
      "           4       0.44      0.29      0.35      1669\n",
      "           5       0.73      0.72      0.72      1298\n",
      "\n",
      "    accuracy                           0.53     13958\n",
      "   macro avg       0.53      0.48      0.49     13958\n",
      "weighted avg       0.53      0.53      0.52     13958\n",
      "\n",
      "test loss:\t1.3660 | test acc:\t0.5260\n",
      "\n",
      "Epoch 9/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1840 | train acc:\t0.5404\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.62      4943\n",
      "           1       0.56      0.43      0.49      2225\n",
      "           2       0.51      0.25      0.33      1539\n",
      "           3       0.46      0.48      0.47      2284\n",
      "           4       0.47      0.31      0.37      1669\n",
      "           5       0.73      0.73      0.73      1298\n",
      "\n",
      "    accuracy                           0.54     13958\n",
      "   macro avg       0.55      0.49      0.50     13958\n",
      "weighted avg       0.54      0.54      0.52     13958\n",
      "\n",
      "test loss:\t1.2885 | test acc:\t0.5525\n",
      "\n",
      "Epoch 10/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1666 | train acc:\t0.5499\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      4943\n",
      "           1       0.58      0.44      0.50      2225\n",
      "           2       0.52      0.26      0.35      1539\n",
      "           3       0.48      0.49      0.49      2284\n",
      "           4       0.48      0.34      0.40      1669\n",
      "           5       0.73      0.73      0.73      1298\n",
      "\n",
      "    accuracy                           0.55     13958\n",
      "   macro avg       0.56      0.50      0.52     13958\n",
      "weighted avg       0.55      0.55      0.54     13958\n",
      "\n",
      "test loss:\t1.3080 | test acc:\t0.5428\n",
      "\n",
      "Epoch 11/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1500 | train acc:\t0.5557\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      4943\n",
      "           1       0.61      0.44      0.51      2225\n",
      "           2       0.50      0.26      0.34      1539\n",
      "           3       0.50      0.51      0.50      2284\n",
      "           4       0.49      0.36      0.41      1669\n",
      "           5       0.74      0.74      0.74      1298\n",
      "\n",
      "    accuracy                           0.56     13958\n",
      "   macro avg       0.56      0.51      0.52     13958\n",
      "weighted avg       0.55      0.56      0.54     13958\n",
      "\n",
      "test loss:\t1.2630 | test acc:\t0.5481\n",
      "\n",
      "Epoch 12/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1293 | train acc:\t0.5666\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      4943\n",
      "           1       0.61      0.45      0.52      2225\n",
      "           2       0.54      0.29      0.38      1539\n",
      "           3       0.50      0.51      0.51      2284\n",
      "           4       0.50      0.37      0.43      1669\n",
      "           5       0.76      0.74      0.75      1298\n",
      "\n",
      "    accuracy                           0.57     13958\n",
      "   macro avg       0.58      0.52      0.54     13958\n",
      "weighted avg       0.57      0.57      0.55     13958\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:\t1.2847 | test acc:\t0.5404\n",
      "\n",
      "Epoch 13/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1144 | train acc:\t0.5685\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      4943\n",
      "           1       0.61      0.47      0.54      2225\n",
      "           2       0.55      0.32      0.40      1539\n",
      "           3       0.50      0.50      0.50      2284\n",
      "           4       0.49      0.37      0.42      1669\n",
      "           5       0.75      0.75      0.75      1298\n",
      "\n",
      "    accuracy                           0.57     13958\n",
      "   macro avg       0.58      0.53      0.54     13958\n",
      "weighted avg       0.57      0.57      0.56     13958\n",
      "\n",
      "test loss:\t1.2511 | test acc:\t0.5576\n",
      "\n",
      "Epoch 14/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1081 | train acc:\t0.5667\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      4943\n",
      "           1       0.60      0.45      0.51      2225\n",
      "           2       0.55      0.31      0.39      1539\n",
      "           3       0.49      0.52      0.50      2284\n",
      "           4       0.49      0.38      0.43      1669\n",
      "           5       0.76      0.77      0.76      1298\n",
      "\n",
      "    accuracy                           0.57     13958\n",
      "   macro avg       0.58      0.52      0.54     13958\n",
      "weighted avg       0.57      0.57      0.56     13958\n",
      "\n",
      "test loss:\t1.2321 | test acc:\t0.5690\n",
      "\n",
      "Epoch 15/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0933 | train acc:\t0.5793\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65      4943\n",
      "           1       0.62      0.47      0.53      2225\n",
      "           2       0.57      0.33      0.42      1539\n",
      "           3       0.51      0.53      0.52      2284\n",
      "           4       0.50      0.39      0.44      1669\n",
      "           5       0.77      0.78      0.77      1298\n",
      "\n",
      "    accuracy                           0.58     13958\n",
      "   macro avg       0.59      0.54      0.56     13958\n",
      "weighted avg       0.58      0.58      0.57     13958\n",
      "\n",
      "test loss:\t1.2000 | test acc:\t0.5726\n",
      "\n",
      "Epoch 16/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0825 | train acc:\t0.5846\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      4943\n",
      "           1       0.63      0.49      0.55      2225\n",
      "           2       0.56      0.35      0.43      1539\n",
      "           3       0.52      0.52      0.52      2284\n",
      "           4       0.52      0.41      0.46      1669\n",
      "           5       0.77      0.78      0.77      1298\n",
      "\n",
      "    accuracy                           0.58     13958\n",
      "   macro avg       0.59      0.55      0.56     13958\n",
      "weighted avg       0.58      0.58      0.58     13958\n",
      "\n",
      "test loss:\t1.2209 | test acc:\t0.5665\n",
      "\n",
      "Epoch 17/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0736 | train acc:\t0.5911\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      4943\n",
      "           1       0.63      0.47      0.54      2225\n",
      "           2       0.59      0.37      0.45      1539\n",
      "           3       0.53      0.54      0.53      2284\n",
      "           4       0.52      0.41      0.46      1669\n",
      "           5       0.78      0.80      0.79      1298\n",
      "\n",
      "    accuracy                           0.59     13958\n",
      "   macro avg       0.60      0.56      0.57     13958\n",
      "weighted avg       0.59      0.59      0.58     13958\n",
      "\n",
      "test loss:\t1.2815 | test acc:\t0.5562\n",
      "\n",
      "Epoch 18/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0557 | train acc:\t0.5903\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      4943\n",
      "           1       0.64      0.48      0.55      2225\n",
      "           2       0.59      0.37      0.45      1539\n",
      "           3       0.52      0.52      0.52      2284\n",
      "           4       0.52      0.42      0.47      1669\n",
      "           5       0.78      0.79      0.78      1298\n",
      "\n",
      "    accuracy                           0.59     13958\n",
      "   macro avg       0.60      0.56      0.57     13958\n",
      "weighted avg       0.59      0.59      0.58     13958\n",
      "\n",
      "test loss:\t1.1991 | test acc:\t0.5828\n",
      "\n",
      "Epoch 19/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0617 | train acc:\t0.5907\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      4943\n",
      "           1       0.63      0.47      0.54      2225\n",
      "           2       0.60      0.38      0.47      1539\n",
      "           3       0.52      0.53      0.53      2284\n",
      "           4       0.52      0.42      0.46      1669\n",
      "           5       0.77      0.78      0.78      1298\n",
      "\n",
      "    accuracy                           0.59     13958\n",
      "   macro avg       0.61      0.55      0.57     13958\n",
      "weighted avg       0.59      0.59      0.58     13958\n",
      "\n",
      "test loss:\t1.2730 | test acc:\t0.5643\n",
      "\n",
      "Epoch 20/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0417 | train acc:\t0.5989\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      4943\n",
      "           1       0.64      0.50      0.56      2225\n",
      "           2       0.59      0.38      0.46      1539\n",
      "           3       0.53      0.53      0.53      2284\n",
      "           4       0.54      0.44      0.49      1669\n",
      "           5       0.78      0.80      0.79      1298\n",
      "\n",
      "    accuracy                           0.60     13958\n",
      "   macro avg       0.61      0.57      0.58     13958\n",
      "weighted avg       0.60      0.60      0.59     13958\n",
      "\n",
      "test loss:\t1.2144 | test acc:\t0.5775\n",
      "\n",
      "Epoch 21/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0261 | train acc:\t0.6061\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      4943\n",
      "           1       0.66      0.49      0.56      2225\n",
      "           2       0.59      0.39      0.47      1539\n",
      "           3       0.54      0.55      0.54      2284\n",
      "           4       0.57      0.44      0.50      1669\n",
      "           5       0.79      0.81      0.80      1298\n",
      "\n",
      "    accuracy                           0.61     13958\n",
      "   macro avg       0.62      0.57      0.59     13958\n",
      "weighted avg       0.61      0.61      0.60     13958\n",
      "\n",
      "test loss:\t1.1905 | test acc:\t0.5862\n",
      "\n",
      "Epoch 22/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0210 | train acc:\t0.6085\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.66      4943\n",
      "           1       0.65      0.50      0.56      2225\n",
      "           2       0.59      0.41      0.48      1539\n",
      "           3       0.54      0.55      0.55      2284\n",
      "           4       0.55      0.46      0.50      1669\n",
      "           5       0.79      0.81      0.80      1298\n",
      "\n",
      "    accuracy                           0.61     13958\n",
      "   macro avg       0.62      0.58      0.59     13958\n",
      "weighted avg       0.61      0.61      0.60     13958\n",
      "\n",
      "test loss:\t1.1807 | test acc:\t0.5907\n",
      "\n",
      "Epoch 23/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0040 | train acc:\t0.6148\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      4943\n",
      "           1       0.65      0.50      0.57      2225\n",
      "           2       0.61      0.42      0.50      1539\n",
      "           3       0.55      0.55      0.55      2284\n",
      "           4       0.57      0.47      0.52      1669\n",
      "           5       0.80      0.81      0.80      1298\n",
      "\n",
      "    accuracy                           0.61     13958\n",
      "   macro avg       0.63      0.58      0.60     13958\n",
      "weighted avg       0.62      0.61      0.61     13958\n",
      "\n",
      "test loss:\t1.1925 | test acc:\t0.5856\n",
      "\n",
      "Epoch 24/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0045 | train acc:\t0.6067\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      4943\n",
      "           1       0.65      0.49      0.56      2225\n",
      "           2       0.61      0.41      0.49      1539\n",
      "           3       0.53      0.55      0.54      2284\n",
      "           4       0.55      0.46      0.50      1669\n",
      "           5       0.79      0.80      0.80      1298\n",
      "\n",
      "    accuracy                           0.61     13958\n",
      "   macro avg       0.62      0.58      0.59     13958\n",
      "weighted avg       0.61      0.61      0.60     13958\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:\t1.2161 | test acc:\t0.5795\n",
      "\n",
      "Epoch 25/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9982 | train acc:\t0.6167\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      4943\n",
      "           1       0.67      0.51      0.58      2225\n",
      "           2       0.62      0.42      0.50      1539\n",
      "           3       0.56      0.56      0.56      2284\n",
      "           4       0.56      0.46      0.51      1669\n",
      "           5       0.80      0.80      0.80      1298\n",
      "\n",
      "    accuracy                           0.62     13958\n",
      "   macro avg       0.63      0.59      0.60     13958\n",
      "weighted avg       0.62      0.62      0.61     13958\n",
      "\n",
      "test loss:\t1.1492 | test acc:\t0.5917\n",
      "\n",
      "Epoch 26/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9750 | train acc:\t0.6252\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      4943\n",
      "           1       0.65      0.52      0.58      2225\n",
      "           2       0.62      0.46      0.53      1539\n",
      "           3       0.57      0.57      0.57      2284\n",
      "           4       0.58      0.49      0.53      1669\n",
      "           5       0.80      0.82      0.81      1298\n",
      "\n",
      "    accuracy                           0.63     13958\n",
      "   macro avg       0.64      0.60      0.62     13958\n",
      "weighted avg       0.63      0.63      0.62     13958\n",
      "\n",
      "test loss:\t1.1288 | test acc:\t0.5923\n",
      "\n",
      "Epoch 27/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9720 | train acc:\t0.6282\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      4943\n",
      "           1       0.69      0.52      0.59      2225\n",
      "           2       0.63      0.44      0.52      1539\n",
      "           3       0.56      0.58      0.57      2284\n",
      "           4       0.58      0.48      0.52      1669\n",
      "           5       0.81      0.82      0.81      1298\n",
      "\n",
      "    accuracy                           0.63     13958\n",
      "   macro avg       0.65      0.60      0.62     13958\n",
      "weighted avg       0.63      0.63      0.62     13958\n",
      "\n",
      "test loss:\t1.1491 | test acc:\t0.5978\n",
      "\n",
      "Epoch 28/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9614 | train acc:\t0.6281\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.67      4943\n",
      "           1       0.68      0.52      0.59      2225\n",
      "           2       0.64      0.44      0.52      1539\n",
      "           3       0.56      0.57      0.57      2284\n",
      "           4       0.58      0.50      0.54      1669\n",
      "           5       0.80      0.81      0.81      1298\n",
      "\n",
      "    accuracy                           0.63     13958\n",
      "   macro avg       0.65      0.60      0.62     13958\n",
      "weighted avg       0.63      0.63      0.62     13958\n",
      "\n",
      "test loss:\t1.1363 | test acc:\t0.5968\n",
      "\n",
      "Epoch 29/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9508 | train acc:\t0.6379\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      4943\n",
      "           1       0.69      0.52      0.59      2225\n",
      "           2       0.66      0.48      0.56      1539\n",
      "           3       0.57      0.58      0.58      2284\n",
      "           4       0.59      0.50      0.54      1669\n",
      "           5       0.81      0.82      0.81      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.66      0.61      0.63     13958\n",
      "weighted avg       0.64      0.64      0.63     13958\n",
      "\n",
      "test loss:\t1.1497 | test acc:\t0.5984\n",
      "\n",
      "Epoch 30/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9463 | train acc:\t0.6372\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      4943\n",
      "           1       0.69      0.52      0.60      2225\n",
      "           2       0.65      0.46      0.54      1539\n",
      "           3       0.58      0.59      0.58      2284\n",
      "           4       0.58      0.52      0.55      1669\n",
      "           5       0.81      0.83      0.82      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.66      0.61      0.63     13958\n",
      "weighted avg       0.64      0.64      0.63     13958\n",
      "\n",
      "test loss:\t1.1264 | test acc:\t0.5982\n",
      "\n",
      "Epoch 31/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9445 | train acc:\t0.6366\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      4943\n",
      "           1       0.68      0.52      0.59      2225\n",
      "           2       0.63      0.46      0.53      1539\n",
      "           3       0.58      0.59      0.58      2284\n",
      "           4       0.59      0.51      0.55      1669\n",
      "           5       0.82      0.82      0.82      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.65      0.61      0.63     13958\n",
      "weighted avg       0.64      0.64      0.63     13958\n",
      "\n",
      "test loss:\t1.1464 | test acc:\t0.5964\n",
      "\n",
      "Epoch 32/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9363 | train acc:\t0.6422\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.69      4943\n",
      "           1       0.69      0.53      0.60      2225\n",
      "           2       0.66      0.50      0.57      1539\n",
      "           3       0.58      0.59      0.59      2284\n",
      "           4       0.58      0.50      0.53      1669\n",
      "           5       0.82      0.83      0.82      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.66      0.62      0.63     13958\n",
      "weighted avg       0.64      0.64      0.64     13958\n",
      "\n",
      "test loss:\t1.1142 | test acc:\t0.5994\n",
      "\n",
      "Epoch 33/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9334 | train acc:\t0.6443\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      4943\n",
      "           1       0.69      0.55      0.61      2225\n",
      "           2       0.64      0.50      0.56      1539\n",
      "           3       0.59      0.59      0.59      2284\n",
      "           4       0.60      0.51      0.55      1669\n",
      "           5       0.82      0.82      0.82      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.66      0.62      0.64     13958\n",
      "weighted avg       0.65      0.64      0.64     13958\n",
      "\n",
      "test loss:\t1.1447 | test acc:\t0.5911\n",
      "\n",
      "Epoch 34/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9175 | train acc:\t0.6552\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      4943\n",
      "           1       0.71      0.55      0.62      2225\n",
      "           2       0.66      0.51      0.57      1539\n",
      "           3       0.59      0.60      0.59      2284\n",
      "           4       0.62      0.54      0.58      1669\n",
      "           5       0.81      0.85      0.83      1298\n",
      "\n",
      "    accuracy                           0.66     13958\n",
      "   macro avg       0.67      0.63      0.65     13958\n",
      "weighted avg       0.66      0.66      0.65     13958\n",
      "\n",
      "test loss:\t1.1412 | test acc:\t0.5972\n",
      "\n",
      "Epoch 35/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9139 | train acc:\t0.6479\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      4943\n",
      "           1       0.70      0.54      0.61      2225\n",
      "           2       0.65      0.50      0.57      1539\n",
      "           3       0.58      0.60      0.59      2284\n",
      "           4       0.61      0.53      0.57      1669\n",
      "           5       0.82      0.83      0.82      1298\n",
      "\n",
      "    accuracy                           0.65     13958\n",
      "   macro avg       0.67      0.63      0.64     13958\n",
      "weighted avg       0.65      0.65      0.64     13958\n",
      "\n",
      "test loss:\t1.1215 | test acc:\t0.5913\n",
      "\n",
      "Epoch 36/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9042 | train acc:\t0.6537\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      4943\n",
      "           1       0.70      0.56      0.62      2225\n",
      "           2       0.66      0.51      0.57      1539\n",
      "           3       0.59      0.59      0.59      2284\n",
      "           4       0.61      0.53      0.57      1669\n",
      "           5       0.82      0.84      0.83      1298\n",
      "\n",
      "    accuracy                           0.65     13958\n",
      "   macro avg       0.67      0.63      0.65     13958\n",
      "weighted avg       0.66      0.65      0.65     13958\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:\t1.1153 | test acc:\t0.6034\n",
      "\n",
      "Epoch 37/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8900 | train acc:\t0.6620\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      4943\n",
      "           1       0.71      0.55      0.62      2225\n",
      "           2       0.67      0.53      0.60      1539\n",
      "           3       0.61      0.62      0.61      2284\n",
      "           4       0.61      0.54      0.57      1669\n",
      "           5       0.82      0.83      0.83      1298\n",
      "\n",
      "    accuracy                           0.66     13958\n",
      "   macro avg       0.68      0.64      0.65     13958\n",
      "weighted avg       0.66      0.66      0.66     13958\n",
      "\n",
      "test loss:\t1.0958 | test acc:\t0.5915\n",
      "\n",
      "Epoch 38/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8980 | train acc:\t0.6599\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      4943\n",
      "           1       0.71      0.58      0.64      2225\n",
      "           2       0.67      0.50      0.57      1539\n",
      "           3       0.61      0.61      0.61      2284\n",
      "           4       0.61      0.54      0.57      1669\n",
      "           5       0.82      0.83      0.83      1298\n",
      "\n",
      "    accuracy                           0.66     13958\n",
      "   macro avg       0.68      0.64      0.65     13958\n",
      "weighted avg       0.66      0.66      0.66     13958\n",
      "\n",
      "test loss:\t1.1199 | test acc:\t0.5943\n",
      "\n",
      "Epoch 39/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8834 | train acc:\t0.6616\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      4943\n",
      "           1       0.71      0.56      0.63      2225\n",
      "           2       0.68      0.53      0.60      1539\n",
      "           3       0.60      0.61      0.60      2284\n",
      "           4       0.63      0.55      0.59      1669\n",
      "           5       0.83      0.83      0.83      1298\n",
      "\n",
      "    accuracy                           0.66     13958\n",
      "   macro avg       0.68      0.64      0.66     13958\n",
      "weighted avg       0.66      0.66      0.66     13958\n",
      "\n",
      "test loss:\t1.1133 | test acc:\t0.6012\n",
      "\n",
      "Epoch 40/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8787 | train acc:\t0.6664\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      4943\n",
      "           1       0.71      0.56      0.63      2225\n",
      "           2       0.67      0.53      0.59      1539\n",
      "           3       0.61      0.63      0.62      2284\n",
      "           4       0.63      0.55      0.59      1669\n",
      "           5       0.84      0.84      0.84      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.68      0.65      0.66     13958\n",
      "weighted avg       0.67      0.67      0.66     13958\n",
      "\n",
      "test loss:\t1.1010 | test acc:\t0.5968\n",
      "\n",
      "Epoch 41/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8708 | train acc:\t0.6667\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      4943\n",
      "           1       0.71      0.56      0.63      2225\n",
      "           2       0.68      0.54      0.60      1539\n",
      "           3       0.61      0.62      0.61      2284\n",
      "           4       0.62      0.56      0.59      1669\n",
      "           5       0.84      0.84      0.84      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.68      0.65      0.66     13958\n",
      "weighted avg       0.67      0.67      0.66     13958\n",
      "\n",
      "test loss:\t1.1007 | test acc:\t0.6030\n",
      "\n",
      "Epoch 42/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8682 | train acc:\t0.6703\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      4943\n",
      "           1       0.71      0.58      0.64      2225\n",
      "           2       0.67      0.53      0.59      1539\n",
      "           3       0.61      0.63      0.62      2284\n",
      "           4       0.62      0.56      0.59      1669\n",
      "           5       0.83      0.85      0.84      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.68      0.65      0.66     13958\n",
      "weighted avg       0.67      0.67      0.67     13958\n",
      "\n",
      "test loss:\t1.1151 | test acc:\t0.5951\n",
      "\n",
      "Epoch 43/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8524 | train acc:\t0.6701\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.75      0.70      4943\n",
      "           1       0.72      0.59      0.65      2225\n",
      "           2       0.67      0.54      0.60      1539\n",
      "           3       0.61      0.63      0.62      2284\n",
      "           4       0.64      0.57      0.60      1669\n",
      "           5       0.83      0.85      0.84      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.69      0.65      0.67     13958\n",
      "weighted avg       0.67      0.67      0.67     13958\n",
      "\n",
      "test loss:\t1.0961 | test acc:\t0.6030\n",
      "\n",
      "Epoch 44/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8394 | train acc:\t0.6823\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.72      4943\n",
      "           1       0.72      0.58      0.64      2225\n",
      "           2       0.70      0.58      0.63      1539\n",
      "           3       0.62      0.64      0.63      2284\n",
      "           4       0.64      0.58      0.61      1669\n",
      "           5       0.85      0.84      0.85      1298\n",
      "\n",
      "    accuracy                           0.68     13958\n",
      "   macro avg       0.70      0.66      0.68     13958\n",
      "weighted avg       0.68      0.68      0.68     13958\n",
      "\n",
      "test loss:\t1.0655 | test acc:\t0.6077\n",
      "\n",
      "Epoch 45/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8407 | train acc:\t0.6813\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      4943\n",
      "           1       0.71      0.60      0.65      2225\n",
      "           2       0.69      0.55      0.61      1539\n",
      "           3       0.63      0.63      0.63      2284\n",
      "           4       0.64      0.58      0.61      1669\n",
      "           5       0.84      0.85      0.85      1298\n",
      "\n",
      "    accuracy                           0.68     13958\n",
      "   macro avg       0.70      0.66      0.68     13958\n",
      "weighted avg       0.68      0.68      0.68     13958\n",
      "\n",
      "test loss:\t1.1243 | test acc:\t0.5929\n",
      "\n",
      "Epoch 46/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8392 | train acc:\t0.6753\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      4943\n",
      "           1       0.71      0.58      0.64      2225\n",
      "           2       0.67      0.56      0.61      1539\n",
      "           3       0.62      0.63      0.62      2284\n",
      "           4       0.64      0.57      0.60      1669\n",
      "           5       0.85      0.85      0.85      1298\n",
      "\n",
      "    accuracy                           0.68     13958\n",
      "   macro avg       0.69      0.66      0.67     13958\n",
      "weighted avg       0.68      0.68      0.67     13958\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:\t1.1100 | test acc:\t0.5963\n",
      "\n",
      "{'val_Other_f1': 0.6334212419084153, 'val_Other_precision': 0.605685465382852, 'val_Other_recall': 0.6638190954773869, 'val_Saggital_Right_f1': 0.5407788390889052, 'val_Saggital_Right_precision': 0.6444833625218914, 'val_Saggital_Right_recall': 0.46582278481012657, 'val_Transverse_Right_f1': 0.5833333333333334, 'val_Transverse_Right_precision': 0.735, 'val_Transverse_Right_recall': 0.48355263157894735, 'val_Saggital_Left_f1': 0.47706422018348627, 'val_Saggital_Left_precision': 0.4323040380047506, 'val_Saggital_Left_recall': 0.5321637426900585, 'val_Transverse_Left_f1': 0.4990892531876139, 'val_Transverse_Left_precision': 0.4798598949211909, 'val_Transverse_Left_recall': 0.5199240986717267, 'val_Bladder_f1': 0.8237704918032787, 'val_Bladder_precision': 0.7960396039603961, 'val_Bladder_recall': 0.8535031847133758, 'test_precision_average_macro': 0.6155620607985135, 'test_recall_average_macro': 0.5864642563236037, 'test_f1score_average_macro': 0.5929095632508387, 'test_precision_average_micro': 0.5962524654832347, 'test_recall_average_micro': 0.5962524654832347, 'test_f1score_average_micro': 0.5962524654832347, 'test_precision_average_weighted': 0.6084522115558646, 'test_recall_average_weighted': 0.5962524654832347, 'test_f1score_average_weighted': 0.5956051450925779, 'test_acc_average': 0.5962524654832347}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.66      0.63      1990\n",
      "           1       0.64      0.47      0.54       790\n",
      "           2       0.73      0.48      0.58       608\n",
      "           3       0.43      0.53      0.48       684\n",
      "           4       0.48      0.52      0.50       527\n",
      "           5       0.80      0.85      0.82       471\n",
      "\n",
      "    accuracy                           0.60      5070\n",
      "   macro avg       0.62      0.59      0.59      5070\n",
      "weighted avg       0.61      0.60      0.60      5070\n",
      "\n",
      "\n",
      "[[1321  101   84  210  194   80]\n",
      " [ 187  368    6  203   18    8]\n",
      " [ 224    8  294   20   58    4]\n",
      " [ 204   81    1  364   26    8]\n",
      " [ 179   13   13   45  274    3]\n",
      " [  66    0    2    0    1  402]]\n",
      "Training complete in 22m 25s\n",
      "Best val acc: 0.596252\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/denizjafari/hnultra_test\" target=\"_blank\">https://app.wandb.ai/denizjafari/hnultra_test</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/denizjafari/hnultra_test/runs/12ean8yu\" target=\"_blank\">https://app.wandb.ai/denizjafari/hnultra_test/runs/12ean8yu</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t conv1.weight\n",
      "\t conv1.bias\n",
      "\t conv2.weight\n",
      "\t conv2.bias\n",
      "\t conv3.weight\n",
      "\t conv3.bias\n",
      "\t linear1.weight\n",
      "\t linear1.bias\n",
      "\t linear2.weight\n",
      "\t linear2.bias\n",
      "Epoch 1/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.6244 | train acc:\t0.3617\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.97      0.53      4943\n",
      "           1       0.29      0.01      0.03      2225\n",
      "           2       0.07      0.00      0.01      1539\n",
      "           3       0.35      0.03      0.06      2284\n",
      "           4       0.00      0.00      0.00      1669\n",
      "           5       0.59      0.11      0.19      1298\n",
      "\n",
      "    accuracy                           0.36     13958\n",
      "   macro avg       0.28      0.19      0.14     13958\n",
      "weighted avg       0.29      0.36      0.22     13958\n",
      "\n",
      "test loss:\t1.5880 | test acc:\t0.4057\n",
      "\n",
      "Epoch 2/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.4388 | train acc:\t0.4299\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.80      0.55      4943\n",
      "           1       0.38      0.19      0.26      2225\n",
      "           2       0.50      0.00      0.00      1539\n",
      "           3       0.38      0.32      0.35      2284\n",
      "           4       0.38      0.02      0.04      1669\n",
      "           5       0.58      0.65      0.61      1298\n",
      "\n",
      "    accuracy                           0.43     13958\n",
      "   macro avg       0.44      0.33      0.30     13958\n",
      "weighted avg       0.43      0.43      0.36     13958\n",
      "\n",
      "test loss:\t1.4908 | test acc:\t0.4661\n",
      "\n",
      "Epoch 3/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3607 | train acc:\t0.4658\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.78      0.58      4943\n",
      "           1       0.44      0.33      0.38      2225\n",
      "           2       0.63      0.03      0.05      1539\n",
      "           3       0.39      0.37      0.38      2284\n",
      "           4       0.42      0.08      0.13      1669\n",
      "           5       0.64      0.68      0.66      1298\n",
      "\n",
      "    accuracy                           0.47     13958\n",
      "   macro avg       0.50      0.38      0.36     13958\n",
      "weighted avg       0.48      0.47      0.41     13958\n",
      "\n",
      "test loss:\t1.4198 | test acc:\t0.5061\n",
      "\n",
      "Epoch 4/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3109 | train acc:\t0.4898\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.76      0.60      4943\n",
      "           1       0.46      0.39      0.42      2225\n",
      "           2       0.55      0.10      0.16      1539\n",
      "           3       0.42      0.41      0.42      2284\n",
      "           4       0.44      0.14      0.21      1669\n",
      "           5       0.66      0.67      0.66      1298\n",
      "\n",
      "    accuracy                           0.49     13958\n",
      "   macro avg       0.50      0.41      0.41     13958\n",
      "weighted avg       0.49      0.49      0.45     13958\n",
      "\n",
      "test loss:\t1.3741 | test acc:\t0.5249\n",
      "\n",
      "Epoch 5/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2768 | train acc:\t0.5069\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.76      0.61      4943\n",
      "           1       0.49      0.40      0.44      2225\n",
      "           2       0.48      0.15      0.23      1539\n",
      "           3       0.44      0.44      0.44      2284\n",
      "           4       0.45      0.18      0.25      1669\n",
      "           5       0.69      0.70      0.70      1298\n",
      "\n",
      "    accuracy                           0.51     13958\n",
      "   macro avg       0.51      0.44      0.44     13958\n",
      "weighted avg       0.50      0.51      0.48     13958\n",
      "\n",
      "test loss:\t1.4222 | test acc:\t0.4923\n",
      "\n",
      "Epoch 6/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2528 | train acc:\t0.5167\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.74      0.61      4943\n",
      "           1       0.53      0.40      0.46      2225\n",
      "           2       0.48      0.19      0.27      1539\n",
      "           3       0.45      0.46      0.45      2284\n",
      "           4       0.42      0.23      0.30      1669\n",
      "           5       0.70      0.72      0.71      1298\n",
      "\n",
      "    accuracy                           0.52     13958\n",
      "   macro avg       0.52      0.46      0.47     13958\n",
      "weighted avg       0.51      0.52      0.49     13958\n",
      "\n",
      "test loss:\t1.3474 | test acc:\t0.5312\n",
      "\n",
      "Epoch 7/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2212 | train acc:\t0.5249\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.74      0.62      4943\n",
      "           1       0.52      0.43      0.47      2225\n",
      "           2       0.51      0.21      0.29      1539\n",
      "           3       0.45      0.45      0.45      2284\n",
      "           4       0.45      0.26      0.33      1669\n",
      "           5       0.73      0.72      0.72      1298\n",
      "\n",
      "    accuracy                           0.52     13958\n",
      "   macro avg       0.53      0.47      0.48     13958\n",
      "weighted avg       0.52      0.52      0.50     13958\n",
      "\n",
      "test loss:\t1.3328 | test acc:\t0.5487\n",
      "\n",
      "Epoch 8/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1981 | train acc:\t0.5392\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.74      0.62      4943\n",
      "           1       0.57      0.42      0.48      2225\n",
      "           2       0.50      0.23      0.32      1539\n",
      "           3       0.47      0.48      0.48      2284\n",
      "           4       0.47      0.32      0.38      1669\n",
      "           5       0.73      0.73      0.73      1298\n",
      "\n",
      "    accuracy                           0.54     13958\n",
      "   macro avg       0.55      0.49      0.50     13958\n",
      "weighted avg       0.54      0.54      0.52     13958\n",
      "\n",
      "test loss:\t1.3299 | test acc:\t0.5430\n",
      "\n",
      "Epoch 9/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1960 | train acc:\t0.5388\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.62      4943\n",
      "           1       0.57      0.41      0.48      2225\n",
      "           2       0.53      0.27      0.35      1539\n",
      "           3       0.46      0.47      0.46      2284\n",
      "           4       0.47      0.31      0.37      1669\n",
      "           5       0.73      0.73      0.73      1298\n",
      "\n",
      "    accuracy                           0.54     13958\n",
      "   macro avg       0.55      0.49      0.50     13958\n",
      "weighted avg       0.54      0.54      0.52     13958\n",
      "\n",
      "test loss:\t1.3183 | test acc:\t0.5475\n",
      "\n",
      "Epoch 10/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1717 | train acc:\t0.5504\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      4943\n",
      "           1       0.60      0.44      0.51      2225\n",
      "           2       0.53      0.27      0.36      1539\n",
      "           3       0.49      0.49      0.49      2284\n",
      "           4       0.47      0.34      0.40      1669\n",
      "           5       0.74      0.74      0.74      1298\n",
      "\n",
      "    accuracy                           0.55     13958\n",
      "   macro avg       0.56      0.50      0.52     13958\n",
      "weighted avg       0.55      0.55      0.54     13958\n",
      "\n",
      "test loss:\t1.2957 | test acc:\t0.5465\n",
      "\n",
      "Epoch 11/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1629 | train acc:\t0.5506\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      4943\n",
      "           1       0.59      0.45      0.51      2225\n",
      "           2       0.50      0.28      0.36      1539\n",
      "           3       0.49      0.49      0.49      2284\n",
      "           4       0.47      0.35      0.40      1669\n",
      "           5       0.75      0.74      0.75      1298\n",
      "\n",
      "    accuracy                           0.55     13958\n",
      "   macro avg       0.56      0.51      0.52     13958\n",
      "weighted avg       0.55      0.55      0.54     13958\n",
      "\n",
      "test loss:\t1.2972 | test acc:\t0.5469\n",
      "\n",
      "Epoch 12/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1462 | train acc:\t0.5522\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      4943\n",
      "           1       0.59      0.43      0.50      2225\n",
      "           2       0.53      0.28      0.36      1539\n",
      "           3       0.49      0.49      0.49      2284\n",
      "           4       0.48      0.35      0.40      1669\n",
      "           5       0.74      0.75      0.74      1298\n",
      "\n",
      "    accuracy                           0.55     13958\n",
      "   macro avg       0.56      0.51      0.52     13958\n",
      "weighted avg       0.55      0.55      0.54     13958\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:\t1.2705 | test acc:\t0.5469\n",
      "\n",
      "Epoch 13/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1277 | train acc:\t0.5637\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.63      4943\n",
      "           1       0.60      0.45      0.51      2225\n",
      "           2       0.55      0.31      0.40      1539\n",
      "           3       0.49      0.50      0.49      2284\n",
      "           4       0.49      0.36      0.42      1669\n",
      "           5       0.76      0.77      0.76      1298\n",
      "\n",
      "    accuracy                           0.56     13958\n",
      "   macro avg       0.58      0.52      0.54     13958\n",
      "weighted avg       0.56      0.56      0.55     13958\n",
      "\n",
      "test loss:\t1.2541 | test acc:\t0.5629\n",
      "\n",
      "Epoch 14/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1114 | train acc:\t0.5726\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      4943\n",
      "           1       0.62      0.47      0.53      2225\n",
      "           2       0.57      0.32      0.41      1539\n",
      "           3       0.50      0.51      0.51      2284\n",
      "           4       0.50      0.38      0.43      1669\n",
      "           5       0.74      0.76      0.75      1298\n",
      "\n",
      "    accuracy                           0.57     13958\n",
      "   macro avg       0.58      0.53      0.55     13958\n",
      "weighted avg       0.57      0.57      0.56     13958\n",
      "\n",
      "test loss:\t1.2569 | test acc:\t0.5714\n",
      "\n",
      "Epoch 15/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0989 | train acc:\t0.5772\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      4943\n",
      "           1       0.63      0.45      0.53      2225\n",
      "           2       0.56      0.34      0.42      1539\n",
      "           3       0.50      0.52      0.51      2284\n",
      "           4       0.52      0.40      0.45      1669\n",
      "           5       0.77      0.77      0.77      1298\n",
      "\n",
      "    accuracy                           0.58     13958\n",
      "   macro avg       0.59      0.54      0.55     13958\n",
      "weighted avg       0.58      0.58      0.57     13958\n",
      "\n",
      "test loss:\t1.2043 | test acc:\t0.5694\n",
      "\n",
      "Epoch 16/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0896 | train acc:\t0.5802\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      4943\n",
      "           1       0.64      0.47      0.54      2225\n",
      "           2       0.58      0.34      0.43      1539\n",
      "           3       0.50      0.51      0.51      2284\n",
      "           4       0.52      0.41      0.46      1669\n",
      "           5       0.75      0.78      0.76      1298\n",
      "\n",
      "    accuracy                           0.58     13958\n",
      "   macro avg       0.59      0.54      0.56     13958\n",
      "weighted avg       0.58      0.58      0.57     13958\n",
      "\n",
      "test loss:\t1.2216 | test acc:\t0.5751\n",
      "\n",
      "Epoch 17/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0706 | train acc:\t0.5895\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      4943\n",
      "           1       0.64      0.48      0.54      2225\n",
      "           2       0.59      0.37      0.46      1539\n",
      "           3       0.52      0.54      0.53      2284\n",
      "           4       0.53      0.41      0.46      1669\n",
      "           5       0.78      0.78      0.78      1298\n",
      "\n",
      "    accuracy                           0.59     13958\n",
      "   macro avg       0.61      0.55      0.57     13958\n",
      "weighted avg       0.59      0.59      0.58     13958\n",
      "\n",
      "test loss:\t1.2223 | test acc:\t0.5718\n",
      "\n",
      "Epoch 18/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0608 | train acc:\t0.5913\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      4943\n",
      "           1       0.65      0.47      0.54      2225\n",
      "           2       0.60      0.39      0.47      1539\n",
      "           3       0.51      0.53      0.52      2284\n",
      "           4       0.53      0.41      0.46      1669\n",
      "           5       0.78      0.78      0.78      1298\n",
      "\n",
      "    accuracy                           0.59     13958\n",
      "   macro avg       0.61      0.56      0.57     13958\n",
      "weighted avg       0.59      0.59      0.58     13958\n",
      "\n",
      "test loss:\t1.2144 | test acc:\t0.5761\n",
      "\n",
      "Epoch 19/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0505 | train acc:\t0.5931\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.65      4943\n",
      "           1       0.65      0.48      0.55      2225\n",
      "           2       0.57      0.37      0.45      1539\n",
      "           3       0.52      0.54      0.53      2284\n",
      "           4       0.53      0.42      0.47      1669\n",
      "           5       0.76      0.79      0.77      1298\n",
      "\n",
      "    accuracy                           0.59     13958\n",
      "   macro avg       0.60      0.56      0.57     13958\n",
      "weighted avg       0.59      0.59      0.59     13958\n",
      "\n",
      "test loss:\t1.2056 | test acc:\t0.5781\n",
      "\n",
      "Epoch 20/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0504 | train acc:\t0.5990\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.66      4943\n",
      "           1       0.64      0.49      0.55      2225\n",
      "           2       0.60      0.39      0.47      1539\n",
      "           3       0.53      0.53      0.53      2284\n",
      "           4       0.55      0.44      0.49      1669\n",
      "           5       0.78      0.79      0.78      1298\n",
      "\n",
      "    accuracy                           0.60     13958\n",
      "   macro avg       0.61      0.56      0.58     13958\n",
      "weighted avg       0.60      0.60      0.59     13958\n",
      "\n",
      "test loss:\t1.1904 | test acc:\t0.5903\n",
      "\n",
      "Epoch 21/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0337 | train acc:\t0.6032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      4943\n",
      "           1       0.66      0.48      0.56      2225\n",
      "           2       0.61      0.40      0.49      1539\n",
      "           3       0.53      0.55      0.54      2284\n",
      "           4       0.56      0.45      0.50      1669\n",
      "           5       0.78      0.79      0.78      1298\n",
      "\n",
      "    accuracy                           0.60     13958\n",
      "   macro avg       0.62      0.57      0.59     13958\n",
      "weighted avg       0.61      0.60      0.60     13958\n",
      "\n",
      "test loss:\t1.1970 | test acc:\t0.5903\n",
      "\n",
      "Epoch 22/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0215 | train acc:\t0.6073\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.66      4943\n",
      "           1       0.65      0.50      0.56      2225\n",
      "           2       0.60      0.41      0.49      1539\n",
      "           3       0.53      0.55      0.54      2284\n",
      "           4       0.55      0.45      0.50      1669\n",
      "           5       0.79      0.81      0.80      1298\n",
      "\n",
      "    accuracy                           0.61     13958\n",
      "   macro avg       0.62      0.58      0.59     13958\n",
      "weighted avg       0.61      0.61      0.60     13958\n",
      "\n",
      "test loss:\t1.1711 | test acc:\t0.5911\n",
      "\n",
      "Epoch 23/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0177 | train acc:\t0.6103\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.66      4943\n",
      "           1       0.67      0.50      0.57      2225\n",
      "           2       0.60      0.42      0.49      1539\n",
      "           3       0.54      0.56      0.55      2284\n",
      "           4       0.56      0.45      0.50      1669\n",
      "           5       0.79      0.80      0.80      1298\n",
      "\n",
      "    accuracy                           0.61     13958\n",
      "   macro avg       0.63      0.58      0.60     13958\n",
      "weighted avg       0.61      0.61      0.60     13958\n",
      "\n",
      "test loss:\t1.1733 | test acc:\t0.5892\n",
      "\n",
      "Epoch 24/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0086 | train acc:\t0.6146\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      4943\n",
      "           1       0.66      0.50      0.57      2225\n",
      "           2       0.62      0.43      0.51      1539\n",
      "           3       0.54      0.55      0.55      2284\n",
      "           4       0.55      0.47      0.51      1669\n",
      "           5       0.80      0.80      0.80      1298\n",
      "\n",
      "    accuracy                           0.61     13958\n",
      "   macro avg       0.63      0.58      0.60     13958\n",
      "weighted avg       0.62      0.61      0.61     13958\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:\t1.1675 | test acc:\t0.5949\n",
      "\n",
      "Epoch 25/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0001 | train acc:\t0.6195\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      4943\n",
      "           1       0.68      0.51      0.58      2225\n",
      "           2       0.61      0.44      0.51      1539\n",
      "           3       0.54      0.56      0.55      2284\n",
      "           4       0.57      0.49      0.53      1669\n",
      "           5       0.80      0.80      0.80      1298\n",
      "\n",
      "    accuracy                           0.62     13958\n",
      "   macro avg       0.64      0.59      0.61     13958\n",
      "weighted avg       0.62      0.62      0.61     13958\n",
      "\n",
      "test loss:\t1.1989 | test acc:\t0.5888\n",
      "\n",
      "Epoch 26/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9907 | train acc:\t0.6206\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      4943\n",
      "           1       0.68      0.51      0.58      2225\n",
      "           2       0.61      0.43      0.50      1539\n",
      "           3       0.56      0.57      0.57      2284\n",
      "           4       0.57      0.46      0.51      1669\n",
      "           5       0.80      0.81      0.80      1298\n",
      "\n",
      "    accuracy                           0.62     13958\n",
      "   macro avg       0.64      0.59      0.61     13958\n",
      "weighted avg       0.62      0.62      0.61     13958\n",
      "\n",
      "test loss:\t1.1735 | test acc:\t0.5925\n",
      "\n",
      "Epoch 27/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9739 | train acc:\t0.6309\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      4943\n",
      "           1       0.68      0.53      0.60      2225\n",
      "           2       0.63      0.45      0.53      1539\n",
      "           3       0.56      0.57      0.57      2284\n",
      "           4       0.60      0.49      0.54      1669\n",
      "           5       0.79      0.82      0.80      1298\n",
      "\n",
      "    accuracy                           0.63     13958\n",
      "   macro avg       0.65      0.60      0.62     13958\n",
      "weighted avg       0.63      0.63      0.63     13958\n",
      "\n",
      "test loss:\t1.1621 | test acc:\t0.5980\n",
      "\n",
      "Epoch 28/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9757 | train acc:\t0.6292\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      4943\n",
      "           1       0.67      0.51      0.58      2225\n",
      "           2       0.61      0.45      0.52      1539\n",
      "           3       0.57      0.58      0.57      2284\n",
      "           4       0.58      0.50      0.54      1669\n",
      "           5       0.80      0.82      0.81      1298\n",
      "\n",
      "    accuracy                           0.63     13958\n",
      "   macro avg       0.64      0.60      0.62     13958\n",
      "weighted avg       0.63      0.63      0.62     13958\n",
      "\n",
      "test loss:\t1.1776 | test acc:\t0.5913\n",
      "\n",
      "Epoch 29/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9629 | train acc:\t0.6323\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      4943\n",
      "           1       0.69      0.53      0.60      2225\n",
      "           2       0.64      0.47      0.54      1539\n",
      "           3       0.56      0.59      0.57      2284\n",
      "           4       0.57      0.49      0.53      1669\n",
      "           5       0.80      0.81      0.80      1298\n",
      "\n",
      "    accuracy                           0.63     13958\n",
      "   macro avg       0.65      0.61      0.62     13958\n",
      "weighted avg       0.63      0.63      0.63     13958\n",
      "\n",
      "test loss:\t1.1439 | test acc:\t0.6006\n",
      "\n",
      "Epoch 30/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9544 | train acc:\t0.6391\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      4943\n",
      "           1       0.70      0.54      0.61      2225\n",
      "           2       0.64      0.48      0.55      1539\n",
      "           3       0.58      0.59      0.58      2284\n",
      "           4       0.59      0.50      0.54      1669\n",
      "           5       0.81      0.82      0.81      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.66      0.61      0.63     13958\n",
      "weighted avg       0.64      0.64      0.63     13958\n",
      "\n",
      "test loss:\t1.1466 | test acc:\t0.5953\n",
      "\n",
      "Epoch 31/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9487 | train acc:\t0.6349\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      4943\n",
      "           1       0.69      0.53      0.60      2225\n",
      "           2       0.63      0.49      0.55      1539\n",
      "           3       0.57      0.58      0.57      2284\n",
      "           4       0.59      0.50      0.54      1669\n",
      "           5       0.80      0.82      0.81      1298\n",
      "\n",
      "    accuracy                           0.63     13958\n",
      "   macro avg       0.65      0.61      0.63     13958\n",
      "weighted avg       0.64      0.63      0.63     13958\n",
      "\n",
      "test loss:\t1.1278 | test acc:\t0.5963\n",
      "\n",
      "Epoch 32/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9366 | train acc:\t0.6425\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      4943\n",
      "           1       0.69      0.54      0.61      2225\n",
      "           2       0.66      0.49      0.56      1539\n",
      "           3       0.57      0.58      0.58      2284\n",
      "           4       0.61      0.51      0.56      1669\n",
      "           5       0.81      0.84      0.82      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.66      0.62      0.63     13958\n",
      "weighted avg       0.65      0.64      0.64     13958\n",
      "\n",
      "test loss:\t1.1652 | test acc:\t0.5846\n",
      "\n",
      "Epoch 33/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9434 | train acc:\t0.6411\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      4943\n",
      "           1       0.68      0.54      0.60      2225\n",
      "           2       0.66      0.48      0.56      1539\n",
      "           3       0.58      0.59      0.59      2284\n",
      "           4       0.58      0.52      0.55      1669\n",
      "           5       0.81      0.82      0.82      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.66      0.62      0.63     13958\n",
      "weighted avg       0.64      0.64      0.64     13958\n",
      "\n",
      "test loss:\t1.1529 | test acc:\t0.5864\n",
      "\n",
      "Epoch 34/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9330 | train acc:\t0.6398\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      4943\n",
      "           1       0.69      0.54      0.61      2225\n",
      "           2       0.64      0.48      0.55      1539\n",
      "           3       0.57      0.58      0.57      2284\n",
      "           4       0.61      0.51      0.55      1669\n",
      "           5       0.82      0.83      0.83      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.66      0.62      0.63     13958\n",
      "weighted avg       0.64      0.64      0.64     13958\n",
      "\n",
      "test loss:\t1.1428 | test acc:\t0.5935\n",
      "\n",
      "Epoch 35/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9273 | train acc:\t0.6461\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      4943\n",
      "           1       0.70      0.54      0.61      2225\n",
      "           2       0.66      0.51      0.58      1539\n",
      "           3       0.58      0.60      0.59      2284\n",
      "           4       0.60      0.52      0.56      1669\n",
      "           5       0.81      0.83      0.82      1298\n",
      "\n",
      "    accuracy                           0.65     13958\n",
      "   macro avg       0.66      0.63      0.64     13958\n",
      "weighted avg       0.65      0.65      0.64     13958\n",
      "\n",
      "test loss:\t1.1566 | test acc:\t0.5888\n",
      "\n",
      "Epoch 36/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9096 | train acc:\t0.6507\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      4943\n",
      "           1       0.69      0.55      0.61      2225\n",
      "           2       0.68      0.52      0.59      1539\n",
      "           3       0.58      0.59      0.58      2284\n",
      "           4       0.62      0.53      0.57      1669\n",
      "           5       0.82      0.83      0.82      1298\n",
      "\n",
      "    accuracy                           0.65     13958\n",
      "   macro avg       0.67      0.63      0.64     13958\n",
      "weighted avg       0.65      0.65      0.65     13958\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:\t1.1188 | test acc:\t0.6034\n",
      "\n",
      "Epoch 37/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9109 | train acc:\t0.6542\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      4943\n",
      "           1       0.68      0.54      0.60      2225\n",
      "           2       0.66      0.51      0.58      1539\n",
      "           3       0.60      0.62      0.61      2284\n",
      "           4       0.61      0.53      0.57      1669\n",
      "           5       0.82      0.83      0.82      1298\n",
      "\n",
      "    accuracy                           0.65     13958\n",
      "   macro avg       0.67      0.63      0.65     13958\n",
      "weighted avg       0.66      0.65      0.65     13958\n",
      "\n",
      "test loss:\t1.1244 | test acc:\t0.6043\n",
      "\n",
      "Epoch 38/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8887 | train acc:\t0.6619\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      4943\n",
      "           1       0.71      0.57      0.63      2225\n",
      "           2       0.68      0.53      0.60      1539\n",
      "           3       0.59      0.60      0.59      2284\n",
      "           4       0.63      0.54      0.58      1669\n",
      "           5       0.83      0.85      0.84      1298\n",
      "\n",
      "    accuracy                           0.66     13958\n",
      "   macro avg       0.68      0.64      0.66     13958\n",
      "weighted avg       0.66      0.66      0.66     13958\n",
      "\n",
      "test loss:\t1.1277 | test acc:\t0.5980\n",
      "\n",
      "Epoch 39/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8931 | train acc:\t0.6621\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      4943\n",
      "           1       0.71      0.57      0.63      2225\n",
      "           2       0.66      0.51      0.58      1539\n",
      "           3       0.59      0.60      0.60      2284\n",
      "           4       0.62      0.54      0.58      1669\n",
      "           5       0.83      0.85      0.84      1298\n",
      "\n",
      "    accuracy                           0.66     13958\n",
      "   macro avg       0.68      0.64      0.66     13958\n",
      "weighted avg       0.66      0.66      0.66     13958\n",
      "\n",
      "test loss:\t1.1003 | test acc:\t0.6075\n",
      "\n",
      "Epoch 40/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8907 | train acc:\t0.6568\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      4943\n",
      "           1       0.70      0.56      0.62      2225\n",
      "           2       0.66      0.54      0.59      1539\n",
      "           3       0.60      0.60      0.60      2284\n",
      "           4       0.63      0.54      0.58      1669\n",
      "           5       0.82      0.83      0.83      1298\n",
      "\n",
      "    accuracy                           0.66     13958\n",
      "   macro avg       0.67      0.64      0.65     13958\n",
      "weighted avg       0.66      0.66      0.65     13958\n",
      "\n",
      "test loss:\t1.1211 | test acc:\t0.6049\n",
      "\n",
      "Epoch 41/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8909 | train acc:\t0.6630\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      4943\n",
      "           1       0.71      0.57      0.63      2225\n",
      "           2       0.65      0.52      0.57      1539\n",
      "           3       0.61      0.61      0.61      2284\n",
      "           4       0.61      0.54      0.57      1669\n",
      "           5       0.83      0.86      0.84      1298\n",
      "\n",
      "    accuracy                           0.66     13958\n",
      "   macro avg       0.68      0.64      0.66     13958\n",
      "weighted avg       0.66      0.66      0.66     13958\n",
      "\n",
      "test loss:\t1.1076 | test acc:\t0.5976\n",
      "\n",
      "Epoch 42/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8723 | train acc:\t0.6702\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      4943\n",
      "           1       0.72      0.57      0.64      2225\n",
      "           2       0.67      0.53      0.59      1539\n",
      "           3       0.61      0.63      0.62      2284\n",
      "           4       0.62      0.56      0.59      1669\n",
      "           5       0.83      0.84      0.83      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.68      0.65      0.66     13958\n",
      "weighted avg       0.67      0.67      0.67     13958\n",
      "\n",
      "test loss:\t1.1349 | test acc:\t0.5911\n",
      "\n",
      "Epoch 43/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8668 | train acc:\t0.6736\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      4943\n",
      "           1       0.71      0.59      0.64      2225\n",
      "           2       0.69      0.56      0.61      1539\n",
      "           3       0.62      0.62      0.62      2284\n",
      "           4       0.63      0.57      0.60      1669\n",
      "           5       0.83      0.85      0.84      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.69      0.66      0.67     13958\n",
      "weighted avg       0.67      0.67      0.67     13958\n",
      "\n",
      "test loss:\t1.1070 | test acc:\t0.5998\n",
      "\n",
      "Epoch 44/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8680 | train acc:\t0.6635\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      4943\n",
      "           1       0.70      0.58      0.63      2225\n",
      "           2       0.67      0.54      0.60      1539\n",
      "           3       0.61      0.62      0.61      2284\n",
      "           4       0.62      0.55      0.58      1669\n",
      "           5       0.83      0.84      0.83      1298\n",
      "\n",
      "    accuracy                           0.66     13958\n",
      "   macro avg       0.68      0.65      0.66     13958\n",
      "weighted avg       0.66      0.66      0.66     13958\n",
      "\n",
      "test loss:\t1.1023 | test acc:\t0.6045\n",
      "\n",
      "Epoch 45/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8584 | train acc:\t0.6755\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      4943\n",
      "           1       0.72      0.58      0.65      2225\n",
      "           2       0.67      0.56      0.61      1539\n",
      "           3       0.63      0.64      0.63      2284\n",
      "           4       0.64      0.56      0.59      1669\n",
      "           5       0.83      0.83      0.83      1298\n",
      "\n",
      "    accuracy                           0.68     13958\n",
      "   macro avg       0.69      0.66      0.67     13958\n",
      "weighted avg       0.68      0.68      0.67     13958\n",
      "\n",
      "test loss:\t1.1001 | test acc:\t0.6075\n",
      "\n",
      "Epoch 46/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8535 | train acc:\t0.6717\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      4943\n",
      "           1       0.72      0.58      0.64      2225\n",
      "           2       0.65      0.54      0.59      1539\n",
      "           3       0.62      0.62      0.62      2284\n",
      "           4       0.63      0.57      0.60      1669\n",
      "           5       0.84      0.84      0.84      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.69      0.65      0.67     13958\n",
      "weighted avg       0.67      0.67      0.67     13958\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:\t1.1033 | test acc:\t0.6087\n",
      "\n",
      "{'val_Other_f1': 0.6500576701268741, 'val_Other_precision': 0.6008528784648187, 'val_Other_recall': 0.7080402010050251, 'val_Saggital_Right_f1': 0.5636232925952552, 'val_Saggital_Right_precision': 0.6522462562396006, 'val_Saggital_Right_recall': 0.4962025316455696, 'val_Transverse_Right_f1': 0.5946935041171089, 'val_Transverse_Right_precision': 0.6701030927835051, 'val_Transverse_Right_recall': 0.5345394736842105, 'val_Saggital_Left_f1': 0.4565537555228277, 'val_Saggital_Left_precision': 0.4599406528189911, 'val_Saggital_Left_recall': 0.45321637426900585, 'val_Transverse_Left_f1': 0.5020661157024793, 'val_Transverse_Left_precision': 0.5510204081632653, 'val_Transverse_Left_recall': 0.46110056925996207, 'val_Bladder_f1': 0.8180904522613066, 'val_Bladder_precision': 0.7767175572519084, 'val_Bladder_recall': 0.8641188959660298, 'test_precision_average_macro': 0.6184801409536815, 'test_recall_average_macro': 0.5862030076383005, 'test_f1score_average_macro': 0.5975141317209753, 'test_precision_average_micro': 0.6086785009861932, 'test_recall_average_micro': 0.6086785009861932, 'test_f1score_average_micro': 0.6086785009861932, 'test_precision_average_weighted': 0.6093127380833381, 'test_recall_average_weighted': 0.6086785009861932, 'test_f1score_average_weighted': 0.6040716035451341, 'test_acc_average': 0.6086785009861932}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.71      0.65      1990\n",
      "           1       0.65      0.50      0.56       790\n",
      "           2       0.67      0.53      0.59       608\n",
      "           3       0.46      0.45      0.46       684\n",
      "           4       0.55      0.46      0.50       527\n",
      "           5       0.78      0.86      0.82       471\n",
      "\n",
      "    accuracy                           0.61      5070\n",
      "   macro avg       0.62      0.59      0.60      5070\n",
      "weighted avg       0.61      0.61      0.60      5070\n",
      "\n",
      "\n",
      "[[1409   86  114  169  133   79]\n",
      " [ 230  392   11  139   11    7]\n",
      " [ 215    7  325   13   40    8]\n",
      " [ 222  111   13  310   14   14]\n",
      " [ 205    5   22   43  243    9]\n",
      " [  64    0    0    0    0  407]]\n",
      "Training complete in 23m 4s\n",
      "Best val acc: 0.608679\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/denizjafari/hnultra_test\" target=\"_blank\">https://app.wandb.ai/denizjafari/hnultra_test</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/denizjafari/hnultra_test/runs/2426maqo\" target=\"_blank\">https://app.wandb.ai/denizjafari/hnultra_test/runs/2426maqo</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t conv1.weight\n",
      "\t conv1.bias\n",
      "\t conv2.weight\n",
      "\t conv2.bias\n",
      "\t conv3.weight\n",
      "\t conv3.bias\n",
      "\t linear1.weight\n",
      "\t linear1.bias\n",
      "\t linear2.weight\n",
      "\t linear2.bias\n",
      "Epoch 1/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.6038 | train acc:\t0.3679\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.94      0.53      4943\n",
      "           1       0.30      0.03      0.06      2225\n",
      "           2       0.11      0.01      0.01      1539\n",
      "           3       0.33      0.08      0.13      2284\n",
      "           4       0.00      0.00      0.00      1669\n",
      "           5       0.59      0.16      0.25      1298\n",
      "\n",
      "    accuracy                           0.37     13958\n",
      "   macro avg       0.28      0.20      0.16     13958\n",
      "weighted avg       0.30      0.37      0.24     13958\n",
      "\n",
      "test loss:\t1.5587 | test acc:\t0.4465\n",
      "\n",
      "Epoch 2/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3984 | train acc:\t0.4531\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.80      0.58      4943\n",
      "           1       0.41      0.32      0.36      2225\n",
      "           2       0.54      0.01      0.02      1539\n",
      "           3       0.39      0.33      0.36      2284\n",
      "           4       0.47      0.03      0.05      1669\n",
      "           5       0.61      0.63      0.62      1298\n",
      "\n",
      "    accuracy                           0.45     13958\n",
      "   macro avg       0.48      0.35      0.33     13958\n",
      "weighted avg       0.46      0.45      0.39     13958\n",
      "\n",
      "test loss:\t1.4648 | test acc:\t0.4732\n",
      "\n",
      "Epoch 3/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3369 | train acc:\t0.4750\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.77      0.59      4943\n",
      "           1       0.43      0.37      0.40      2225\n",
      "           2       0.42      0.05      0.08      1539\n",
      "           3       0.41      0.40      0.41      2284\n",
      "           4       0.40      0.11      0.17      1669\n",
      "           5       0.64      0.67      0.65      1298\n",
      "\n",
      "    accuracy                           0.47     13958\n",
      "   macro avg       0.47      0.39      0.38     13958\n",
      "weighted avg       0.46      0.47      0.43     13958\n",
      "\n",
      "test loss:\t1.4057 | test acc:\t0.5089\n",
      "\n",
      "Epoch 4/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2893 | train acc:\t0.4928\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.76      0.60      4943\n",
      "           1       0.46      0.40      0.43      2225\n",
      "           2       0.49      0.10      0.16      1539\n",
      "           3       0.42      0.40      0.41      2284\n",
      "           4       0.40      0.16      0.23      1669\n",
      "           5       0.69      0.69      0.69      1298\n",
      "\n",
      "    accuracy                           0.49     13958\n",
      "   macro avg       0.49      0.42      0.42     13958\n",
      "weighted avg       0.48      0.49      0.46     13958\n",
      "\n",
      "test loss:\t1.4070 | test acc:\t0.5063\n",
      "\n",
      "Epoch 5/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2627 | train acc:\t0.5087\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.76      0.61      4943\n",
      "           1       0.51      0.40      0.45      2225\n",
      "           2       0.50      0.16      0.25      1539\n",
      "           3       0.43      0.43      0.43      2284\n",
      "           4       0.42      0.20      0.27      1669\n",
      "           5       0.70      0.71      0.70      1298\n",
      "\n",
      "    accuracy                           0.51     13958\n",
      "   macro avg       0.51      0.44      0.45     13958\n",
      "weighted avg       0.50      0.51      0.48     13958\n",
      "\n",
      "test loss:\t1.3841 | test acc:\t0.5006\n",
      "\n",
      "Epoch 6/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2474 | train acc:\t0.5152\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.74      0.61      4943\n",
      "           1       0.53      0.42      0.47      2225\n",
      "           2       0.47      0.19      0.27      1539\n",
      "           3       0.44      0.45      0.45      2284\n",
      "           4       0.41      0.22      0.28      1669\n",
      "           5       0.72      0.71      0.71      1298\n",
      "\n",
      "    accuracy                           0.52     13958\n",
      "   macro avg       0.52      0.45      0.47     13958\n",
      "weighted avg       0.51      0.52      0.49     13958\n",
      "\n",
      "test loss:\t1.4035 | test acc:\t0.5209\n",
      "\n",
      "Epoch 7/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2233 | train acc:\t0.5251\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.74      0.61      4943\n",
      "           1       0.54      0.42      0.47      2225\n",
      "           2       0.49      0.22      0.31      1539\n",
      "           3       0.46      0.46      0.46      2284\n",
      "           4       0.45      0.25      0.33      1669\n",
      "           5       0.72      0.70      0.71      1298\n",
      "\n",
      "    accuracy                           0.53     13958\n",
      "   macro avg       0.53      0.47      0.48     13958\n",
      "weighted avg       0.52      0.53      0.51     13958\n",
      "\n",
      "test loss:\t1.3372 | test acc:\t0.5400\n",
      "\n",
      "Epoch 8/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2023 | train acc:\t0.5343\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.74      0.62      4943\n",
      "           1       0.57      0.42      0.48      2225\n",
      "           2       0.50      0.24      0.32      1539\n",
      "           3       0.47      0.47      0.47      2284\n",
      "           4       0.45      0.29      0.35      1669\n",
      "           5       0.73      0.72      0.73      1298\n",
      "\n",
      "    accuracy                           0.53     13958\n",
      "   macro avg       0.54      0.48      0.49     13958\n",
      "weighted avg       0.53      0.53      0.52     13958\n",
      "\n",
      "test loss:\t1.3298 | test acc:\t0.5426\n",
      "\n",
      "Epoch 9/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1843 | train acc:\t0.5393\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      4943\n",
      "           1       0.58      0.42      0.49      2225\n",
      "           2       0.51      0.26      0.35      1539\n",
      "           3       0.47      0.48      0.47      2284\n",
      "           4       0.44      0.30      0.36      1669\n",
      "           5       0.74      0.74      0.74      1298\n",
      "\n",
      "    accuracy                           0.54     13958\n",
      "   macro avg       0.55      0.49      0.50     13958\n",
      "weighted avg       0.54      0.54      0.52     13958\n",
      "\n",
      "test loss:\t1.3109 | test acc:\t0.5529\n",
      "\n",
      "Epoch 10/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1602 | train acc:\t0.5484\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.63      4943\n",
      "           1       0.58      0.43      0.50      2225\n",
      "           2       0.51      0.28      0.36      1539\n",
      "           3       0.49      0.49      0.49      2284\n",
      "           4       0.46      0.32      0.37      1669\n",
      "           5       0.75      0.74      0.75      1298\n",
      "\n",
      "    accuracy                           0.55     13958\n",
      "   macro avg       0.56      0.50      0.52     13958\n",
      "weighted avg       0.55      0.55      0.53     13958\n",
      "\n",
      "test loss:\t1.2907 | test acc:\t0.5456\n",
      "\n",
      "Epoch 11/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1468 | train acc:\t0.5568\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.75      0.64      4943\n",
      "           1       0.60      0.44      0.51      2225\n",
      "           2       0.53      0.30      0.38      1539\n",
      "           3       0.48      0.49      0.48      2284\n",
      "           4       0.47      0.33      0.39      1669\n",
      "           5       0.77      0.74      0.75      1298\n",
      "\n",
      "    accuracy                           0.56     13958\n",
      "   macro avg       0.57      0.51      0.53     13958\n",
      "weighted avg       0.56      0.56      0.54     13958\n",
      "\n",
      "test loss:\t1.2768 | test acc:\t0.5611\n",
      "\n",
      "Epoch 12/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1222 | train acc:\t0.5666\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.63      4943\n",
      "           1       0.63      0.46      0.53      2225\n",
      "           2       0.53      0.31      0.39      1539\n",
      "           3       0.50      0.51      0.50      2284\n",
      "           4       0.49      0.37      0.43      1669\n",
      "           5       0.76      0.76      0.76      1298\n",
      "\n",
      "    accuracy                           0.57     13958\n",
      "   macro avg       0.58      0.53      0.54     13958\n",
      "weighted avg       0.57      0.57      0.56     13958\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:\t1.2375 | test acc:\t0.5667\n",
      "\n",
      "Epoch 13/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1206 | train acc:\t0.5683\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      4943\n",
      "           1       0.61      0.46      0.52      2225\n",
      "           2       0.55      0.31      0.39      1539\n",
      "           3       0.51      0.50      0.50      2284\n",
      "           4       0.48      0.38      0.42      1669\n",
      "           5       0.77      0.76      0.76      1298\n",
      "\n",
      "    accuracy                           0.57     13958\n",
      "   macro avg       0.58      0.52      0.54     13958\n",
      "weighted avg       0.57      0.57      0.56     13958\n",
      "\n",
      "test loss:\t1.2438 | test acc:\t0.5570\n",
      "\n",
      "Epoch 14/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1005 | train acc:\t0.5699\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.63      4943\n",
      "           1       0.63      0.46      0.53      2225\n",
      "           2       0.55      0.32      0.41      1539\n",
      "           3       0.50      0.52      0.51      2284\n",
      "           4       0.49      0.38      0.43      1669\n",
      "           5       0.77      0.76      0.76      1298\n",
      "\n",
      "    accuracy                           0.57     13958\n",
      "   macro avg       0.58      0.53      0.55     13958\n",
      "weighted avg       0.57      0.57      0.56     13958\n",
      "\n",
      "test loss:\t1.2531 | test acc:\t0.5649\n",
      "\n",
      "Epoch 15/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0957 | train acc:\t0.5768\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      4943\n",
      "           1       0.62      0.46      0.53      2225\n",
      "           2       0.54      0.34      0.42      1539\n",
      "           3       0.51      0.53      0.52      2284\n",
      "           4       0.52      0.39      0.45      1669\n",
      "           5       0.76      0.78      0.77      1298\n",
      "\n",
      "    accuracy                           0.58     13958\n",
      "   macro avg       0.59      0.54      0.55     13958\n",
      "weighted avg       0.58      0.58      0.57     13958\n",
      "\n",
      "test loss:\t1.2392 | test acc:\t0.5690\n",
      "\n",
      "Epoch 16/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0882 | train acc:\t0.5815\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65      4943\n",
      "           1       0.63      0.46      0.53      2225\n",
      "           2       0.57      0.35      0.43      1539\n",
      "           3       0.51      0.53      0.52      2284\n",
      "           4       0.52      0.42      0.46      1669\n",
      "           5       0.76      0.77      0.76      1298\n",
      "\n",
      "    accuracy                           0.58     13958\n",
      "   macro avg       0.60      0.54      0.56     13958\n",
      "weighted avg       0.58      0.58      0.57     13958\n",
      "\n",
      "test loss:\t1.2138 | test acc:\t0.5817\n",
      "\n",
      "Epoch 17/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0718 | train acc:\t0.5909\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      4943\n",
      "           1       0.66      0.48      0.55      2225\n",
      "           2       0.57      0.35      0.44      1539\n",
      "           3       0.52      0.54      0.53      2284\n",
      "           4       0.52      0.41      0.46      1669\n",
      "           5       0.79      0.78      0.78      1298\n",
      "\n",
      "    accuracy                           0.59     13958\n",
      "   macro avg       0.61      0.55      0.57     13958\n",
      "weighted avg       0.59      0.59      0.58     13958\n",
      "\n",
      "test loss:\t1.2078 | test acc:\t0.5809\n",
      "\n",
      "Epoch 18/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0584 | train acc:\t0.5945\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      4943\n",
      "           1       0.65      0.47      0.55      2225\n",
      "           2       0.58      0.36      0.45      1539\n",
      "           3       0.53      0.54      0.54      2284\n",
      "           4       0.52      0.44      0.48      1669\n",
      "           5       0.78      0.79      0.78      1298\n",
      "\n",
      "    accuracy                           0.59     13958\n",
      "   macro avg       0.61      0.56      0.57     13958\n",
      "weighted avg       0.60      0.59      0.59     13958\n",
      "\n",
      "test loss:\t1.1925 | test acc:\t0.5817\n",
      "\n",
      "Epoch 19/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0481 | train acc:\t0.5958\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      4943\n",
      "           1       0.65      0.49      0.56      2225\n",
      "           2       0.58      0.39      0.46      1539\n",
      "           3       0.53      0.53      0.53      2284\n",
      "           4       0.53      0.44      0.48      1669\n",
      "           5       0.79      0.79      0.79      1298\n",
      "\n",
      "    accuracy                           0.60     13958\n",
      "   macro avg       0.61      0.56      0.58     13958\n",
      "weighted avg       0.60      0.60      0.59     13958\n",
      "\n",
      "test loss:\t1.1832 | test acc:\t0.5832\n",
      "\n",
      "Epoch 20/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0405 | train acc:\t0.5989\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      4943\n",
      "           1       0.67      0.48      0.56      2225\n",
      "           2       0.58      0.40      0.47      1539\n",
      "           3       0.53      0.55      0.54      2284\n",
      "           4       0.54      0.44      0.48      1669\n",
      "           5       0.78      0.79      0.78      1298\n",
      "\n",
      "    accuracy                           0.60     13958\n",
      "   macro avg       0.61      0.57      0.58     13958\n",
      "weighted avg       0.60      0.60      0.59     13958\n",
      "\n",
      "test loss:\t1.1932 | test acc:\t0.5838\n",
      "\n",
      "Epoch 21/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0380 | train acc:\t0.6014\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      4943\n",
      "           1       0.67      0.48      0.56      2225\n",
      "           2       0.59      0.40      0.48      1539\n",
      "           3       0.54      0.56      0.55      2284\n",
      "           4       0.54      0.44      0.49      1669\n",
      "           5       0.78      0.78      0.78      1298\n",
      "\n",
      "    accuracy                           0.60     13958\n",
      "   macro avg       0.62      0.57      0.59     13958\n",
      "weighted avg       0.60      0.60      0.59     13958\n",
      "\n",
      "test loss:\t1.1869 | test acc:\t0.5951\n",
      "\n",
      "Epoch 22/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0191 | train acc:\t0.6065\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      4943\n",
      "           1       0.66      0.51      0.57      2225\n",
      "           2       0.61      0.40      0.48      1539\n",
      "           3       0.54      0.55      0.54      2284\n",
      "           4       0.54      0.45      0.49      1669\n",
      "           5       0.79      0.80      0.79      1298\n",
      "\n",
      "    accuracy                           0.61     13958\n",
      "   macro avg       0.62      0.57      0.59     13958\n",
      "weighted avg       0.61      0.61      0.60     13958\n",
      "\n",
      "test loss:\t1.1887 | test acc:\t0.5882\n",
      "\n",
      "Epoch 23/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0092 | train acc:\t0.6152\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      4943\n",
      "           1       0.66      0.50      0.57      2225\n",
      "           2       0.61      0.43      0.51      1539\n",
      "           3       0.55      0.56      0.55      2284\n",
      "           4       0.55      0.45      0.50      1669\n",
      "           5       0.80      0.81      0.81      1298\n",
      "\n",
      "    accuracy                           0.62     13958\n",
      "   macro avg       0.63      0.58      0.60     13958\n",
      "weighted avg       0.62      0.62      0.61     13958\n",
      "\n",
      "test loss:\t1.1818 | test acc:\t0.5828\n",
      "\n",
      "Epoch 24/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9990 | train acc:\t0.6174\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      4943\n",
      "           1       0.67      0.52      0.59      2225\n",
      "           2       0.60      0.43      0.50      1539\n",
      "           3       0.56      0.58      0.57      2284\n",
      "           4       0.56      0.45      0.50      1669\n",
      "           5       0.79      0.81      0.80      1298\n",
      "\n",
      "    accuracy                           0.62     13958\n",
      "   macro avg       0.63      0.59      0.60     13958\n",
      "weighted avg       0.62      0.62      0.61     13958\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:\t1.1922 | test acc:\t0.5919\n",
      "\n",
      "Epoch 25/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0047 | train acc:\t0.6143\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      4943\n",
      "           1       0.65      0.50      0.56      2225\n",
      "           2       0.61      0.44      0.51      1539\n",
      "           3       0.56      0.55      0.55      2284\n",
      "           4       0.55      0.46      0.50      1669\n",
      "           5       0.79      0.81      0.80      1298\n",
      "\n",
      "    accuracy                           0.61     13958\n",
      "   macro avg       0.63      0.58      0.60     13958\n",
      "weighted avg       0.61      0.61      0.61     13958\n",
      "\n",
      "test loss:\t1.1517 | test acc:\t0.5892\n",
      "\n",
      "Epoch 26/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9902 | train acc:\t0.6211\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      4943\n",
      "           1       0.67      0.52      0.59      2225\n",
      "           2       0.63      0.44      0.52      1539\n",
      "           3       0.56      0.57      0.57      2284\n",
      "           4       0.56      0.47      0.51      1669\n",
      "           5       0.80      0.81      0.80      1298\n",
      "\n",
      "    accuracy                           0.62     13958\n",
      "   macro avg       0.64      0.59      0.61     13958\n",
      "weighted avg       0.62      0.62      0.62     13958\n",
      "\n",
      "test loss:\t1.1855 | test acc:\t0.6012\n",
      "\n",
      "Epoch 27/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9787 | train acc:\t0.6247\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      4943\n",
      "           1       0.68      0.52      0.59      2225\n",
      "           2       0.63      0.45      0.53      1539\n",
      "           3       0.56      0.57      0.57      2284\n",
      "           4       0.58      0.47      0.52      1669\n",
      "           5       0.80      0.81      0.81      1298\n",
      "\n",
      "    accuracy                           0.62     13958\n",
      "   macro avg       0.64      0.60      0.61     13958\n",
      "weighted avg       0.63      0.62      0.62     13958\n",
      "\n",
      "test loss:\t1.1875 | test acc:\t0.5907\n",
      "\n",
      "Epoch 28/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9680 | train acc:\t0.6313\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      4943\n",
      "           1       0.69      0.51      0.59      2225\n",
      "           2       0.63      0.47      0.54      1539\n",
      "           3       0.58      0.58      0.58      2284\n",
      "           4       0.58      0.49      0.53      1669\n",
      "           5       0.81      0.81      0.81      1298\n",
      "\n",
      "    accuracy                           0.63     13958\n",
      "   macro avg       0.65      0.60      0.62     13958\n",
      "weighted avg       0.63      0.63      0.63     13958\n",
      "\n",
      "test loss:\t1.2117 | test acc:\t0.5753\n",
      "\n",
      "Epoch 29/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9682 | train acc:\t0.6328\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      4943\n",
      "           1       0.69      0.52      0.60      2225\n",
      "           2       0.61      0.46      0.53      1539\n",
      "           3       0.57      0.58      0.58      2284\n",
      "           4       0.60      0.49      0.54      1669\n",
      "           5       0.80      0.80      0.80      1298\n",
      "\n",
      "    accuracy                           0.63     13958\n",
      "   macro avg       0.65      0.60      0.62     13958\n",
      "weighted avg       0.64      0.63      0.63     13958\n",
      "\n",
      "test loss:\t1.1485 | test acc:\t0.5964\n",
      "\n",
      "Epoch 30/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9603 | train acc:\t0.6355\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      4943\n",
      "           1       0.69      0.53      0.60      2225\n",
      "           2       0.63      0.47      0.54      1539\n",
      "           3       0.59      0.58      0.58      2284\n",
      "           4       0.57      0.49      0.53      1669\n",
      "           5       0.81      0.81      0.81      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.65      0.61      0.62     13958\n",
      "weighted avg       0.64      0.64      0.63     13958\n",
      "\n",
      "test loss:\t1.1557 | test acc:\t0.6047\n",
      "\n",
      "Epoch 31/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9436 | train acc:\t0.6310\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      4943\n",
      "           1       0.68      0.53      0.59      2225\n",
      "           2       0.64      0.49      0.55      1539\n",
      "           3       0.57      0.57      0.57      2284\n",
      "           4       0.57      0.49      0.53      1669\n",
      "           5       0.81      0.81      0.81      1298\n",
      "\n",
      "    accuracy                           0.63     13958\n",
      "   macro avg       0.65      0.61      0.62     13958\n",
      "weighted avg       0.63      0.63      0.63     13958\n",
      "\n",
      "test loss:\t1.1480 | test acc:\t0.5925\n",
      "\n",
      "Epoch 32/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9414 | train acc:\t0.6400\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.68      4943\n",
      "           1       0.69      0.53      0.60      2225\n",
      "           2       0.64      0.49      0.55      1539\n",
      "           3       0.57      0.59      0.58      2284\n",
      "           4       0.59      0.50      0.54      1669\n",
      "           5       0.81      0.83      0.82      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.66      0.62      0.63     13958\n",
      "weighted avg       0.64      0.64      0.64     13958\n",
      "\n",
      "test loss:\t1.1095 | test acc:\t0.6089\n",
      "\n",
      "Epoch 33/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9301 | train acc:\t0.6434\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      4943\n",
      "           1       0.70      0.54      0.61      2225\n",
      "           2       0.65      0.50      0.56      1539\n",
      "           3       0.58      0.59      0.58      2284\n",
      "           4       0.59      0.51      0.55      1669\n",
      "           5       0.82      0.82      0.82      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.66      0.62      0.64     13958\n",
      "weighted avg       0.65      0.64      0.64     13958\n",
      "\n",
      "test loss:\t1.1700 | test acc:\t0.6002\n",
      "\n",
      "Epoch 34/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9303 | train acc:\t0.6424\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.69      4943\n",
      "           1       0.69      0.54      0.61      2225\n",
      "           2       0.65      0.49      0.56      1539\n",
      "           3       0.57      0.58      0.58      2284\n",
      "           4       0.59      0.52      0.55      1669\n",
      "           5       0.82      0.81      0.82      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.66      0.62      0.63     13958\n",
      "weighted avg       0.65      0.64      0.64     13958\n",
      "\n",
      "test loss:\t1.1325 | test acc:\t0.6002\n",
      "\n",
      "Epoch 35/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9220 | train acc:\t0.6450\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.77      0.69      4943\n",
      "           1       0.70      0.53      0.60      2225\n",
      "           2       0.65      0.49      0.56      1539\n",
      "           3       0.59      0.60      0.59      2284\n",
      "           4       0.60      0.50      0.55      1669\n",
      "           5       0.82      0.83      0.82      1298\n",
      "\n",
      "    accuracy                           0.65     13958\n",
      "   macro avg       0.66      0.62      0.64     13958\n",
      "weighted avg       0.65      0.65      0.64     13958\n",
      "\n",
      "test loss:\t1.1218 | test acc:\t0.5953\n",
      "\n",
      "Epoch 36/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9064 | train acc:\t0.6523\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      4943\n",
      "           1       0.68      0.55      0.61      2225\n",
      "           2       0.66      0.51      0.58      1539\n",
      "           3       0.60      0.59      0.60      2284\n",
      "           4       0.60      0.53      0.56      1669\n",
      "           5       0.82      0.82      0.82      1298\n",
      "\n",
      "    accuracy                           0.65     13958\n",
      "   macro avg       0.67      0.63      0.64     13958\n",
      "weighted avg       0.65      0.65      0.65     13958\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:\t1.1284 | test acc:\t0.5955\n",
      "\n",
      "Epoch 37/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9113 | train acc:\t0.6546\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      4943\n",
      "           1       0.70      0.55      0.62      2225\n",
      "           2       0.66      0.52      0.58      1539\n",
      "           3       0.60      0.60      0.60      2284\n",
      "           4       0.61      0.54      0.57      1669\n",
      "           5       0.82      0.82      0.82      1298\n",
      "\n",
      "    accuracy                           0.65     13958\n",
      "   macro avg       0.67      0.63      0.65     13958\n",
      "weighted avg       0.66      0.65      0.65     13958\n",
      "\n",
      "test loss:\t1.1311 | test acc:\t0.6067\n",
      "\n",
      "Epoch 38/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9002 | train acc:\t0.6548\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      4943\n",
      "           1       0.71      0.56      0.63      2225\n",
      "           2       0.65      0.52      0.58      1539\n",
      "           3       0.59      0.60      0.60      2284\n",
      "           4       0.61      0.53      0.57      1669\n",
      "           5       0.82      0.82      0.82      1298\n",
      "\n",
      "    accuracy                           0.65     13958\n",
      "   macro avg       0.67      0.63      0.65     13958\n",
      "weighted avg       0.66      0.65      0.65     13958\n",
      "\n",
      "test loss:\t1.1275 | test acc:\t0.6087\n",
      "\n",
      "Epoch 39/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9004 | train acc:\t0.6573\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      4943\n",
      "           1       0.71      0.56      0.63      2225\n",
      "           2       0.66      0.52      0.58      1539\n",
      "           3       0.60      0.61      0.61      2284\n",
      "           4       0.62      0.53      0.57      1669\n",
      "           5       0.81      0.83      0.82      1298\n",
      "\n",
      "    accuracy                           0.66     13958\n",
      "   macro avg       0.67      0.64      0.65     13958\n",
      "weighted avg       0.66      0.66      0.65     13958\n",
      "\n",
      "test loss:\t1.1442 | test acc:\t0.5953\n",
      "\n",
      "Epoch 40/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8938 | train acc:\t0.6638\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      4943\n",
      "           1       0.72      0.57      0.64      2225\n",
      "           2       0.68      0.55      0.60      1539\n",
      "           3       0.61      0.61      0.61      2284\n",
      "           4       0.61      0.54      0.57      1669\n",
      "           5       0.82      0.83      0.83      1298\n",
      "\n",
      "    accuracy                           0.66     13958\n",
      "   macro avg       0.68      0.64      0.66     13958\n",
      "weighted avg       0.67      0.66      0.66     13958\n",
      "\n",
      "test loss:\t1.1298 | test acc:\t0.5943\n",
      "\n",
      "Epoch 41/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8836 | train acc:\t0.6613\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      4943\n",
      "           1       0.70      0.57      0.63      2225\n",
      "           2       0.66      0.52      0.58      1539\n",
      "           3       0.61      0.61      0.61      2284\n",
      "           4       0.61      0.54      0.57      1669\n",
      "           5       0.83      0.84      0.83      1298\n",
      "\n",
      "    accuracy                           0.66     13958\n",
      "   macro avg       0.68      0.64      0.65     13958\n",
      "weighted avg       0.66      0.66      0.66     13958\n",
      "\n",
      "test loss:\t1.1198 | test acc:\t0.6037\n",
      "\n",
      "Epoch 42/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8810 | train acc:\t0.6630\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      4943\n",
      "           1       0.72      0.57      0.63      2225\n",
      "           2       0.67      0.53      0.59      1539\n",
      "           3       0.60      0.60      0.60      2284\n",
      "           4       0.62      0.54      0.58      1669\n",
      "           5       0.83      0.84      0.83      1298\n",
      "\n",
      "    accuracy                           0.66     13958\n",
      "   macro avg       0.68      0.64      0.66     13958\n",
      "weighted avg       0.67      0.66      0.66     13958\n",
      "\n",
      "test loss:\t1.1282 | test acc:\t0.6006\n",
      "\n",
      "Epoch 43/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8731 | train acc:\t0.6707\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.71      4943\n",
      "           1       0.70      0.56      0.63      2225\n",
      "           2       0.67      0.54      0.60      1539\n",
      "           3       0.63      0.63      0.63      2284\n",
      "           4       0.64      0.57      0.60      1669\n",
      "           5       0.83      0.84      0.83      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.69      0.65      0.67     13958\n",
      "weighted avg       0.67      0.67      0.67     13958\n",
      "\n",
      "test loss:\t1.1443 | test acc:\t0.5913\n",
      "\n",
      "Epoch 44/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8641 | train acc:\t0.6689\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      4943\n",
      "           1       0.72      0.57      0.64      2225\n",
      "           2       0.68      0.53      0.60      1539\n",
      "           3       0.62      0.62      0.62      2284\n",
      "           4       0.63      0.56      0.59      1669\n",
      "           5       0.83      0.84      0.83      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.69      0.65      0.66     13958\n",
      "weighted avg       0.67      0.67      0.67     13958\n",
      "\n",
      "test loss:\t1.1008 | test acc:\t0.6067\n",
      "\n",
      "Epoch 45/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8611 | train acc:\t0.6734\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.71      4943\n",
      "           1       0.71      0.58      0.64      2225\n",
      "           2       0.67      0.56      0.61      1539\n",
      "           3       0.62      0.62      0.62      2284\n",
      "           4       0.65      0.56      0.60      1669\n",
      "           5       0.83      0.84      0.83      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.69      0.65      0.67     13958\n",
      "weighted avg       0.68      0.67      0.67     13958\n",
      "\n",
      "test loss:\t1.1308 | test acc:\t0.5775\n",
      "\n",
      "Epoch 46/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8559 | train acc:\t0.6732\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71      4943\n",
      "           1       0.72      0.59      0.65      2225\n",
      "           2       0.68      0.54      0.60      1539\n",
      "           3       0.62      0.62      0.62      2284\n",
      "           4       0.61      0.56      0.59      1669\n",
      "           5       0.83      0.83      0.83      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.69      0.65      0.67     13958\n",
      "weighted avg       0.68      0.67      0.67     13958\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:\t1.1134 | test acc:\t0.6030\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Storing tensors in summary requires h5py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_Other_f1': 0.6492617162422427, 'val_Other_precision': 0.5654118524040254, 'val_Other_recall': 0.7623115577889448, 'val_Saggital_Right_f1': 0.5801853171774769, 'val_Saggital_Right_precision': 0.6639477977161501, 'val_Saggital_Right_recall': 0.5151898734177215, 'val_Transverse_Right_f1': 0.5579937304075235, 'val_Transverse_Right_precision': 0.7650429799426934, 'val_Transverse_Right_recall': 0.43914473684210525, 'val_Saggital_Left_f1': 0.4609250398724083, 'val_Saggital_Left_precision': 0.5070175438596491, 'val_Saggital_Left_recall': 0.42251461988304095, 'val_Transverse_Left_f1': 0.44004400440044006, 'val_Transverse_Left_precision': 0.5235602094240838, 'val_Transverse_Left_recall': 0.3795066413662239, 'val_Bladder_f1': 0.7987288135593221, 'val_Bladder_precision': 0.7970401691331924, 'val_Bladder_recall': 0.8004246284501062, 'test_precision_average_macro': 0.637003425413299, 'test_recall_average_macro': 0.5531820096246904, 'test_f1score_average_macro': 0.5811897702765689, 'test_precision_average_micro': 0.6029585798816568, 'test_recall_average_micro': 0.6029585798816568, 'test_f1score_average_micro': 0.6029585798816568, 'test_precision_average_weighted': 0.6139953901998327, 'test_recall_average_weighted': 0.6029585798816568, 'test_f1score_average_weighted': 0.5942829571515273, 'test_acc_average': 0.6029585798816568}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.76      0.65      1990\n",
      "           1       0.66      0.52      0.58       790\n",
      "           2       0.77      0.44      0.56       608\n",
      "           3       0.51      0.42      0.46       684\n",
      "           4       0.52      0.38      0.44       527\n",
      "           5       0.80      0.80      0.80       471\n",
      "\n",
      "    accuracy                           0.60      5070\n",
      "   macro avg       0.64      0.55      0.58      5070\n",
      "weighted avg       0.61      0.60      0.59      5070\n",
      "\n",
      "\n",
      "[[1517   96   61  131  118   67]\n",
      " [ 250  407    8  108   11    6]\n",
      " [ 285    6  267    8   35    7]\n",
      " [ 267   98    3  289   17   10]\n",
      " [ 271    6   10   34  200    6]\n",
      " [  93    0    0    0    1  377]]\n",
      "Training complete in 24m 9s\n",
      "Best val acc: 0.602959\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/denizjafari/hnultra_test\" target=\"_blank\">https://app.wandb.ai/denizjafari/hnultra_test</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/denizjafari/hnultra_test/runs/19fm5d2o\" target=\"_blank\">https://app.wandb.ai/denizjafari/hnultra_test/runs/19fm5d2o</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t conv1.weight\n",
      "\t conv1.bias\n",
      "\t conv2.weight\n",
      "\t conv2.bias\n",
      "\t conv3.weight\n",
      "\t conv3.bias\n",
      "\t linear1.weight\n",
      "\t linear1.bias\n",
      "\t linear2.weight\n",
      "\t linear2.bias\n",
      "Epoch 1/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.6051 | train acc:\t0.3670\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.95      0.53      4943\n",
      "           1       0.28      0.03      0.05      2225\n",
      "           2       0.14      0.01      0.02      1539\n",
      "           3       0.33      0.06      0.10      2284\n",
      "           4       0.00      0.00      0.00      1669\n",
      "           5       0.60      0.16      0.26      1298\n",
      "\n",
      "    accuracy                           0.37     13958\n",
      "   macro avg       0.29      0.20      0.16     13958\n",
      "weighted avg       0.30      0.37      0.24     13958\n",
      "\n",
      "test loss:\t1.5741 | test acc:\t0.4032\n",
      "\n",
      "Epoch 2/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.4309 | train acc:\t0.4337\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.79      0.56      4943\n",
      "           1       0.37      0.25      0.30      2225\n",
      "           2       0.31      0.00      0.01      1539\n",
      "           3       0.37      0.30      0.34      2284\n",
      "           4       0.37      0.02      0.03      1669\n",
      "           5       0.59      0.66      0.62      1298\n",
      "\n",
      "    accuracy                           0.43     13958\n",
      "   macro avg       0.41      0.34      0.31     13958\n",
      "weighted avg       0.41      0.43      0.36     13958\n",
      "\n",
      "test loss:\t1.4859 | test acc:\t0.4627\n",
      "\n",
      "Epoch 3/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3515 | train acc:\t0.4712\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.77      0.59      4943\n",
      "           1       0.43      0.35      0.38      2225\n",
      "           2       0.51      0.04      0.08      1539\n",
      "           3       0.40      0.39      0.40      2284\n",
      "           4       0.38      0.07      0.12      1669\n",
      "           5       0.64      0.69      0.67      1298\n",
      "\n",
      "    accuracy                           0.47     13958\n",
      "   macro avg       0.47      0.39      0.37     13958\n",
      "weighted avg       0.46      0.47      0.42     13958\n",
      "\n",
      "test loss:\t1.4403 | test acc:\t0.4953\n",
      "\n",
      "Epoch 4/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3080 | train acc:\t0.4928\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.76      0.60      4943\n",
      "           1       0.47      0.38      0.42      2225\n",
      "           2       0.54      0.10      0.16      1539\n",
      "           3       0.42      0.42      0.42      2284\n",
      "           4       0.43      0.15      0.22      1669\n",
      "           5       0.66      0.69      0.67      1298\n",
      "\n",
      "    accuracy                           0.49     13958\n",
      "   macro avg       0.50      0.42      0.42     13958\n",
      "weighted avg       0.49      0.49      0.46     13958\n",
      "\n",
      "test loss:\t1.4048 | test acc:\t0.5195\n",
      "\n",
      "Epoch 5/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2723 | train acc:\t0.5085\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.76      0.61      4943\n",
      "           1       0.50      0.40      0.45      2225\n",
      "           2       0.49      0.16      0.24      1539\n",
      "           3       0.43      0.43      0.43      2284\n",
      "           4       0.42      0.19      0.26      1669\n",
      "           5       0.70      0.70      0.70      1298\n",
      "\n",
      "    accuracy                           0.51     13958\n",
      "   macro avg       0.51      0.44      0.45     13958\n",
      "weighted avg       0.50      0.51      0.48     13958\n",
      "\n",
      "test loss:\t1.3920 | test acc:\t0.5221\n",
      "\n",
      "Epoch 6/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2492 | train acc:\t0.5138\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.75      0.61      4943\n",
      "           1       0.52      0.41      0.46      2225\n",
      "           2       0.48      0.18      0.26      1539\n",
      "           3       0.45      0.44      0.45      2284\n",
      "           4       0.43      0.21      0.28      1669\n",
      "           5       0.70      0.70      0.70      1298\n",
      "\n",
      "    accuracy                           0.51     13958\n",
      "   macro avg       0.51      0.45      0.46     13958\n",
      "weighted avg       0.51      0.51      0.49     13958\n",
      "\n",
      "test loss:\t1.3400 | test acc:\t0.5377\n",
      "\n",
      "Epoch 7/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2314 | train acc:\t0.5216\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.75      0.62      4943\n",
      "           1       0.53      0.40      0.45      2225\n",
      "           2       0.49      0.22      0.30      1539\n",
      "           3       0.45      0.45      0.45      2284\n",
      "           4       0.43      0.25      0.32      1669\n",
      "           5       0.72      0.70      0.71      1298\n",
      "\n",
      "    accuracy                           0.52     13958\n",
      "   macro avg       0.52      0.46      0.48     13958\n",
      "weighted avg       0.52      0.52      0.50     13958\n",
      "\n",
      "test loss:\t1.3342 | test acc:\t0.5280\n",
      "\n",
      "Epoch 8/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2100 | train acc:\t0.5340\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.74      0.62      4943\n",
      "           1       0.56      0.42      0.48      2225\n",
      "           2       0.50      0.23      0.31      1539\n",
      "           3       0.46      0.48      0.47      2284\n",
      "           4       0.46      0.28      0.35      1669\n",
      "           5       0.72      0.72      0.72      1298\n",
      "\n",
      "    accuracy                           0.53     13958\n",
      "   macro avg       0.54      0.48      0.49     13958\n",
      "weighted avg       0.53      0.53      0.52     13958\n",
      "\n",
      "test loss:\t1.3415 | test acc:\t0.5205\n",
      "\n",
      "Epoch 9/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1884 | train acc:\t0.5399\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.62      4943\n",
      "           1       0.58      0.42      0.48      2225\n",
      "           2       0.52      0.26      0.34      1539\n",
      "           3       0.45      0.48      0.46      2284\n",
      "           4       0.47      0.30      0.37      1669\n",
      "           5       0.74      0.73      0.74      1298\n",
      "\n",
      "    accuracy                           0.54     13958\n",
      "   macro avg       0.55      0.49      0.50     13958\n",
      "weighted avg       0.54      0.54      0.52     13958\n",
      "\n",
      "test loss:\t1.3174 | test acc:\t0.5294\n",
      "\n",
      "Epoch 10/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1840 | train acc:\t0.5418\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.62      4943\n",
      "           1       0.57      0.42      0.48      2225\n",
      "           2       0.50      0.26      0.34      1539\n",
      "           3       0.47      0.48      0.48      2284\n",
      "           4       0.45      0.32      0.38      1669\n",
      "           5       0.74      0.74      0.74      1298\n",
      "\n",
      "    accuracy                           0.54     13958\n",
      "   macro avg       0.55      0.49      0.51     13958\n",
      "weighted avg       0.54      0.54      0.53     13958\n",
      "\n",
      "test loss:\t1.2949 | test acc:\t0.5588\n",
      "\n",
      "Epoch 11/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1553 | train acc:\t0.5519\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.63      4943\n",
      "           1       0.60      0.43      0.50      2225\n",
      "           2       0.50      0.27      0.35      1539\n",
      "           3       0.48      0.49      0.49      2284\n",
      "           4       0.48      0.35      0.41      1669\n",
      "           5       0.75      0.76      0.76      1298\n",
      "\n",
      "    accuracy                           0.55     13958\n",
      "   macro avg       0.56      0.51      0.52     13958\n",
      "weighted avg       0.55      0.55      0.54     13958\n",
      "\n",
      "test loss:\t1.2700 | test acc:\t0.5540\n",
      "\n",
      "Epoch 12/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1358 | train acc:\t0.5571\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.63      4943\n",
      "           1       0.60      0.44      0.51      2225\n",
      "           2       0.50      0.29      0.37      1539\n",
      "           3       0.49      0.50      0.49      2284\n",
      "           4       0.48      0.36      0.41      1669\n",
      "           5       0.76      0.75      0.75      1298\n",
      "\n",
      "    accuracy                           0.56     13958\n",
      "   macro avg       0.56      0.51      0.53     13958\n",
      "weighted avg       0.55      0.56      0.55     13958\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:\t1.2750 | test acc:\t0.5412\n",
      "\n",
      "Epoch 13/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1264 | train acc:\t0.5670\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      4943\n",
      "           1       0.61      0.43      0.50      2225\n",
      "           2       0.53      0.32      0.40      1539\n",
      "           3       0.49      0.52      0.51      2284\n",
      "           4       0.50      0.36      0.42      1669\n",
      "           5       0.76      0.76      0.76      1298\n",
      "\n",
      "    accuracy                           0.57     13958\n",
      "   macro avg       0.58      0.52      0.54     13958\n",
      "weighted avg       0.57      0.57      0.56     13958\n",
      "\n",
      "test loss:\t1.2611 | test acc:\t0.5690\n",
      "\n",
      "Epoch 14/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1094 | train acc:\t0.5758\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.75      0.64      4943\n",
      "           1       0.62      0.46      0.52      2225\n",
      "           2       0.55      0.32      0.41      1539\n",
      "           3       0.51      0.52      0.52      2284\n",
      "           4       0.51      0.40      0.45      1669\n",
      "           5       0.76      0.77      0.76      1298\n",
      "\n",
      "    accuracy                           0.58     13958\n",
      "   macro avg       0.59      0.53      0.55     13958\n",
      "weighted avg       0.58      0.58      0.57     13958\n",
      "\n",
      "test loss:\t1.2378 | test acc:\t0.5639\n",
      "\n",
      "Epoch 15/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1065 | train acc:\t0.5716\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.64      4943\n",
      "           1       0.62      0.45      0.52      2225\n",
      "           2       0.56      0.33      0.42      1539\n",
      "           3       0.51      0.52      0.51      2284\n",
      "           4       0.49      0.38      0.43      1669\n",
      "           5       0.75      0.79      0.77      1298\n",
      "\n",
      "    accuracy                           0.57     13958\n",
      "   macro avg       0.58      0.53      0.55     13958\n",
      "weighted avg       0.57      0.57      0.56     13958\n",
      "\n",
      "test loss:\t1.2659 | test acc:\t0.5667\n",
      "\n",
      "Epoch 16/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0904 | train acc:\t0.5777\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.73      0.64      4943\n",
      "           1       0.62      0.45      0.52      2225\n",
      "           2       0.56      0.38      0.45      1539\n",
      "           3       0.50      0.51      0.51      2284\n",
      "           4       0.52      0.41      0.46      1669\n",
      "           5       0.76      0.78      0.77      1298\n",
      "\n",
      "    accuracy                           0.58     13958\n",
      "   macro avg       0.59      0.54      0.56     13958\n",
      "weighted avg       0.58      0.58      0.57     13958\n",
      "\n",
      "test loss:\t1.2210 | test acc:\t0.5663\n",
      "\n",
      "Epoch 17/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0793 | train acc:\t0.5876\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      4943\n",
      "           1       0.65      0.48      0.55      2225\n",
      "           2       0.57      0.36      0.44      1539\n",
      "           3       0.52      0.52      0.52      2284\n",
      "           4       0.52      0.42      0.46      1669\n",
      "           5       0.77      0.78      0.78      1298\n",
      "\n",
      "    accuracy                           0.59     13958\n",
      "   macro avg       0.60      0.55      0.57     13958\n",
      "weighted avg       0.59      0.59      0.58     13958\n",
      "\n",
      "test loss:\t1.2404 | test acc:\t0.5623\n",
      "\n",
      "Epoch 18/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0746 | train acc:\t0.5830\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      4943\n",
      "           1       0.63      0.46      0.53      2225\n",
      "           2       0.54      0.37      0.44      1539\n",
      "           3       0.51      0.52      0.51      2284\n",
      "           4       0.52      0.40      0.45      1669\n",
      "           5       0.77      0.79      0.78      1298\n",
      "\n",
      "    accuracy                           0.58     13958\n",
      "   macro avg       0.59      0.55      0.56     13958\n",
      "weighted avg       0.58      0.58      0.57     13958\n",
      "\n",
      "test loss:\t1.2448 | test acc:\t0.5716\n",
      "\n",
      "Epoch 19/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0551 | train acc:\t0.5934\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      4943\n",
      "           1       0.65      0.48      0.55      2225\n",
      "           2       0.58      0.37      0.45      1539\n",
      "           3       0.52      0.54      0.53      2284\n",
      "           4       0.52      0.43      0.47      1669\n",
      "           5       0.78      0.78      0.78      1298\n",
      "\n",
      "    accuracy                           0.59     13958\n",
      "   macro avg       0.61      0.56      0.57     13958\n",
      "weighted avg       0.60      0.59      0.59     13958\n",
      "\n",
      "test loss:\t1.1763 | test acc:\t0.5832\n",
      "\n",
      "Epoch 20/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0526 | train acc:\t0.6001\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      4943\n",
      "           1       0.64      0.48      0.55      2225\n",
      "           2       0.59      0.40      0.48      1539\n",
      "           3       0.53      0.54      0.54      2284\n",
      "           4       0.54      0.43      0.48      1669\n",
      "           5       0.78      0.80      0.79      1298\n",
      "\n",
      "    accuracy                           0.60     13958\n",
      "   macro avg       0.61      0.57      0.58     13958\n",
      "weighted avg       0.60      0.60      0.59     13958\n",
      "\n",
      "test loss:\t1.1838 | test acc:\t0.5712\n",
      "\n",
      "Epoch 21/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0429 | train acc:\t0.5997\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      4943\n",
      "           1       0.66      0.48      0.56      2225\n",
      "           2       0.58      0.40      0.47      1539\n",
      "           3       0.54      0.55      0.55      2284\n",
      "           4       0.54      0.43      0.48      1669\n",
      "           5       0.78      0.80      0.79      1298\n",
      "\n",
      "    accuracy                           0.60     13958\n",
      "   macro avg       0.61      0.57      0.58     13958\n",
      "weighted avg       0.60      0.60      0.59     13958\n",
      "\n",
      "test loss:\t1.1870 | test acc:\t0.5759\n",
      "\n",
      "Epoch 22/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0286 | train acc:\t0.6070\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      4943\n",
      "           1       0.68      0.49      0.57      2225\n",
      "           2       0.59      0.40      0.48      1539\n",
      "           3       0.54      0.57      0.56      2284\n",
      "           4       0.55      0.46      0.50      1669\n",
      "           5       0.78      0.80      0.79      1298\n",
      "\n",
      "    accuracy                           0.61     13958\n",
      "   macro avg       0.62      0.58      0.59     13958\n",
      "weighted avg       0.61      0.61      0.60     13958\n",
      "\n",
      "test loss:\t1.1637 | test acc:\t0.5917\n",
      "\n",
      "Epoch 23/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0179 | train acc:\t0.6111\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      4943\n",
      "           1       0.67      0.50      0.57      2225\n",
      "           2       0.61      0.41      0.49      1539\n",
      "           3       0.54      0.56      0.55      2284\n",
      "           4       0.55      0.45      0.49      1669\n",
      "           5       0.80      0.81      0.80      1298\n",
      "\n",
      "    accuracy                           0.61     13958\n",
      "   macro avg       0.63      0.58      0.60     13958\n",
      "weighted avg       0.61      0.61      0.60     13958\n",
      "\n",
      "test loss:\t1.1504 | test acc:\t0.5872\n",
      "\n",
      "Epoch 24/46\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0049 | train acc:\t0.6179\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      4943\n",
      "           1       0.68      0.50      0.57      2225\n",
      "           2       0.62      0.44      0.52      1539\n",
      "           3       0.55      0.56      0.55      2284\n",
      "           4       0.56      0.47      0.51      1669\n",
      "           5       0.78      0.81      0.79      1298\n",
      "\n",
      "    accuracy                           0.62     13958\n",
      "   macro avg       0.63      0.59      0.60     13958\n",
      "weighted avg       0.62      0.62      0.61     13958\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:\t1.1987 | test acc:\t0.5933\n",
      "\n",
      "Epoch 25/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9905 | train acc:\t0.6177\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      4943\n",
      "           1       0.67      0.50      0.57      2225\n",
      "           2       0.62      0.44      0.51      1539\n",
      "           3       0.56      0.57      0.56      2284\n",
      "           4       0.56      0.47      0.51      1669\n",
      "           5       0.80      0.81      0.81      1298\n",
      "\n",
      "    accuracy                           0.62     13958\n",
      "   macro avg       0.64      0.59      0.61     13958\n",
      "weighted avg       0.62      0.62      0.61     13958\n",
      "\n",
      "test loss:\t1.1668 | test acc:\t0.5978\n",
      "\n",
      "Epoch 26/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9776 | train acc:\t0.6232\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      4943\n",
      "           1       0.68      0.50      0.58      2225\n",
      "           2       0.63      0.44      0.52      1539\n",
      "           3       0.55      0.58      0.56      2284\n",
      "           4       0.59      0.48      0.53      1669\n",
      "           5       0.79      0.81      0.80      1298\n",
      "\n",
      "    accuracy                           0.62     13958\n",
      "   macro avg       0.64      0.59      0.61     13958\n",
      "weighted avg       0.63      0.62      0.62     13958\n",
      "\n",
      "test loss:\t1.1702 | test acc:\t0.5903\n",
      "\n",
      "Epoch 27/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9754 | train acc:\t0.6271\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      4943\n",
      "           1       0.67      0.52      0.58      2225\n",
      "           2       0.62      0.45      0.52      1539\n",
      "           3       0.57      0.58      0.57      2284\n",
      "           4       0.57      0.49      0.53      1669\n",
      "           5       0.81      0.82      0.81      1298\n",
      "\n",
      "    accuracy                           0.63     13958\n",
      "   macro avg       0.64      0.60      0.62     13958\n",
      "weighted avg       0.63      0.63      0.62     13958\n",
      "\n",
      "test loss:\t1.1469 | test acc:\t0.5945\n",
      "\n",
      "Epoch 28/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9688 | train acc:\t0.6315\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      4943\n",
      "           1       0.70      0.52      0.59      2225\n",
      "           2       0.63      0.46      0.53      1539\n",
      "           3       0.56      0.59      0.58      2284\n",
      "           4       0.59      0.49      0.53      1669\n",
      "           5       0.81      0.81      0.81      1298\n",
      "\n",
      "    accuracy                           0.63     13958\n",
      "   macro avg       0.65      0.60      0.62     13958\n",
      "weighted avg       0.64      0.63      0.63     13958\n",
      "\n",
      "test loss:\t1.1394 | test acc:\t0.6075\n",
      "\n",
      "Epoch 29/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9636 | train acc:\t0.6330\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      4943\n",
      "           1       0.69      0.51      0.59      2225\n",
      "           2       0.62      0.46      0.53      1539\n",
      "           3       0.56      0.60      0.58      2284\n",
      "           4       0.59      0.49      0.53      1669\n",
      "           5       0.82      0.82      0.82      1298\n",
      "\n",
      "    accuracy                           0.63     13958\n",
      "   macro avg       0.65      0.61      0.62     13958\n",
      "weighted avg       0.64      0.63      0.63     13958\n",
      "\n",
      "test loss:\t1.1364 | test acc:\t0.6020\n",
      "\n",
      "Epoch 30/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9459 | train acc:\t0.6395\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      4943\n",
      "           1       0.69      0.53      0.60      2225\n",
      "           2       0.64      0.47      0.55      1539\n",
      "           3       0.57      0.59      0.58      2284\n",
      "           4       0.60      0.51      0.55      1669\n",
      "           5       0.81      0.82      0.82      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.66      0.61      0.63     13958\n",
      "weighted avg       0.64      0.64      0.63     13958\n",
      "\n",
      "test loss:\t1.1441 | test acc:\t0.6079\n",
      "\n",
      "Epoch 31/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9410 | train acc:\t0.6405\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      4943\n",
      "           1       0.69      0.54      0.61      2225\n",
      "           2       0.65      0.48      0.55      1539\n",
      "           3       0.57      0.58      0.58      2284\n",
      "           4       0.60      0.53      0.56      1669\n",
      "           5       0.81      0.81      0.81      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.66      0.62      0.63     13958\n",
      "weighted avg       0.64      0.64      0.64     13958\n",
      "\n",
      "test loss:\t1.1372 | test acc:\t0.6039\n",
      "\n",
      "Epoch 32/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9432 | train acc:\t0.6414\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      4943\n",
      "           1       0.69      0.54      0.60      2225\n",
      "           2       0.65      0.48      0.55      1539\n",
      "           3       0.59      0.59      0.59      2284\n",
      "           4       0.59      0.51      0.54      1669\n",
      "           5       0.81      0.82      0.82      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.66      0.62      0.63     13958\n",
      "weighted avg       0.64      0.64      0.64     13958\n",
      "\n",
      "test loss:\t1.1496 | test acc:\t0.5972\n",
      "\n",
      "Epoch 33/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9405 | train acc:\t0.6432\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      4943\n",
      "           1       0.70      0.54      0.61      2225\n",
      "           2       0.65      0.50      0.56      1539\n",
      "           3       0.59      0.60      0.59      2284\n",
      "           4       0.59      0.51      0.55      1669\n",
      "           5       0.82      0.82      0.82      1298\n",
      "\n",
      "    accuracy                           0.64     13958\n",
      "   macro avg       0.66      0.62      0.64     13958\n",
      "weighted avg       0.65      0.64      0.64     13958\n",
      "\n",
      "test loss:\t1.1159 | test acc:\t0.6061\n",
      "\n",
      "Epoch 34/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9287 | train acc:\t0.6453\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      4943\n",
      "           1       0.69      0.54      0.61      2225\n",
      "           2       0.65      0.49      0.56      1539\n",
      "           3       0.58      0.60      0.59      2284\n",
      "           4       0.60      0.50      0.55      1669\n",
      "           5       0.81      0.84      0.83      1298\n",
      "\n",
      "    accuracy                           0.65     13958\n",
      "   macro avg       0.66      0.62      0.64     13958\n",
      "weighted avg       0.65      0.65      0.64     13958\n",
      "\n",
      "test loss:\t1.1396 | test acc:\t0.6045\n",
      "\n",
      "Epoch 35/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9132 | train acc:\t0.6506\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      4943\n",
      "           1       0.70      0.55      0.62      2225\n",
      "           2       0.67      0.50      0.57      1539\n",
      "           3       0.58      0.60      0.59      2284\n",
      "           4       0.60      0.52      0.56      1669\n",
      "           5       0.82      0.84      0.83      1298\n",
      "\n",
      "    accuracy                           0.65     13958\n",
      "   macro avg       0.67      0.63      0.64     13958\n",
      "weighted avg       0.65      0.65      0.65     13958\n",
      "\n",
      "test loss:\t1.1253 | test acc:\t0.6018\n",
      "\n",
      "Epoch 36/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9069 | train acc:\t0.6496\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      4943\n",
      "           1       0.69      0.53      0.60      2225\n",
      "           2       0.67      0.51      0.58      1539\n",
      "           3       0.58      0.60      0.59      2284\n",
      "           4       0.61      0.53      0.57      1669\n",
      "           5       0.81      0.84      0.82      1298\n",
      "\n",
      "    accuracy                           0.65     13958\n",
      "   macro avg       0.67      0.63      0.64     13958\n",
      "weighted avg       0.65      0.65      0.65     13958\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:\t1.1170 | test acc:\t0.6004\n",
      "\n",
      "Epoch 37/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9074 | train acc:\t0.6510\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      4943\n",
      "           1       0.71      0.55      0.62      2225\n",
      "           2       0.64      0.49      0.56      1539\n",
      "           3       0.59      0.61      0.60      2284\n",
      "           4       0.61      0.52      0.56      1669\n",
      "           5       0.83      0.84      0.83      1298\n",
      "\n",
      "    accuracy                           0.65     13958\n",
      "   macro avg       0.67      0.63      0.64     13958\n",
      "weighted avg       0.65      0.65      0.65     13958\n",
      "\n",
      "test loss:\t1.0890 | test acc:\t0.6043\n",
      "\n",
      "Epoch 38/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8902 | train acc:\t0.6591\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      4943\n",
      "           1       0.70      0.56      0.62      2225\n",
      "           2       0.66      0.52      0.58      1539\n",
      "           3       0.60      0.62      0.61      2284\n",
      "           4       0.61      0.54      0.58      1669\n",
      "           5       0.83      0.83      0.83      1298\n",
      "\n",
      "    accuracy                           0.66     13958\n",
      "   macro avg       0.67      0.64      0.65     13958\n",
      "weighted avg       0.66      0.66      0.66     13958\n",
      "\n",
      "test loss:\t1.1291 | test acc:\t0.6000\n",
      "\n",
      "Epoch 39/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8753 | train acc:\t0.6655\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.71      4943\n",
      "           1       0.70      0.57      0.63      2225\n",
      "           2       0.67      0.51      0.58      1539\n",
      "           3       0.61      0.62      0.61      2284\n",
      "           4       0.61      0.55      0.58      1669\n",
      "           5       0.82      0.84      0.83      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.68      0.64      0.66     13958\n",
      "weighted avg       0.67      0.67      0.66     13958\n",
      "\n",
      "test loss:\t1.1106 | test acc:\t0.6118\n",
      "\n",
      "Epoch 40/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8829 | train acc:\t0.6658\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.78      0.71      4943\n",
      "           1       0.71      0.57      0.63      2225\n",
      "           2       0.67      0.51      0.58      1539\n",
      "           3       0.60      0.62      0.61      2284\n",
      "           4       0.63      0.55      0.59      1669\n",
      "           5       0.82      0.83      0.83      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.68      0.64      0.66     13958\n",
      "weighted avg       0.67      0.67      0.66     13958\n",
      "\n",
      "test loss:\t1.1139 | test acc:\t0.6061\n",
      "\n",
      "Epoch 41/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8716 | train acc:\t0.6676\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      4943\n",
      "           1       0.71      0.57      0.63      2225\n",
      "           2       0.68      0.54      0.60      1539\n",
      "           3       0.61      0.63      0.62      2284\n",
      "           4       0.62      0.55      0.59      1669\n",
      "           5       0.83      0.84      0.83      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.68      0.65      0.66     13958\n",
      "weighted avg       0.67      0.67      0.66     13958\n",
      "\n",
      "test loss:\t1.0882 | test acc:\t0.6081\n",
      "\n",
      "Epoch 42/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8590 | train acc:\t0.6722\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      4943\n",
      "           1       0.71      0.58      0.64      2225\n",
      "           2       0.66      0.53      0.59      1539\n",
      "           3       0.62      0.64      0.63      2284\n",
      "           4       0.64      0.56      0.60      1669\n",
      "           5       0.82      0.85      0.84      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.68      0.65      0.67     13958\n",
      "weighted avg       0.67      0.67      0.67     13958\n",
      "\n",
      "test loss:\t1.1106 | test acc:\t0.6166\n",
      "\n",
      "Epoch 43/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8669 | train acc:\t0.6705\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      4943\n",
      "           1       0.71      0.57      0.64      2225\n",
      "           2       0.68      0.54      0.60      1539\n",
      "           3       0.62      0.63      0.62      2284\n",
      "           4       0.64      0.56      0.60      1669\n",
      "           5       0.83      0.85      0.84      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.69      0.65      0.67     13958\n",
      "weighted avg       0.67      0.67      0.67     13958\n",
      "\n",
      "test loss:\t1.1202 | test acc:\t0.6002\n",
      "\n",
      "Epoch 44/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8504 | train acc:\t0.6785\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      4943\n",
      "           1       0.73      0.59      0.65      2225\n",
      "           2       0.68      0.54      0.61      1539\n",
      "           3       0.63      0.65      0.64      2284\n",
      "           4       0.63      0.55      0.59      1669\n",
      "           5       0.84      0.85      0.85      1298\n",
      "\n",
      "    accuracy                           0.68     13958\n",
      "   macro avg       0.70      0.66      0.67     13958\n",
      "weighted avg       0.68      0.68      0.68     13958\n",
      "\n",
      "test loss:\t1.0846 | test acc:\t0.6089\n",
      "\n",
      "Epoch 45/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8483 | train acc:\t0.6775\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      4943\n",
      "           1       0.71      0.59      0.64      2225\n",
      "           2       0.69      0.55      0.61      1539\n",
      "           3       0.62      0.64      0.63      2284\n",
      "           4       0.63      0.55      0.59      1669\n",
      "           5       0.84      0.85      0.84      1298\n",
      "\n",
      "    accuracy                           0.68     13958\n",
      "   macro avg       0.69      0.66      0.67     13958\n",
      "weighted avg       0.68      0.68      0.67     13958\n",
      "\n",
      "test loss:\t1.1114 | test acc:\t0.6049\n",
      "\n",
      "Epoch 46/46\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8495 | train acc:\t0.6743\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      4943\n",
      "           1       0.73      0.59      0.65      2225\n",
      "           2       0.67      0.54      0.60      1539\n",
      "           3       0.62      0.64      0.63      2284\n",
      "           4       0.64      0.57      0.60      1669\n",
      "           5       0.84      0.84      0.84      1298\n",
      "\n",
      "    accuracy                           0.67     13958\n",
      "   macro avg       0.69      0.66      0.67     13958\n",
      "weighted avg       0.68      0.67      0.67     13958\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n",
      "wandb: ERROR Storing tensors in summary requires h5py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:\t1.1068 | test acc:\t0.6020\n",
      "\n",
      "{'val_Other_f1': 0.6391601562499999, 'val_Other_precision': 0.6215574548907882, 'val_Other_recall': 0.657788944723618, 'val_Saggital_Right_f1': 0.5478841870824053, 'val_Saggital_Right_precision': 0.6624775583482945, 'val_Saggital_Right_recall': 0.4670886075949367, 'val_Transverse_Right_f1': 0.6098418277680141, 'val_Transverse_Right_precision': 0.6547169811320754, 'val_Transverse_Right_recall': 0.5707236842105263, 'val_Saggital_Left_f1': 0.47138554216867473, 'val_Saggital_Left_precision': 0.4860248447204969, 'val_Saggital_Left_recall': 0.45760233918128657, 'val_Transverse_Left_f1': 0.5044176706827309, 'val_Transverse_Left_precision': 0.4373259052924791, 'val_Transverse_Left_recall': 0.5958254269449715, 'val_Bladder_f1': 0.8113590263691685, 'val_Bladder_precision': 0.7766990291262136, 'val_Bladder_recall': 0.8492569002123143, 'test_precision_average_macro': 0.6064669622517246, 'test_recall_average_macro': 0.5997143171446089, 'test_f1score_average_macro': 0.5973414017201656, 'test_precision_average_micro': 0.6019723865877712, 'test_recall_average_micro': 0.6019723865877712, 'test_f1score_average_micro': 0.6019723865877712, 'test_precision_average_weighted': 0.6088878736592753, 'test_recall_average_weighted': 0.6019723865877712, 'test_f1score_average_weighted': 0.600778496001697, 'test_acc_average': 0.6019723865877712}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.66      0.64      1990\n",
      "           1       0.66      0.47      0.55       790\n",
      "           2       0.65      0.57      0.61       608\n",
      "           3       0.49      0.46      0.47       684\n",
      "           4       0.44      0.60      0.50       527\n",
      "           5       0.78      0.85      0.81       471\n",
      "\n",
      "    accuracy                           0.60      5070\n",
      "   macro avg       0.61      0.60      0.60      5070\n",
      "weighted avg       0.61      0.60      0.60      5070\n",
      "\n",
      "\n",
      "[[1309   78  140  154  228   81]\n",
      " [ 218  369   17  136   43    7]\n",
      " [ 166    4  347    9   74    8]\n",
      " [ 199   97    9  313   53   13]\n",
      " [ 150    8   17   32  314    6]\n",
      " [  64    1    0    0    6  400]]\n",
      "Training complete in 26m 12s\n",
      "Best val acc: 0.601972\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/denizjafari/hnultra_test\" target=\"_blank\">https://app.wandb.ai/denizjafari/hnultra_test</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/denizjafari/hnultra_test/runs/3ujz536f\" target=\"_blank\">https://app.wandb.ai/denizjafari/hnultra_test/runs/3ujz536f</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-90d0443ef86e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m               'conv3_filters': conv3_filters, 'linear1_size': linear1_size }\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain5fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_configs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion_used\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-d11a4986515d>\u001b[0m in \u001b[0;36mtrain5fold\u001b[0;34m(network_configs, criterion_used, model_ft, lr, wd, amsgrad, repetitions)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdate_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%d-%m-%Y.%H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hnultra_test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwandb_username\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_username\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_fold_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mpartition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_folds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hnu/lib/python3.8/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, name, notes, id, magic, anonymous, config_exclude_keys, config_include_keys)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0mallow_val_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtelemetry_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m         run.config._update(config,\n\u001b[0m\u001b[1;32m   1110\u001b[0m                 \u001b[0mexclude_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_exclude_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m                 \u001b[0minclude_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_include_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hnu/lib/python3.8/site-packages/wandb/wandb_config.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, params, allow_val_change, as_defaults, exclude_keys, include_keys)\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_val_change\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hnu/lib/python3.8/site-packages/wandb/wandb_config.py\u001b[0m in \u001b[0;36mpersist\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mconf_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jupyter_agent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jupyter_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hnu/lib/python3.8/site-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcloud\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"dryrun\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_stream_api\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmirror_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hnu/lib/python3.8/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, run, project, tags, cloud, output, port)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;31m# Calling .start() on _meta and _system_stats will spin a thread that reports system stats every 30 seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSystemStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"jobType\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hnu/lib/python3.8/site-packages/wandb/meta.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api, out_dir)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hnu/lib/python3.8/site-packages/wandb/meta.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Windows\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SIGALRM'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0min_jupyter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"non time limited probe of code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_code_git\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_code_program\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hnu/lib/python3.8/site-packages/wandb/meta.py\u001b[0m in \u001b[0;36m_setup_code_git\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m             }\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"email\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_code_program\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hnu/lib/python3.8/site-packages/wandb/git_repo.py\u001b[0m in \u001b[0;36mroot\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrev_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--show-toplevel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hnu/lib/python3.8/site-packages/git/cmd.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mLazyMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_persistent_git_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hnu/lib/python3.8/site-packages/git/cmd.py\u001b[0m in \u001b[0;36m_call_process\u001b[0;34m(self, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1003\u001b[0m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexec_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parse_object_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hnu/lib/python3.8/site-packages/git/cmd.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, istream, with_extended_output, with_exceptions, as_process, output_stream, stdout_as_string, kill_after_timeout, with_stdout, universal_newlines, shell, env, max_chunk_size, **subprocess_kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m                   command, cwd, universal_newlines, shell, istream_ok)\n\u001b[1;32m    720\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m             proc = Popen(command,\n\u001b[0m\u001b[1;32m    722\u001b[0m                          \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                          \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hnu/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    852\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hnu/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1635\u001b[0m                     \u001b[0mfds_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpass_fds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m                     \u001b[0mfds_to_keep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrpipe_write\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1637\u001b[0;31m                     self.pid = _posixsubprocess.fork_exec(\n\u001b[0m\u001b[1;32m   1638\u001b[0m                             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutable_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m                             \u001b[0mclose_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfds_to_keep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "repetitions = 5\n",
    "\n",
    "conv1_filters = 8\n",
    "conv2_filters = 16\n",
    "conv3_filters = 32\n",
    "linear1_size = 512\n",
    "\n",
    "dropout = 0.25\n",
    "lr = 0.0005\n",
    "wd = 0.001\n",
    "amsgrad = False\n",
    "\n",
    "config_string = f\"{conv1_filters}_{conv2_filters}_{conv3_filters}_{linear1_size}_{dropout}_{lr}_{wd}_{amsgrad}\"\n",
    "model_ft = ViewNet(num_classes, conv1_filters, conv2_filters, conv3_filters, linear1_size, dropout)\n",
    "run_configs = {'lr': lr, 'wd': wd, 'amsgrad': amsgrad,'dropout': dropout, \n",
    "              'conv1_filters': conv1_filters, 'conv2_filters': conv2_filters, \n",
    "              'conv3_filters': conv3_filters, 'linear1_size': linear1_size }\n",
    "\n",
    "final_df = train5fold(run_configs,criterion_used, model_ft, lr, wd, amsgrad, repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "image_label.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "hnu",
   "language": "python",
   "name": "hnu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
