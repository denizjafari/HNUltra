{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "import time\n",
    "import os\n",
    "from torch.utils import data\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0') \n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root directory\n",
    "andrea_dir = \"/home/andreasabo/Documents/HNProject/\"\n",
    "\n",
    "# data directory on current machine: abhishekmoturu, andreasabo, denizjafari, navidkorhani\n",
    "data_dir = \"/home/nkorhani/Documents/HNProject/all_label_img/\"\n",
    "\n",
    "# read target df\n",
    "csv_path = os.path.join(andrea_dir, \"all_splits_1000000.csv\")\n",
    "data_df = pd.read_csv(csv_path, usecols=['subj_id', 'image_ids', 'view_label', 'view_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'Other':0, 'Saggital_Right':1, 'Transverse_Right':2, \n",
    "                 'Saggital_Left':3, 'Transverse_Left':4, 'Bladder':5}\n",
    "\n",
    "data_df['view_label'] = data_df['view_label'].map(label_mapping)\n",
    "\n",
    "train_df = data_df[data_df.view_train == 1]\n",
    "test_df = data_df[data_df.view_train == 0]\n",
    "\n",
    "labels = {}\n",
    "train_and_valid_subj_ids = []\n",
    "train_and_valid_image_ids = []\n",
    "test_ids = []\n",
    "\n",
    "for ind, row in train_df.iterrows():\n",
    "    train_and_valid_subj_ids.append(row['subj_id'])\n",
    "    train_and_valid_image_ids.append(row['image_ids'])\n",
    "    labels[row['image_ids']] = row['view_label']\n",
    "\n",
    "for ind, row in test_df.iterrows():\n",
    "    test_ids.append(row['image_ids'])\n",
    "    labels[row['image_ids']] = row['view_label']\n",
    "\n",
    "s = set()\n",
    "t_v_ids = pd.DataFrame(list(zip(train_and_valid_subj_ids, train_and_valid_image_ids)), columns=['subj_ids', 'image_ids'])\n",
    "id_groups = [t_v_ids for _, t_v_ids in t_v_ids.groupby('subj_ids')]\n",
    "random.shuffle(id_groups)\n",
    "id_groups = pd.concat(id_groups).reset_index(drop=True)\n",
    "train_val_split = int(0.8*len(set(id_groups['subj_ids'].values)))\n",
    "train_val_set = [i for i in id_groups['subj_ids'].values if not (i in s or s.add(i))]\n",
    "cutoff = train_val_set[train_val_split]\n",
    "train_portion = (id_groups['subj_ids'].values == cutoff).argmax()\n",
    "\n",
    "train_ids = id_groups[:train_portion]['image_ids'].tolist()\n",
    "valid_ids = id_groups[train_portion:]['image_ids'].tolist()\n",
    "\n",
    "partition = {'train':train_ids, 'valid':valid_ids, 'test':test_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(partition['train'], labels)\n",
    "train_loader = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(partition['valid'], labels)\n",
    "validation_loader = data.DataLoader(validation_set, **params)\n",
    "\n",
    "test_set = Dataset(partition['valid'], labels)\n",
    "validation_loader = data.DataLoader(test_set, **params)\n",
    "\n",
    "dataloaders_dict = {'train':training_generator, 'val':validation_generator}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        hidden_dim = 800\n",
    "        latent_dim = 100\n",
    "        self.fc1 = nn.Linear(65536, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 65536)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        #print(\"z.size() =\", z.size())\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        #print(\"h3.size() =\", h3.size())\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 65536))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "vae_model = VAE().to(device)\n",
    "checkpoint = torch.load('results/h800_l100_e30/vae_model.pt')\n",
    "vae_model.load_state_dict(checkpoint)\n",
    "vae_model.eval()\n",
    "\n",
    "for params in vae_model.parameters():\n",
    "    params.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, list_IDs, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        img_path = data_dir + ID + '.jpg'\n",
    "        image = Image.open(img_path)#.convert('L')\n",
    "        image = ToTensor()(image)\n",
    "        \n",
    "        mu, logvar = vae_model.encode(image.view(-1, 65536))\n",
    "        z = vae_model.reparameterize(mu, logvar)\n",
    "\n",
    "        y = torch.FloatTensor(self.labels[ID])\n",
    "\n",
    "        return z, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepClassifier, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(100, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 6)\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    \n",
    "    def forward(self, z):\n",
    "        h1 = F.relu(self.fc1(z))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        out = self.softmax(self.fc3(h2))\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "val_check_interval = 40\n",
    "\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = DeepClassifier().to(device)\n",
    "\n",
    "optimizer = optim.Adam(classifier_model.parameters(), lr=1e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_loss_array = np.array([])\n",
    "val_loss_array = np.array([])\n",
    "test_loss_array = np.array([])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    global batches_loss_array\n",
    "    global train_loss_array\n",
    "    \n",
    "    classifier_model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = classifier_model(inputs)\n",
    "        \n",
    "        loss = criterion(preds, targets)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.2f} '.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item()))\n",
    "            batches_loss_array = np.append(batches_loss_array,loss.item())\n",
    "            \n",
    "            \n",
    "        avg_train_loss = train_loss / len(train_loader.dataset) * len(inputs)\n",
    "    \n",
    "    #print('====> Epoch: {} Average Training loss: {:.2f}'.format(\n",
    "    #      epoch, avg_train_loss))\n",
    "    \n",
    "    train_loss_array = np.append(train_loss_array, avg_train_loss)\n",
    "    \n",
    "    return avg_train_loss\n",
    "\n",
    "\n",
    "def evaluation(epoch):\n",
    "    global least_error\n",
    "    global val_loss_array\n",
    "    \n",
    "    classifier_model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in validation_loader:\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            preds = classifier_model(inputs)\n",
    "\n",
    "            loss = criterion(preds, targets)\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = eval_loss / len(eval_loader.dataset) * len(inputs)\n",
    "        \n",
    "    val_loss_array = np.append(val_loss_array, avg_val_loss)\n",
    "    \n",
    "    if least_error==-1 or least_error>avg_val_loss:\n",
    "        PATH = os.path.join('saved_models', 'vae_classifier.pt')\n",
    "    \n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        \n",
    "        least_error = avg_val_loss\n",
    "        \n",
    "    return avg_val_loss\n",
    "        \n",
    "def test(epoch):\n",
    "\n",
    "    #global test_loss_array\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            preds = model(inputs)\n",
    "            \n",
    "            test_loss += citerion(preds, targets).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset) * len(inputs)\n",
    "    \n",
    "    #test_loss_array = np.append(test_loss_array, test_loss)\n",
    "    \n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in epochs:\n",
    "    train_loss = train()\n",
    "    val_loss = evaluation()\n",
    "    \n",
    "    print('====> Epoch: {}  Avg Train loss: {:.2f} ... Avg Validation loss: {:.2f}'.format(\n",
    "              epoch, train_loss, val_loss))\n",
    "    \n",
    "test_loss = test()\n",
    "\n",
    "print(\"Test Loss: {}\".format(test_loss))\n",
    "\n",
    "np.save('vae_class_train.npy', train_loss_array)\n",
    "np.save('vae_class_val.npy', val_loss_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}