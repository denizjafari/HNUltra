{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This notebook is used for splitting data into train/val/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to your local\n",
    "ROOT_DIR = \"/home/andreasabo/Documents/HNProject\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_to_split_on = \"at_least_one_target_and_labelled_view\"\n",
    "data_set_to_split_on = \"labelled_view\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(ROOT_DIR, \"all_label_df.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()\n",
    "split_names = [\"at_least_one_target_and_labelled_view\", \"labelled_view\"]\n",
    "full_data_frame = df.copy()\n",
    "# print(full_data_frame)\n",
    "print(len(full_data_frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths_to_img(dataset, image_dir):\n",
    "    filepaths = pd.DataFrame()\n",
    "    filepaths['paths'] = image_dir + dataset['subj_id'].map(str) + \"_\" + dataset['scan_num'].map(str) + \"_\" + dataset['image_num'].map(str) + \".jpg\" \n",
    "    return filepaths\n",
    "\n",
    "\n",
    "def split_data_based_on_patients(df_to_split, image_dir, train_ids, test_ids):\n",
    "    train_set = df_to_split[df_to_split.subj_id.isin(train_ids)]\n",
    "    test_set = df_to_split[df_to_split.subj_id.isin(test_ids)]\n",
    "    \n",
    "    train_filepaths = get_filepaths_to_img(train_set, image_dir)\n",
    "    test_filepaths = get_filepaths_to_img(test_set, image_dir)\n",
    "\n",
    "    return train_filepaths, test_filepaths, train_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scanner_proportion_diff(patient_scan_details, train, test):\n",
    "    train_machine_counts = patient_scan_details[patient_scan_details.index.isin(train)]\n",
    "    test_machine_counts = patient_scan_details[patient_scan_details.index.isin(test)]\n",
    "\n",
    "    proportions_test = test_machine_counts.sum(axis=0) / test_machine_counts['img_count'].sum()\n",
    "    proportions_train = train_machine_counts.sum(axis=0) / train_machine_counts['img_count'].sum()\n",
    "    diff = abs(proportions_train - proportions_test)\n",
    "    \n",
    "    return diff.sum()\n",
    "\n",
    "def calculate_outcome_proportion_diff(df, train, test):\n",
    "    diff_sum = 0\n",
    "    train_set = df[df.subj_id.isin(train)]\n",
    "    test_set = df[df.subj_id.isin(test)]\n",
    "    \n",
    "    outcomes = ['reflux_label', 'surgery_label', 'function_label']\n",
    "\n",
    "    for outcome in outcomes:\n",
    "        train_set_counts = train_set[outcome].value_counts()\n",
    "        test_set_counts = test_set[outcome].value_counts()\n",
    "        \n",
    "        train_count = len(train_set)\n",
    "        test_count = len(test_set)\n",
    "        \n",
    "        if outcome == \"function_label\":\n",
    "            # For function, we are only interested in missing vs. non-missing\n",
    "            train_missing_frac = train_set_counts[train_set_counts.index == \"Missing\"] / train_count\n",
    "            test_missing_frac = test_set_counts[test_set_counts.index == \"Missing\"] / test_count\n",
    "            \n",
    "            # Some mess\n",
    "            train_no_missing = train_set_counts[train_set_counts.index != \"Missing\"]\n",
    "            test_no_missing = test_set_counts[test_set_counts.index != \"Missing\"]\n",
    "            train_no_missing = train_no_missing.reset_index()\n",
    "            train_no_missing['function'] = train_no_missing['index']\n",
    "            train_no_missing[[\"function\"]] = train_no_missing[[\"function\"]].apply(pd.to_numeric)\n",
    "\n",
    "            \n",
    "            test_no_missing = test_no_missing.reset_index()\n",
    "            test_no_missing['function'] = test_no_missing['index']\n",
    "            test_no_missing[[\"function\"]] = test_no_missing[[\"function\"]].apply(pd.to_numeric)\n",
    "            \n",
    "            \n",
    "            train_good_frac = train_no_missing[(train_no_missing.function <= 60) & (train_no_missing.function >= 40)]\n",
    "            test_good_frac = test_no_missing[(test_no_missing.function <= 60) & (test_no_missing.function >= 40)] \n",
    "            \n",
    "            good_diff = abs((train_good_frac.function_label.sum() / train_count) \\\n",
    "                            - (test_good_frac.function_label.sum()/ test_count))\n",
    "            \n",
    "            \n",
    "            train_bad_frac = train_no_missing[(train_no_missing.function > 60) | (train_no_missing.function < 40)]\n",
    "            test_bad_frac = test_no_missing[(test_no_missing.function > 60) | (test_no_missing.function < 40)] \n",
    "\n",
    "            bad_diff = abs((train_bad_frac.function_label.sum() / train_count) \\\n",
    "                            - (test_bad_frac.function_label.sum()/ test_count))\n",
    "            \n",
    "            # This is proportion missing vs. not\n",
    "            new_diff = list(abs(train_missing_frac - test_missing_frac))[0]\n",
    "\n",
    "            \n",
    "            diff_sum = diff_sum + new_diff + good_diff + bad_diff\n",
    "        else:\n",
    "            train_set_counts = train_set_counts/ train_count\n",
    "            test_set_counts = test_set_counts/ test_count\n",
    "            \n",
    "            new_diff = abs(test_set_counts - train_set_counts).sum() / 2\n",
    "            \n",
    "            diff_sum += new_diff\n",
    "            \n",
    "    return diff_sum\n",
    "\n",
    "\n",
    "def select_split(df, patient_scan_details, test_percentage=0.2, num_trials=1000000):\n",
    "    # Just intialize to large value, this will be overwritten in the first iteration\n",
    "    lowest_diff = 100\n",
    "    lowest_diff_seed = 0\n",
    "    \n",
    "    # Sample many trials, and then select the one with the smallest difference in proportions\n",
    "    for i in range(num_trials):\n",
    "        patients = df.subj_id.unique()\n",
    "        train, test = train_test_split(patients, test_size=test_percentage, random_state=i)\n",
    "        cur_diff_scanner = calculate_scanner_proportion_diff(patient_scan_details, train, test)\n",
    "        cur_diff_label = calculate_outcome_proportion_diff(df, train, test)\n",
    "        \n",
    "        if data_set_to_split_on == \"labelled_view\":\n",
    "            cur_diff = cur_diff_scanner\n",
    "        else:\n",
    "            cur_diff = cur_diff_label + cur_diff_scanner\n",
    "        # if this split has the lowest proportions, remember it\n",
    "        if cur_diff < lowest_diff:\n",
    "            lowest_diff = cur_diff\n",
    "            lowest_diff_seed = i\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    print(\"Best config: \",lowest_diff, lowest_diff_seed)\n",
    "    \n",
    "    # Split on the best random state we found\n",
    "    train, test = train_test_split(patients, test_size=test_percentage, random_state=lowest_diff_seed)\n",
    "    cur_diff = calculate_scanner_proportion_diff(patient_scan_details, train, test)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in range(len(split_names)):\n",
    "    data_set_to_split_on = split_names[split]\n",
    "    # Define the df of images we want to consider as the entire dataset\n",
    "    if data_set_to_split_on == \"at_least_one_target_and_labelled_view\":\n",
    "        labelled_view_df = df[(df.view_label != \"Missing\") & (df.view_label != \"Other\")]\n",
    "        frame_to_split = labelled_view_df[(labelled_view_df.function_label != \"Missing\") | (labelled_view_df.reflux_label != \"Missing\") | (labelled_view_df.surgery_label != \"Missing\")]\n",
    "\n",
    "    elif  data_set_to_split_on == \"labelled_view\":\n",
    "        frame_to_split = df[(df.view_label != \"Missing\")]\n",
    "    elif 0: # Replace this with other datasets as we define them\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception(\"Invalid data to split on!\")\n",
    "\n",
    "\n",
    "    # Some QA to make sure the dataset we are using is what we expect it to be\n",
    "    frame_to_split_count = frame_to_split.groupby('subj_id').scan_num.agg(['nunique'])\n",
    "    print(f\"There are {len(frame_to_split)} from {frame_to_split_count['nunique'].sum()} scans from {frame_to_split.subj_id.nunique()} patients in this dataframe.\")\n",
    "    print(frame_to_split.columns)\n",
    "\n",
    "\n",
    "    # Add image counts and percentage of total images that belong to each patient\n",
    "    patient_scan_details = frame_to_split.groupby('subj_id').image_num.agg(img_count='count')\n",
    "    patient_scan_details['img_percent_count'] = patient_scan_details['img_count'] / patient_scan_details['img_count'].sum()\n",
    "\n",
    "    # Calculate percent (of all images) that were taken by each machine for each patient \n",
    "    machine_labels = dict(frame_to_split['image_manu'].value_counts())\n",
    "    for machine in machine_labels:\n",
    "        this_machine = frame_to_split[frame_to_split['image_manu'] == machine]\n",
    "        this_machine_count = this_machine.groupby('subj_id').image_manu.count()\n",
    "    #     patient_scan_details[\"percent_\" + machine] = this_machine.groupby('subj_id').image_manu.count() / len(frame_to_split)\n",
    "        patient_scan_details[machine] = this_machine.groupby('subj_id').image_manu.count()\n",
    "\n",
    "    # Replace nans with 0s because they're easier to work with\n",
    "    patient_scan_details = patient_scan_details.fillna(0)\n",
    "    print(patient_scan_details.sum(axis = 0))\n",
    "    \n",
    "    train_subjs, test_subjs = select_split(frame_to_split, patient_scan_details)\n",
    "          \n",
    "          \n",
    "    train_filepaths, test_filepaths, train_set, test_set = split_data_based_on_patients(frame_to_split, os.path.join(ROOT_DIR, 'all_label_img/'), train_subjs, test_subjs)  \n",
    "\n",
    "          \n",
    "    if data_set_to_split_on == \"at_least_one_target_and_labelled_view\":\n",
    "        output_name = 'outcome_train'\n",
    "    elif  data_set_to_split_on == \"labelled_view\": \n",
    "        output_name = 'view_train'\n",
    "    # Add all of these labels to the overall dataframe\n",
    "    full_data_frame[output_name] = np.nan\n",
    "    train_image_inds = full_data_frame[full_data_frame.image_ids.isin(train_set.image_ids)].index\n",
    "    test_image_inds = full_data_frame[full_data_frame.image_ids.isin(test_set.image_ids)].index\n",
    "\n",
    "    full_data_frame.loc[ train_image_inds , output_name ] = 1\n",
    "    full_data_frame.loc[ test_image_inds , output_name ] = 0\n",
    "          \n",
    "full_data_frame.to_csv(os.path.join(ROOT_DIR, 'all_splits_1000000.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_frame.view_train.hist()\n",
    "full_data_frame.outcome_train.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# train_filepaths, test_filepaths, train_set, test_set = split_data_based_on_patients(frame_to_split, os.path.join(ROOT_DIR, 'all_label_img/'), train_subjs, test_subjs)  \n",
    "\n",
    "# # Add all of these labels to the overall dataframe\n",
    "# full_data_frame['view_train'] = np.nan\n",
    "# train_image_inds = full_data_frame[full_data_frame.image_ids.isin(train_set.image_ids)].index\n",
    "# test_image_inds = full_data_frame[full_data_frame.image_ids.isin(test_set.image_ids)].index\n",
    "\n",
    "# full_data_frame.loc[ train_image_inds , 'view_train' ] = 1\n",
    "# full_data_frame.loc[ test_image_inds , 'view_train' ] = 0\n",
    "\n",
    "# full_data_frame.view_train.hist()\n",
    "\n",
    "# full_data_frame['view_train']  = full_data_frame[full_data_frame.image_ids.isin(train_set.image_ids)]\n",
    "\n",
    "# print(full_data_frame[full_data_frame.image_ids.isin(train_set.image_ids)].index)\n",
    "# print(full_data_frame.head())\n",
    "# print(full_data_frame)\n",
    "\n",
    "# # train_filepaths.to_csv(os.path.join(ROOT_DIR, 'train_filepaths_view_balanced_20.csv'))\n",
    "# # test_filepaths.to_csv(os.path.join(ROOT_DIR, 'test_filepaths_view_and_balanced_20.csv'))\n",
    "# full_data_frame.view_train.hist()\n",
    "# print(len(train_set))\n",
    "# print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data_frame.subj_id.isin(train_subjs)\n",
    "# x = full_data_frame.subj_id.isin(train_subjs)\n",
    "# print(full_data_frame.subj_id.isin(train_subjs))\n",
    "# x.hist()\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "train_set.image_manu.hist()\n",
    "test_set.image_manu.hist()\n",
    "plt.xticks(rotation=90)\n",
    "plt.figure()\n",
    "train_set.image_manu.hist()\n",
    "plt.xticks(rotation=90)\n",
    "plt.figure()\n",
    "test_set.image_manu.hist()\n",
    "plt.xticks(rotation=90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}