{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZbZRovgc-In"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from torch.utils import data\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2aVSXNyc-Iv"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda') \n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylfRtoN9c-Iy"
   },
   "outputs": [],
   "source": [
    "# Change this to your local\n",
    "ANDREA_DIR = \"/home/andreasabo/Documents/HNProject\"\n",
    "\n",
    "# to the ImageFolder structure\n",
    "data_dir = \"/home/navidkorhani/Documents/HNProject/all_label_img/\"\n",
    "\n",
    "\n",
    "# read target df\n",
    "csv_path = os.path.join(ANDREA_DIR, \"all_splits_100000.csv\")\n",
    "data_df = pd.read_csv(csv_path, usecols=['image_ids', 'view_label', 'view_train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72WtxtY8ynt1"
   },
   "source": [
    "### **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYRzqa5Uc-JB"
   },
   "outputs": [],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "# right_sag, right_trav, left_sag, left_trav, bladder, other\n",
    "num_classes = 6\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 64\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 15\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_OCR_7uy52w"
   },
   "source": [
    "### **Reading Data Indicies and Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nnwavxcqGBv"
   },
   "outputs": [],
   "source": [
    "label_mapping = {'Other':0, 'Saggital_Right':1, 'Transverse_Right':2, \n",
    "                 'Saggital_Left':3, 'Transverse_Left':4, 'Bladder':5}\n",
    "\n",
    "data_df['view_label'] = data_df['view_label'].map(label_mapping)\n",
    "\n",
    "train_df = data_df[data_df.view_train == 1]\n",
    "test_df = data_df[data_df.view_train == 0]\n",
    "\n",
    "labels = {}\n",
    "train_and_valid_ids = []\n",
    "test_ids = []\n",
    "\n",
    "for ind, row in train_df.iterrows():\n",
    "    train_and_valid_ids.append(row['image_ids'])\n",
    "    labels[row['image_ids']] = row['view_label']\n",
    "\n",
    "for ind, row in test_df.iterrows():\n",
    "    test_ids.append(row['image_ids'])\n",
    "    labels[row['image_ids']] = row['view_label']\n",
    "\n",
    "train_portion = int(0.8*len(train_and_valid_ids))\n",
    "random.shuffle(train_and_valid_ids)\n",
    "\n",
    "train_ids = train_and_valid_ids[:train_portion]\n",
    "valid_ids = train_and_valid_ids[train_portion:]\n",
    "\n",
    "partition = {'train':train_ids, 'valid':valid_ids}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5p70l48MzfMP"
   },
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_fnkR4Tc-JH"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                labels = labels.type(torch.long)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72y17dlcc-JL"
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BbgEWoqKc-JO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 256\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "#print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qhoGXnXmzjd4"
   },
   "source": [
    "### **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iikM7_G3c-JR"
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, list_IDs, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        img_path = data_dir + ID + '.jpg'\n",
    "        image = Image.open(img_path)\n",
    "        image = ToTensor()(image)#.unsqueeze(0) # unsqueeze to add artificial first dimension\n",
    "        \n",
    "        y = self.labels[ID]\n",
    "\n",
    "        return image, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DdSxc4Dhc-JT"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Data augmentation and normalization for training\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(partition['train'], labels)\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(partition['valid'], labels)\n",
    "validation_generator = data.DataLoader(validation_set, **params)\n",
    "\n",
    "dataloaders_dict = {'train':training_generator, 'val':validation_generator}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8MJGKSRz-Rz"
   },
   "source": [
    "### **Running the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "07uuFRNNc-JW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRFzmztTc-Ja"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 1.1348 Acc: 0.5679\n",
      "val Loss: 1.1539 Acc: 0.5630\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 1.1280 Acc: 0.5675\n",
      "val Loss: 1.1622 Acc: 0.5545\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 1.1272 Acc: 0.5721\n",
      "val Loss: 1.1576 Acc: 0.5609\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 1.1249 Acc: 0.5689\n",
      "val Loss: 1.1574 Acc: 0.5623\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 1.1214 Acc: 0.5716\n",
      "val Loss: 1.1416 Acc: 0.5701\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 1.1169 Acc: 0.5724\n",
      "val Loss: 1.1512 Acc: 0.5605\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 1.1163 Acc: 0.5723\n",
      "val Loss: 1.1501 Acc: 0.5634\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 1.1128 Acc: 0.5750\n",
      "val Loss: 1.1488 Acc: 0.5627\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 1.1118 Acc: 0.5799\n",
      "val Loss: 1.1497 Acc: 0.5616\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 1.1092 Acc: 0.5759\n",
      "val Loss: 1.1485 Acc: 0.5694\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 1.1101 Acc: 0.5744\n",
      "val Loss: 1.1492 Acc: 0.5672\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 1.1041 Acc: 0.5819\n",
      "val Loss: 1.1389 Acc: 0.5644\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 1.1041 Acc: 0.5772\n",
      "val Loss: 1.1411 Acc: 0.5602\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 1.0992 Acc: 0.5794\n",
      "val Loss: 1.1391 Acc: 0.5662\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 1.0986 Acc: 0.5828\n",
      "val Loss: 1.1493 Acc: 0.5602\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 1.0966 Acc: 0.5836\n",
      "val Loss: 1.1435 Acc: 0.5694\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 1.0998 Acc: 0.5815\n",
      "val Loss: 1.1579 Acc: 0.5669\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 1.0961 Acc: 0.5816\n",
      "val Loss: 1.1396 Acc: 0.5676\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 1.0943 Acc: 0.5862\n",
      "val Loss: 1.1502 Acc: 0.5715\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 1.0919 Acc: 0.5868\n",
      "val Loss: 1.1426 Acc: 0.5662\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 1.0930 Acc: 0.5827\n",
      "val Loss: 1.1473 Acc: 0.5619\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 1.0928 Acc: 0.5866\n",
      "val Loss: 1.1418 Acc: 0.5623\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 1.0890 Acc: 0.5862\n",
      "val Loss: 1.1468 Acc: 0.5584\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 1.0835 Acc: 0.5848\n",
      "val Loss: 1.1528 Acc: 0.5637\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 1.0846 Acc: 0.5886\n",
      "val Loss: 1.1425 Acc: 0.5623\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 1.0800 Acc: 0.5908\n",
      "val Loss: 1.1474 Acc: 0.5595\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 1.0842 Acc: 0.5885\n",
      "val Loss: 1.1425 Acc: 0.5637\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 1.0843 Acc: 0.5870\n",
      "val Loss: 1.1481 Acc: 0.5651\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 1.0853 Acc: 0.5834\n",
      "val Loss: 1.1421 Acc: 0.5634\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 1.0820 Acc: 0.5896\n",
      "val Loss: 1.1457 Acc: 0.5637\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 1.0843 Acc: 0.5900\n",
      "val Loss: 1.1579 Acc: 0.5577\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 1.0832 Acc: 0.5867\n",
      "val Loss: 1.1946 Acc: 0.5439\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 1.0839 Acc: 0.5869\n",
      "val Loss: 1.1400 Acc: 0.5676\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 1.0782 Acc: 0.5905\n",
      "val Loss: 1.1472 Acc: 0.5535\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 1.0748 Acc: 0.5926\n",
      "val Loss: 1.1414 Acc: 0.5672\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 1.0779 Acc: 0.5903\n",
      "val Loss: 1.1469 Acc: 0.5588\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 1.0789 Acc: 0.5897\n",
      "val Loss: 1.1516 Acc: 0.5514\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 1.0803 Acc: 0.5863\n",
      "val Loss: 1.1482 Acc: 0.5679\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 1.0725 Acc: 0.5909\n",
      "val Loss: 1.1454 Acc: 0.5602\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 1.0723 Acc: 0.5950\n",
      "val Loss: 1.1462 Acc: 0.5718\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 1.0745 Acc: 0.5905\n",
      "val Loss: 1.1405 Acc: 0.5619\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 1.0723 Acc: 0.5847\n",
      "val Loss: 1.1380 Acc: 0.5690\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 1.0716 Acc: 0.5907\n",
      "val Loss: 1.1541 Acc: 0.5676\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 1.0693 Acc: 0.5932\n",
      "val Loss: 1.1443 Acc: 0.5630\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 1.0702 Acc: 0.5938\n",
      "val Loss: 1.1493 Acc: 0.5725\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 1.0726 Acc: 0.5926\n",
      "val Loss: 1.1551 Acc: 0.5672\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 1.0751 Acc: 0.5920\n",
      "val Loss: 1.1535 Acc: 0.5595\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 1.0730 Acc: 0.5938\n",
      "val Loss: 1.1664 Acc: 0.5545\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 1.0659 Acc: 0.5976\n",
      "val Loss: 1.1479 Acc: 0.5679\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 1.0679 Acc: 0.5960\n",
      "val Loss: 1.1407 Acc: 0.5623\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 1.0672 Acc: 0.5928\n",
      "val Loss: 1.1417 Acc: 0.5623\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 1.0707 Acc: 0.5957\n",
      "val Loss: 1.1597 Acc: 0.5507\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 1.0626 Acc: 0.5965\n",
      "val Loss: 1.1461 Acc: 0.5616\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 1.0657 Acc: 0.5957\n",
      "val Loss: 1.1533 Acc: 0.5676\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 1.0674 Acc: 0.5930\n",
      "val Loss: 1.1548 Acc: 0.5595\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 1.0647 Acc: 0.5965\n",
      "val Loss: 1.1567 Acc: 0.5556\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 1.0659 Acc: 0.5952\n",
      "val Loss: 1.1546 Acc: 0.5567\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 1.0644 Acc: 0.5963\n",
      "val Loss: 1.1440 Acc: 0.5623\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 1.0647 Acc: 0.5954\n",
      "val Loss: 1.1454 Acc: 0.5662\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 1.0633 Acc: 0.5975\n",
      "val Loss: 1.1462 Acc: 0.5605\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 1.0617 Acc: 0.5984\n",
      "val Loss: 1.1586 Acc: 0.5542\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 1.0622 Acc: 0.5941\n",
      "val Loss: 1.1636 Acc: 0.5503\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 1.0635 Acc: 0.5989\n",
      "val Loss: 1.1460 Acc: 0.5694\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 1.0624 Acc: 0.5943\n",
      "val Loss: 1.1462 Acc: 0.5595\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 1.0635 Acc: 0.5955\n",
      "val Loss: 1.1532 Acc: 0.5665\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 1.0612 Acc: 0.5957\n",
      "val Loss: 1.1445 Acc: 0.5605\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 1.0588 Acc: 0.5975\n",
      "val Loss: 1.1430 Acc: 0.5627\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 1.0597 Acc: 0.5960\n",
      "val Loss: 1.1463 Acc: 0.5669\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 1.0560 Acc: 0.5966\n",
      "val Loss: 1.1599 Acc: 0.5584\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 1.0651 Acc: 0.5910\n",
      "val Loss: 1.1508 Acc: 0.5563\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 1.0617 Acc: 0.5945\n",
      "val Loss: 1.1657 Acc: 0.5503\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 1.0593 Acc: 0.5965\n",
      "val Loss: 1.1464 Acc: 0.5665\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 1.0614 Acc: 0.5958\n",
      "val Loss: 1.1618 Acc: 0.5591\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 1.0592 Acc: 0.5959\n",
      "val Loss: 1.1458 Acc: 0.5641\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 1.0544 Acc: 0.5948\n",
      "val Loss: 1.1524 Acc: 0.5567\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 1.0576 Acc: 0.5970\n",
      "val Loss: 1.1506 Acc: 0.5595\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 1.0613 Acc: 0.5968\n",
      "val Loss: 1.1551 Acc: 0.5528\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 1.0523 Acc: 0.5990\n",
      "val Loss: 1.1531 Acc: 0.5598\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 1.0509 Acc: 0.6020\n",
      "val Loss: 1.1481 Acc: 0.5739\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 1.0536 Acc: 0.6016\n",
      "val Loss: 1.1542 Acc: 0.5563\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 1.0535 Acc: 0.6012\n",
      "val Loss: 1.1630 Acc: 0.5598\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 1.0574 Acc: 0.5958\n",
      "val Loss: 1.1588 Acc: 0.5602\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 1.0557 Acc: 0.6017\n",
      "val Loss: 1.1485 Acc: 0.5570\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 1.0560 Acc: 0.5998\n",
      "val Loss: 1.1599 Acc: 0.5581\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 1.0505 Acc: 0.6030\n",
      "val Loss: 1.1490 Acc: 0.5574\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 1.0524 Acc: 0.6003\n",
      "val Loss: 1.1493 Acc: 0.5574\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 1.0535 Acc: 0.6023\n",
      "val Loss: 1.1535 Acc: 0.5612\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 1.0500 Acc: 0.6051\n",
      "val Loss: 1.1605 Acc: 0.5542\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 1.0552 Acc: 0.5995\n",
      "val Loss: 1.1518 Acc: 0.5612\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 1.0531 Acc: 0.5993\n",
      "val Loss: 1.1533 Acc: 0.5665\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 1.0544 Acc: 0.5966\n",
      "val Loss: 1.1612 Acc: 0.5687\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 1.0540 Acc: 0.5999\n",
      "val Loss: 1.1623 Acc: 0.5552\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 1.0485 Acc: 0.5992\n",
      "val Loss: 1.1604 Acc: 0.5651\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 1.0505 Acc: 0.5969\n",
      "val Loss: 1.1598 Acc: 0.5514\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 1.0566 Acc: 0.6030\n",
      "val Loss: 1.1581 Acc: 0.5549\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 1.0463 Acc: 0.6044\n",
      "val Loss: 1.1498 Acc: 0.5627\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 1.0545 Acc: 0.6017\n",
      "val Loss: 1.1527 Acc: 0.5595\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 1.0475 Acc: 0.6049\n",
      "val Loss: 1.1508 Acc: 0.5559\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0497 Acc: 0.6023\n",
      "val Loss: 1.1579 Acc: 0.5602\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 1.0490 Acc: 0.6029\n",
      "val Loss: 1.1509 Acc: 0.5655\n",
      "\n",
      "Training complete in 18m 56s\n",
      "Best val Acc: 0.573950\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=100, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJ6ex4Iu0yiy"
   },
   "source": [
    "Comparing without Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ua2ckaFcc-Je"
   },
   "outputs": [],
   "source": [
    "# Initialize the non-pretrained version of the model used for this run\n",
    "scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "scratch_model = scratch_model.to(device)\n",
    "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "scratch_criterion = nn.CrossEntropyLoss()\n",
    "_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "\n",
    "# Plot the training curves of validation accuracy vs. number\n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "ohist = []\n",
    "shist = []\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "shist = [h.cpu().numpy() for h in scratch_hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sr8JjsG-c-Jh"
   },
   "source": [
    "THE END! :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ozAHtAyc-Ji"
   },
   "outputs": [],
   "source": [
    "# Use dataloader to load the data\n",
    "\n",
    "# Read data from file\n",
    "\n",
    "# tansform the data into the desired shape\n",
    "\n",
    "\n",
    "# split on the train and test\n",
    "\n",
    "\n",
    "# convert into tensor\n",
    "\n",
    "\n",
    "\n",
    "# Data Loading\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WtCwg6zpc-Jk"
   },
   "outputs": [],
   "source": [
    "# Define a small CNN that processes 2-channel images\n",
    "# and output the view label of the image, the output should be one of five labels\n",
    "# kidney_right_sag, kidney_right_trav, kidney_left_sag, kidney_left_trav, bladder\n",
    "\n",
    "class Net(nn.Module):\n",
    "    # Initialize our layers, i.e. the set of trainable parameters\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # A convolutional layer\n",
    "        # The number of input channels is 2 greyscale\n",
    "        self.conv1 = nn.Conv2d(2, 6, 5)\n",
    "        # A max pooling layer (will be reused for each conv layer)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Another convolutional layer\n",
    "        self.conv2 = nn.Conv2d(4, 16, 5)\n",
    "        # Three sets of fully connected (linear) layers \n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # For each conv layer: conv -> relu -> pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Reshape from higher dimensional tensor to a vector for the FC layers\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        # Pass through fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eu6w4OMac-Jn"
   },
   "outputs": [],
   "source": [
    "# Look at the network structure\n",
    "print(model.state_dict().keys())\n",
    "print(\"Conv1:\",model.conv1.weight.size())\n",
    "print(\"Conv2:\",model.conv2.weight.size())\n",
    "print(\"fc1:\",model.fc1.weight.size())\n",
    "print(\"fc2:\",model.fc2.weight.size())\n",
    "print(\"fc3:\",model.fc3.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S1c-WZ_mc-Jp"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    num_total = 0.0\n",
    "    num_correct = 0.0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)              # Forward\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        optimizer.zero_grad()              # Zero the parameter gradients\n",
    "        loss.backward()                    # Backward\n",
    "        optimizer.step()                   # Gradient step\n",
    "\n",
    "        num_correct += (torch.argmax(outputs, dim=1) == labels).sum().float()\n",
    "        num_total += labels.size(0)\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 1000 == 0:    # print every 2000 mini-batches\n",
    "            print('[{}, {:5d}] loss: {:6.4f}, acc: {:6.4f}'.format(\n",
    "                   epoch + 1, i + 1, running_loss / 2000, num_correct / num_total))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xHyV6YOjc-Jt"
   },
   "outputs": [],
   "source": [
    "# Overall accuracy on the test set\n",
    "correct, total = 0, 0\n",
    "for images, labels in testloader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs = model(images)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "print('Accuracy on test set: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "image_label.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
