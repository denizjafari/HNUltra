{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZbZRovgc-In"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from torch.utils import data\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "import io\n",
    "import uuid\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import wandb\n",
    "wandb_username = 'andreasabo'\n",
    "local_username = 'andreasabo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2aVSXNyc-Iv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:1') \n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylfRtoN9c-Iy"
   },
   "outputs": [],
   "source": [
    "# root directory\n",
    "root_dir = \"/home/andreasabo/Documents/HNProject/\"\n",
    "split_file_base = \"/home/andreasabo/Documents/HNUltra/\"\n",
    "\n",
    "# data directory on current machine: abhishekmoturu, andreasabo, denizjafari, navidkorhani\n",
    "data_dir = \"/home/\" + local_username + \"/Documents/HNProject/all_label_img/\"\n",
    "\n",
    "# read target df\n",
    "csv_path = os.path.join(root_dir, \"all_splits_1000000.csv\")\n",
    "data_df = pd.read_csv(csv_path, usecols=['subj_id', 'image_ids', 'view_label', 'view_train'])\n",
    "\n",
    "# Are we doing the final test?\n",
    "test_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_OCR_7uy52w"
   },
   "source": [
    "### **Reading Data Indicies and Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nnwavxcqGBv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading splits from file\n"
     ]
    }
   ],
   "source": [
    "label_mapping = {'Other':0, 'Saggital_Right':1, 'Transverse_Right':2, \n",
    "                 'Saggital_Left':3, 'Transverse_Left':4, 'Bladder':5}\n",
    "label_unmapping = {0: 'Other', 1:'Saggital_Right', 2: 'Transverse_Right', \n",
    "                   3:'Saggital_Left', 4:'Transverse_Left', 5: 'Bladder'}\n",
    "\n",
    "data_df['view_label'] = data_df['view_label'].map(label_mapping)\n",
    "\n",
    "train_df = data_df[data_df.view_train == 1]\n",
    "test_df = data_df[data_df.view_train == 0]\n",
    "\n",
    "unique_subj = train_df.subj_id.unique()\n",
    "\n",
    "# Create the splits for 5-fold cross validation based on subj_id\n",
    "data_split_file = split_file_base + 'data_splits.json'\n",
    "if not os.path.isfile(data_split_file):\n",
    "\n",
    "    kf = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    fold = 0\n",
    "    all_folds = {}\n",
    "    for train_subj, val_subj in kf.split(unique_subj):\n",
    "        train_ids  = unique_subj[train_subj]\n",
    "        val_ids = unique_subj[val_subj]\n",
    "\n",
    "        train_images = train_df[train_df.subj_id.isin(train_ids)].image_ids.tolist()\n",
    "        val_images = train_df[train_df.subj_id.isin(val_ids)].image_ids.tolist()\n",
    "        train_labels = train_df[train_df.subj_id.isin(train_ids)].view_label.tolist()\n",
    "        val_labels = train_df[train_df.subj_id.isin(val_ids)].view_label.tolist()\n",
    "        cur_fold = {'train_ids': train_images, 'valid_ids': val_images, 'train_labels': train_labels, 'valid_labels': val_labels}\n",
    "        all_folds[fold] = cur_fold\n",
    "        fold += 1\n",
    "\n",
    "    print(\"Saving data splits\")\n",
    "    with open(data_split_file, 'w') as f:\n",
    "        json.dump(all_folds, f)\n",
    "        \n",
    "else: # just load from file\n",
    "    print(\"Reading splits from file\")\n",
    "    with open(data_split_file, 'r') as f:\n",
    "        all_folds = json.load(f)\n",
    "\n",
    "# If we're testing, overwrite the training data with the entire train/test data\n",
    "if test_data:\n",
    "    train_images = train_df.image_ids.tolist()\n",
    "    val_images = test_df.image_ids.tolist()\n",
    "    train_labels = train_df.view_label.tolist()\n",
    "    val_labels = test_df.view_label.tolist()\n",
    "\n",
    "    cur_fold = {'train_ids': train_images, 'valid_ids': val_images, 'train_labels': train_labels, 'valid_labels': val_labels}\n",
    "\n",
    "    \n",
    "    all_folds['test'] = cur_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BbgEWoqKc-JO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 256\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    elif model_name == 'viewnet':\n",
    "        conv1_filters = 8\n",
    "        conv2_filters = 16\n",
    "        conv3_filters = 32\n",
    "        linear1_size = 512\n",
    "        dropout = 0.25\n",
    "        model_ft = ViewNet(num_classes, conv1_filters, conv2_filters, conv3_filters, linear1_size, dropout)\n",
    "        input_size = 256\n",
    "        \n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "\n",
    "class ViewNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, conv1_filters, conv2_filters, conv3_filters, linear1_size, dropout):\n",
    "        super(ViewNet, self).__init__()\n",
    "        self.conv1_filters = conv1_filters\n",
    "        self.conv2_filters = conv2_filters\n",
    "        self.conv3_filters = conv3_filters\n",
    "        self.linear1_size = linear1_size\n",
    "        self.drop_percent = dropout\n",
    "        self.max_pool = 4\n",
    "        self.conv_output = int(self.conv3_filters*(256/(self.max_pool**3))*(256/(self.max_pool**3)))\n",
    "        print(\"conv_output: \", self.conv_output)\n",
    "\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, self.conv1_filters, 4, padding=2)\n",
    "        self.conv2 = nn.Conv2d(self.conv1_filters, self.conv2_filters, 4, padding=2)\n",
    "        self.conv3 = nn.Conv2d(self.conv2_filters, self.conv3_filters, 4, padding=2)\n",
    "        self.pool = nn.MaxPool2d(self.max_pool, self.max_pool)\n",
    "        self.dropout = nn.Dropout(self.drop_percent)\n",
    "        self.linear1 = nn.Linear(self.conv_output, self.linear1_size)\n",
    "        self.linear2 = nn.Linear(self.linear1_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.dropout(F.relu(self.conv1(x))))\n",
    "        x = self.pool(self.dropout(F.relu(self.conv2(x))))\n",
    "        x = self.pool(self.dropout(F.relu(self.conv3(x))))\n",
    "        x = x.view(-1, self.conv_output) \n",
    "        x = self.dropout(F.relu((self.linear1(x))))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from: https://gist.github.com/stefanonardo/693d96ceb2f531fa05db530f3e21517d\n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=True):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if np.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False, final_testing=False):\n",
    "    es = EarlyStopping(patience = 15)\n",
    "    stop_now = 0\n",
    "\n",
    "    since = time.time()\n",
    "    classnames = ['Other', 'Saggital_Right', 'Transverse_Right', 'Saggital_Left','Transverse_Left', 'Bladder']\n",
    "    val_acc_history = []\n",
    "    \n",
    "    val_metrics_list = []\n",
    "    train_metrics_list = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    epoch_with_best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 54)\n",
    "\n",
    "        if stop_now:\n",
    "            break\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            running_preds = []\n",
    "            running_labels = []\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                labels = labels.type(torch.long)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        labels = torch.argmax(labels, 1)\n",
    "                        running_preds += torch.argmax(outputs, 1).tolist()\n",
    "                        running_labels += labels.tolist()\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    preds = torch.argmax(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print('{} loss:\\t{:.4f} | {} acc:\\t{:.4f}\\n'.format(phase, epoch_loss, phase, epoch_acc))\n",
    "\n",
    "            if phase == 'train':\n",
    "                wandb.log({'epoch': epoch, 'train_acc':epoch_acc, 'train_loss':epoch_loss})\n",
    "                \n",
    "                cur_train_metrics = {}\n",
    "                                # compute and log f1, precision, recall for each class\n",
    "                for c in range(6):\n",
    "                    running_labels = np.asarray(running_labels)\n",
    "                    running_preds = np.asarray(running_preds)\n",
    "\n",
    "                    cur_c_labs_bin = np.asarray([0] *len(running_labels))\n",
    "                    cur_c_preds_bin = np.asarray([0] *len(running_labels))\n",
    "\n",
    "                    # Need to binarize\n",
    "                    cur_c_preds_bin[running_preds == c] = 1\n",
    "                    cur_c_labs_bin[running_labels == c] = 1\n",
    "                    f1 = f1_score(cur_c_labs_bin, cur_c_preds_bin)\n",
    "                    precision = precision_score(cur_c_labs_bin, cur_c_preds_bin)\n",
    "                    recall = recall_score(cur_c_labs_bin, cur_c_preds_bin)\n",
    "                    \n",
    "                    cur_train_metrics['train_' + label_unmapping[c] + '_f1'] = f1\n",
    "                    cur_train_metrics['train_' + label_unmapping[c] + '_precision'] = precision\n",
    "                    cur_train_metrics['train_' + label_unmapping[c] + '_recall'] = recall\n",
    "                    \n",
    "                \n",
    "                train_metrics_list.append(cur_train_metrics)\n",
    "                \n",
    "                average_types = ['macro', 'micro', 'weighted']\n",
    "                average_metrics_to_log = ['precision', 'recall', 'f1score', 'support']\n",
    "                average_dict = {'epoch': epoch}\n",
    "                for av in average_types:\n",
    "                    results_tuple = precision_recall_fscore_support(running_labels, running_preds, average=av)\n",
    "                    for m in range(len(average_metrics_to_log)):      \n",
    "                        average_dict[phase + '_'+ average_metrics_to_log[m] +'_average_' + av] = results_tuple[m]\n",
    "                        cur_train_metrics[phase + '_'+ average_metrics_to_log[m] +'_average_' + av] = results_tuple[m]\n",
    "                cur_train_metrics[phase + '_acc_average'] = accuracy_score(running_labels, running_preds)                  \n",
    "                average_dict[phase + '_acc_average'] = accuracy_score(running_labels, running_preds)     \n",
    "                wandb.log(cur_train_metrics)\n",
    "                \n",
    "            if phase == 'val':\n",
    "                wandb.log({'valid_loss':epoch_loss, 'valid_acc':epoch_acc, 'epoch': epoch})\n",
    "               \n",
    "            \n",
    "                cur_val_metrics = {}\n",
    "                # compute and log f1, precision, recall for each class\n",
    "                for c in range(6):\n",
    "                    running_labels = np.asarray(running_labels)\n",
    "                    running_preds = np.asarray(running_preds)\n",
    "\n",
    "                    cur_c_labs_bin = np.asarray([0] *len(running_labels))\n",
    "                    cur_c_preds_bin = np.asarray([0] *len(running_labels))\n",
    "\n",
    "                    # Need to binarize\n",
    "                    cur_c_preds_bin[running_preds == c] = 1\n",
    "                    cur_c_labs_bin[running_labels == c] = 1\n",
    "                    f1 = f1_score(cur_c_labs_bin, cur_c_preds_bin)\n",
    "                    precision = precision_score(cur_c_labs_bin, cur_c_preds_bin)\n",
    "                    recall = recall_score(cur_c_labs_bin, cur_c_preds_bin)\n",
    "                    wandb.log({'valid_' + label_unmapping[c] + '_f1': f1})\n",
    "                    wandb.log({'valid_' + label_unmapping[c] + '_precision': precision})\n",
    "                    wandb.log({'valid_' + label_unmapping[c] + '_recall': recall})\n",
    "                \n",
    "                    cur_val_metrics['val_' + label_unmapping[c] + '_f1'] = f1\n",
    "                    cur_val_metrics['val_' + label_unmapping[c] + '_precision'] = precision\n",
    "                    cur_val_metrics['val_' + label_unmapping[c] + '_recall'] = recall\n",
    "                \n",
    "                average_types = ['macro', 'micro', 'weighted']\n",
    "                average_metrics_to_log = ['precision', 'recall', 'f1score']\n",
    "                average_dict = {'epoch': epoch}\n",
    "                for av in average_types:\n",
    "                    results_tuple = precision_recall_fscore_support(running_labels, running_preds, average=av)\n",
    "                    for m in range(len(average_metrics_to_log)):      \n",
    "                        average_dict[phase + '_'+ average_metrics_to_log[m] +'_average_' + av] = results_tuple[m]\n",
    "                        cur_val_metrics[phase + '_'+ average_metrics_to_log[m] +'_average_' + av] = results_tuple[m]\n",
    "                cur_val_metrics[phase + '_acc_average'] = accuracy_score(running_labels, running_preds)                  \n",
    "                average_dict[phase + '_acc_average'] = accuracy_score(running_labels, running_preds)     \n",
    "                print(cur_val_metrics)\n",
    "                wandb.log(cur_val_metrics)\n",
    "                \n",
    "                \n",
    "                val_metrics_list.append(cur_val_metrics)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                print(classification_report(running_labels, running_preds))\n",
    "                train_acc = epoch_acc\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_acc_train = train_acc\n",
    "                epoch_with_best_val_acc = epoch\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), os.path.join(wandb.run.dir, \"model.pt\"))\n",
    "                print(classification_report(running_labels, running_preds))\n",
    "\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                if es.step(epoch_loss) and not final_testing:\n",
    "                    stop_now = 1\n",
    "                    print(\"EARLY STOPPING \" + str(epoch))\n",
    "                    break\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val acc: {:4f}\\n'.format(best_acc))\n",
    "    \n",
    "    # Directly save the best results in this fold\n",
    "    wandb.config.best_acc = best_acc\n",
    "    wandb.config.best_epoch = epoch_with_best_val_acc\n",
    "\n",
    "    wandb.config.val_acc_history = val_acc_history\n",
    "    wandb.config.best_epoch = epoch_with_best_val_acc\n",
    "    \n",
    "    wandb.config.update(val_metrics_list[epoch_with_best_val_acc])\n",
    "    wandb.config.update(train_metrics_list[epoch_with_best_val_acc])\n",
    "    \n",
    "    metrics_from_best_epoch = {'best_epoch': epoch_with_best_val_acc, 'last_epoch': epoch}\n",
    "    metrics_from_best_epoch.update( val_metrics_list[epoch_with_best_val_acc] )\n",
    "    metrics_from_best_epoch.update( train_metrics_list[epoch_with_best_val_acc] )\n",
    "    metrics_from_best_epoch.update( {'val_acc': best_acc.data.cpu(), 'train_acc': best_acc_train.data.cpu()} )    \n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, metrics_from_best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iikM7_G3c-JR"
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, list_IDs, labels, transformations=None):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.transformations = transformations\n",
    "        \n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        img_path = data_dir + ID + '.jpg'\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        \n",
    "        if self.transformations:\n",
    "            image = self.transformations(image)\n",
    "        \n",
    "        image = ToTensor()(image)\n",
    "        \n",
    "        y = torch.FloatTensor([0]*6)        \n",
    "        y[int(self.labels[index])] = 1\n",
    "\n",
    "        return image, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception, viewnet]\n",
    "model_name = \"viewnet\"\n",
    "\n",
    "# Number of classes in the dataset: right_sag, right_trav, left_sag, left_trav, bladder, other\n",
    "num_classes = 6\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 100\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 100\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model; when True we only update the reshaped layer params\n",
    "feature_extract = False\n",
    "\n",
    "# Flag for whether or not to use pretrained model\n",
    "pretrain = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DdSxc4Dhc-JT"
   },
   "outputs": [],
   "source": [
    "def train5fold(network_configs, model_ft, lr, wd, amsgrad, i):\n",
    "    folds = ['test']\n",
    "    folds = ['0', '1', '2', '3', '4']\n",
    "\n",
    "    random_str = str(uuid.uuid4()).split(\"-\")[0]\n",
    "    best_metrics_per_fold = []\n",
    "    model_base = copy.deepcopy(model_ft)\n",
    "    for fold in folds:\n",
    "\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%d-%m-%Y.%H:%M:%S\")\n",
    "        wandb.init(project='hnultra_test_apr4', entity=wandb_username, name=local_username + '_fold_' + fold, group=random_str)\n",
    "        partition = all_folds[fold]\n",
    "\n",
    "        model_ft = copy.deepcopy(model_base)\n",
    "        model_ft = model_ft.to(device)\n",
    "        wandb.watch(model_ft)\n",
    "\n",
    "        # Gather the parameters to be optimized/updated in this run. If we are\n",
    "        #  finetuning we will be updating all parameters. However, if we are\n",
    "        #  doing feature extract method, we will only update the parameters\n",
    "        #  that we have just initialized, i.e. the parameters with requires_grad\n",
    "        #  is True.\n",
    "        params_to_update = model_ft.parameters()\n",
    "        #print(\"Params to learn:\")\n",
    "        if feature_extract:\n",
    "            params_to_update = []\n",
    "            for name,param in model_ft.named_parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    params_to_update.append(param)\n",
    "                    print(\"\\t\",name)\n",
    "        else:\n",
    "            for name,param in model_ft.named_parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    print(\"\\t\",name)\n",
    "\n",
    "        # Observe that all parameters are being optimized\n",
    "        optimizer_ft = optim.Adam(params_to_update, lr=lr, weight_decay=wd, amsgrad=amsgrad)\n",
    "\n",
    "        # Setup the loss fxn\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        shuffle = True\n",
    "        num_workers = 0\n",
    "        params = {'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                  'num_workers': num_workers}\n",
    "\n",
    "        config_dict = {'i': i, 'batch_size': batch_size, 'shuffle': shuffle, 'num_workers': num_workers, 'fold': fold,\n",
    "                       'lr': lr, 'wd': wd, 'amsgrad': amsgrad, 'model_name': model_name, 'num_classes': num_classes, \n",
    "                       'num_epochs': num_epochs, 'feature_extract': feature_extract, \"pretrain\": pretrain }\n",
    "\n",
    "        wandb.config.update(config_dict)\n",
    "        wandb.config.update(network_configs)\n",
    "        # Tranforms\n",
    "        trans = transforms.Compose([transforms.RandomAffine(degrees=8, translate=(0.1, 0.1), scale=(0.95,1.25))])\n",
    "\n",
    "        # Generators\n",
    "        training_set = Dataset(partition['train_ids'], partition['train_labels'], transformations=trans)\n",
    "        training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "        validation_set = Dataset(partition['valid_ids'], partition['valid_labels'])\n",
    "        validation_generator = data.DataLoader(validation_set, **params)\n",
    "\n",
    "        dataloaders_dict = {'train':training_generator, 'val':validation_generator}\n",
    "\n",
    "        # Train & Evaluate\n",
    "        model_ft, hist, metrics_from_best_epoch = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs, is_inception=(model_name==\"inception\"))\n",
    "        best_metrics_per_fold.append(metrics_from_best_epoch)\n",
    "\n",
    "    # Calculate the performance metrics on the best model in each fold\n",
    "    wandb.init(project='hnultra', entity=wandb_username, name=local_username + '_ALL', group=random_str)\n",
    "    config_dict['fold'] = -1\n",
    "    wandb.config.update(config_dict)\n",
    "    wandb.config.update(network_configs)\n",
    "\n",
    "\n",
    "    metrics_all = {}\n",
    "    for fold in best_metrics_per_fold:\n",
    "        for key in fold:\n",
    "            if key not in metrics_all:\n",
    "                metrics_all[key] = [fold[key]]\n",
    "            else:\n",
    "                metrics_all[key].append(fold[key]) \n",
    "    # print(metrics_all)\n",
    "\n",
    "    metrics_to_log = {}\n",
    "    for m in metrics_all:\n",
    "        metric_list = np.asarray(metrics_all[m])\n",
    "#         print(m)\n",
    "#         print(metric_list)\n",
    "#         print(type(metric_list))\n",
    "        metrics_to_log[m + '_mean'] = metric_list.mean()    \n",
    "        metrics_to_log[m + '_stdev'] = metric_list.std()\n",
    "\n",
    "    wandb.config.update(metrics_to_log)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "07uuFRNNc-JW"
   },
   "outputs": [],
   "source": [
    "def SetupAndRunTest(model_ft, lr, wd, amsgrad, run_configs):\n",
    "    \n",
    "    input_size = 256\n",
    "\n",
    "    # Print the model we just instantiated\n",
    "    print(model_ft)\n",
    "\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    # Gather the parameters to be optimized/updated in this run. If we are\n",
    "    #  finetuning we will be updating all parameters. However, if we are\n",
    "    #  doing feature extract method, we will only update the parameters\n",
    "    #  that we have just initialized, i.e. the parameters with requires_grad\n",
    "    #  is True.\n",
    "    params_to_update = model_ft.parameters()\n",
    "#     print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\",name)\n",
    "    else:\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(\"\\t\",name)\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.Adam(params_to_update, lr=lr, weight_decay=wd, amsgrad=amsgrad)\n",
    "\n",
    "    # Setup the loss fxn\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_output:  512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_test_apr4\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_test_apr4</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/andreasabo/hnultra_test_apr4/runs/dhld7jo3\" target=\"_blank\">https://app.wandb.ai/andreasabo/hnultra_test_apr4/runs/dhld7jo3</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.31 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t conv1.weight\n",
      "\t conv1.bias\n",
      "\t conv2.weight\n",
      "\t conv2.bias\n",
      "\t conv3.weight\n",
      "\t conv3.bias\n",
      "\t linear1.weight\n",
      "\t linear1.bias\n",
      "\t linear2.weight\n",
      "\t linear2.bias\n",
      "Epoch 1/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.5758 | train acc:\t0.3777\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.87      0.53      3724\n",
      "           1       0.28      0.08      0.12      1813\n",
      "           2       0.50      0.00      0.00      1218\n",
      "           3       0.35      0.23      0.28      1895\n",
      "           4       0.50      0.00      0.00      1359\n",
      "           5       0.53      0.33      0.41      1072\n",
      "\n",
      "    accuracy                           0.38     11081\n",
      "   macro avg       0.42      0.25      0.22     11081\n",
      "weighted avg       0.40      0.38      0.28     11081\n",
      "\n",
      "val loss:\t1.5747 | val acc:\t0.3876\n",
      "\n",
      "{'val_Other_f1': 0.5369928400954653, 'val_Other_precision': 0.5212355212355212, 'val_Other_recall': 0.5537325676784249, 'val_Saggital_Right_f1': 0.1222879684418146, 'val_Saggital_Right_precision': 0.3263157894736842, 'val_Saggital_Right_recall': 0.07524271844660194, 'val_Transverse_Right_f1': 0.006211180124223603, 'val_Transverse_Right_precision': 1.0, 'val_Transverse_Right_recall': 0.003115264797507788, 'val_Saggital_Left_f1': 0.33333333333333337, 'val_Saggital_Left_precision': 0.21882542236524538, 'val_Saggital_Left_recall': 0.699228791773779, 'val_Transverse_Left_f1': 0.0, 'val_Transverse_Left_precision': 0.0, 'val_Transverse_Left_recall': 0.0, 'val_Bladder_f1': 0.579957356076759, 'val_Bladder_precision': 0.5596707818930041, 'val_Bladder_recall': 0.6017699115044248, 'val_precision_average_macro': 0.4376745858279092, 'val_recall_average_macro': 0.32218154236678975, 'val_f1score_average_macro': 0.263130446345266, 'val_precision_average_micro': 0.3875564824469934, 'val_recall_average_micro': 0.3875564824469934, 'val_f1score_average_micro': 0.3875564824469934, 'val_precision_average_weighted': 0.45270660120165374, 'val_recall_average_weighted': 0.3875564824469934, 'val_f1score_average_weighted': 0.3363600045305144, 'val_acc_average': 0.3875564824469934}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.55      0.54      1219\n",
      "           1       0.33      0.08      0.12       412\n",
      "           2       1.00      0.00      0.01       321\n",
      "           3       0.22      0.70      0.33       389\n",
      "           4       0.00      0.00      0.00       310\n",
      "           5       0.56      0.60      0.58       226\n",
      "\n",
      "    accuracy                           0.39      2877\n",
      "   macro avg       0.44      0.32      0.26      2877\n",
      "weighted avg       0.45      0.39      0.34      2877\n",
      "\n",
      "Epoch 2/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3953 | train acc:\t0.4493\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.78      0.58      3724\n",
      "           1       0.36      0.23      0.28      1813\n",
      "           2       0.50      0.01      0.02      1218\n",
      "           3       0.38      0.44      0.41      1895\n",
      "           4       0.42      0.07      0.11      1359\n",
      "           5       0.62      0.67      0.64      1072\n",
      "\n",
      "    accuracy                           0.45     11081\n",
      "   macro avg       0.46      0.37      0.34     11081\n",
      "weighted avg       0.45      0.45      0.39     11081\n",
      "\n",
      "val loss:\t1.5206 | val acc:\t0.4296\n",
      "\n",
      "{'val_Other_f1': 0.5463414634146342, 'val_Other_precision': 0.5414987912973409, 'val_Other_recall': 0.5512715340442986, 'val_Saggital_Right_f1': 0.42158859470468435, 'val_Saggital_Right_precision': 0.3631578947368421, 'val_Saggital_Right_recall': 0.5024271844660194, 'val_Transverse_Right_f1': 0.05325443786982249, 'val_Transverse_Right_precision': 0.5294117647058824, 'val_Transverse_Right_recall': 0.028037383177570093, 'val_Saggital_Left_f1': 0.34464751958224543, 'val_Saggital_Left_precision': 0.26052631578947366, 'val_Saggital_Left_recall': 0.5089974293059126, 'val_Transverse_Left_f1': 0.12705882352941175, 'val_Transverse_Left_precision': 0.23478260869565218, 'val_Transverse_Left_recall': 0.08709677419354839, 'val_Bladder_f1': 0.615, 'val_Bladder_precision': 0.7068965517241379, 'val_Bladder_recall': 0.5442477876106194, 'val_precision_average_macro': 0.43937898782488816, 'val_recall_average_macro': 0.37034634879966144, 'val_f1score_average_macro': 0.351315139850133, 'val_precision_average_micro': 0.42961418143899893, 'val_recall_average_micro': 0.42961418143899893, 'val_f1score_average_micro': 0.42961418143899893, 'val_precision_average_weighted': 0.45656420643762197, 'val_recall_average_weighted': 0.42961418143899893, 'val_f1score_average_weighted': 0.406404428185121, 'val_acc_average': 0.42961418143899893}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.55      0.55      1219\n",
      "           1       0.36      0.50      0.42       412\n",
      "           2       0.53      0.03      0.05       321\n",
      "           3       0.26      0.51      0.34       389\n",
      "           4       0.23      0.09      0.13       310\n",
      "           5       0.71      0.54      0.61       226\n",
      "\n",
      "    accuracy                           0.43      2877\n",
      "   macro avg       0.44      0.37      0.35      2877\n",
      "weighted avg       0.46      0.43      0.41      2877\n",
      "\n",
      "Epoch 3/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3261 | train acc:\t0.4810\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.76      0.60      3724\n",
      "           1       0.42      0.33      0.37      1813\n",
      "           2       0.44      0.06      0.11      1218\n",
      "           3       0.41      0.46      0.44      1895\n",
      "           4       0.38      0.15      0.22      1359\n",
      "           5       0.65      0.70      0.68      1072\n",
      "\n",
      "    accuracy                           0.48     11081\n",
      "   macro avg       0.47      0.41      0.40     11081\n",
      "weighted avg       0.46      0.48      0.44     11081\n",
      "\n",
      "val loss:\t1.4580 | val acc:\t0.4710\n",
      "\n",
      "{'val_Other_f1': 0.5589723627870766, 'val_Other_precision': 0.5318518518518518, 'val_Other_recall': 0.5890073831009024, 'val_Saggital_Right_f1': 0.503901895206243, 'val_Saggital_Right_precision': 0.465979381443299, 'val_Saggital_Right_recall': 0.5485436893203883, 'val_Transverse_Right_f1': 0.07580174927113702, 'val_Transverse_Right_precision': 0.5909090909090909, 'val_Transverse_Right_recall': 0.040498442367601244, 'val_Saggital_Left_f1': 0.38636363636363635, 'val_Saggital_Left_precision': 0.3229706390328152, 'val_Saggital_Left_recall': 0.480719794344473, 'val_Transverse_Left_f1': 0.22317596566523604, 'val_Transverse_Left_precision': 0.3333333333333333, 'val_Transverse_Left_recall': 0.16774193548387098, 'val_Bladder_f1': 0.6223091976516634, 'val_Bladder_precision': 0.5578947368421052, 'val_Bladder_recall': 0.7035398230088495, 'val_precision_average_macro': 0.46715650556874927, 'val_recall_average_macro': 0.4216751779376809, 'val_f1score_average_macro': 0.3950874678241654, 'val_precision_average_micro': 0.47097671185262424, 'val_recall_average_micro': 0.47097671185262424, 'val_f1score_average_micro': 0.47097671185262424, 'val_precision_average_weighted': 0.48142017837583556, 'val_recall_average_weighted': 0.47097671185262424, 'val_f1score_average_weighted': 0.4426309124606907, 'val_acc_average': 0.47097671185262424}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.59      0.56      1219\n",
      "           1       0.47      0.55      0.50       412\n",
      "           2       0.59      0.04      0.08       321\n",
      "           3       0.32      0.48      0.39       389\n",
      "           4       0.33      0.17      0.22       310\n",
      "           5       0.56      0.70      0.62       226\n",
      "\n",
      "    accuracy                           0.47      2877\n",
      "   macro avg       0.47      0.42      0.40      2877\n",
      "weighted avg       0.48      0.47      0.44      2877\n",
      "\n",
      "Epoch 4/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2832 | train acc:\t0.5074\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.75      0.61      3724\n",
      "           1       0.48      0.36      0.41      1813\n",
      "           2       0.51      0.11      0.18      1218\n",
      "           3       0.43      0.49      0.46      1895\n",
      "           4       0.43      0.24      0.31      1359\n",
      "           5       0.68      0.72      0.70      1072\n",
      "\n",
      "    accuracy                           0.51     11081\n",
      "   macro avg       0.51      0.45      0.45     11081\n",
      "weighted avg       0.50      0.51      0.48     11081\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.4461 | val acc:\t0.4515\n",
      "\n",
      "{'val_Other_f1': 0.5406593406593407, 'val_Other_precision': 0.5823863636363636, 'val_Other_recall': 0.5045118949958983, 'val_Saggital_Right_f1': 0.45881447267128556, 'val_Saggital_Right_precision': 0.3359639233370913, 'val_Saggital_Right_recall': 0.7233009708737864, 'val_Transverse_Right_f1': 0.2978723404255319, 'val_Transverse_Right_precision': 0.4697986577181208, 'val_Transverse_Right_recall': 0.21806853582554517, 'val_Saggital_Left_f1': 0.30098452883263016, 'val_Saggital_Left_precision': 0.33229813664596275, 'val_Saggital_Left_recall': 0.2750642673521851, 'val_Transverse_Left_f1': 0.2517241379310345, 'val_Transverse_Left_precision': 0.27037037037037037, 'val_Transverse_Left_recall': 0.23548387096774193, 'val_Bladder_f1': 0.6491646778042961, 'val_Bladder_precision': 0.7046632124352331, 'val_Bladder_recall': 0.6017699115044248, 'val_precision_average_macro': 0.4492467773571904, 'val_recall_average_macro': 0.42636657525326366, 'val_f1score_average_macro': 0.4165365830540198, 'val_precision_average_micro': 0.45151199165797706, 'val_recall_average_micro': 0.45151199165797706, 'val_f1score_average_micro': 0.45151199165797706, 'val_precision_average_weighted': 0.4767063464704841, 'val_recall_average_weighted': 0.45151199165797706, 'val_f1score_average_weighted': 0.4468338553837979, 'val_acc_average': 0.45151199165797706}\n",
      "Epoch 5/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2395 | train acc:\t0.5245\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.76      0.63      3724\n",
      "           1       0.49      0.39      0.43      1813\n",
      "           2       0.46      0.17      0.25      1218\n",
      "           3       0.45      0.48      0.46      1895\n",
      "           4       0.43      0.28      0.34      1359\n",
      "           5       0.71      0.74      0.72      1072\n",
      "\n",
      "    accuracy                           0.52     11081\n",
      "   macro avg       0.51      0.47      0.47     11081\n",
      "weighted avg       0.51      0.52      0.50     11081\n",
      "\n",
      "val loss:\t1.4349 | val acc:\t0.4609\n",
      "\n",
      "{'val_Other_f1': 0.5320686540198736, 'val_Other_precision': 0.5919597989949749, 'val_Other_recall': 0.4831829368334701, 'val_Saggital_Right_f1': 0.4970530451866405, 'val_Saggital_Right_precision': 0.4174917491749175, 'val_Saggital_Right_recall': 0.6140776699029126, 'val_Transverse_Right_f1': 0.27906976744186046, 'val_Transverse_Right_precision': 0.5504587155963303, 'val_Transverse_Right_recall': 0.18691588785046728, 'val_Saggital_Left_f1': 0.39184397163120566, 'val_Saggital_Left_precision': 0.29905277401894453, 'val_Saggital_Left_recall': 0.5681233933161953, 'val_Transverse_Left_f1': 0.25132743362831855, 'val_Transverse_Left_precision': 0.2784313725490196, 'val_Transverse_Left_recall': 0.22903225806451613, 'val_Bladder_f1': 0.6616541353383459, 'val_Bladder_precision': 0.7630057803468208, 'val_Bladder_recall': 0.584070796460177, 'val_precision_average_macro': 0.48340003178016794, 'val_recall_average_macro': 0.4442338237379564, 'val_f1score_average_macro': 0.4355028345410408, 'val_precision_average_micro': 0.4608967674661105, 'val_recall_average_micro': 0.4608967674661105, 'val_f1score_average_micro': 0.4608967674661105, 'val_precision_average_weighted': 0.5023939535221792, 'val_recall_average_weighted': 0.4608967674661105, 'val_f1score_average_weighted': 0.4597954755619544, 'val_acc_average': 0.4608967674661105}\n",
      "Epoch 6/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2162 | train acc:\t0.5324\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.75      0.63      3724\n",
      "           1       0.51      0.40      0.45      1813\n",
      "           2       0.49      0.18      0.26      1218\n",
      "           3       0.46      0.50      0.48      1895\n",
      "           4       0.45      0.30      0.36      1359\n",
      "           5       0.72      0.75      0.74      1072\n",
      "\n",
      "    accuracy                           0.53     11081\n",
      "   macro avg       0.53      0.48      0.49     11081\n",
      "weighted avg       0.52      0.53      0.51     11081\n",
      "\n",
      "val loss:\t1.3977 | val acc:\t0.4918\n",
      "\n",
      "{'val_Other_f1': 0.5274127775260533, 'val_Other_precision': 0.5890688259109311, 'val_Other_recall': 0.4774405250205086, 'val_Saggital_Right_f1': 0.5589403973509933, 'val_Saggital_Right_precision': 0.6151603498542274, 'val_Saggital_Right_recall': 0.5121359223300971, 'val_Transverse_Right_f1': 0.3928571428571428, 'val_Transverse_Right_precision': 0.4602510460251046, 'val_Transverse_Right_recall': 0.3426791277258567, 'val_Saggital_Left_f1': 0.4560075685903501, 'val_Saggital_Left_precision': 0.36077844311377244, 'val_Saggital_Left_recall': 0.6195372750642674, 'val_Transverse_Left_f1': 0.335180055401662, 'val_Transverse_Left_precision': 0.2936893203883495, 'val_Transverse_Left_recall': 0.3903225806451613, 'val_Bladder_f1': 0.662251655629139, 'val_Bladder_precision': 0.6607929515418502, 'val_Bladder_recall': 0.6637168141592921, 'val_precision_average_macro': 0.4966234894723726, 'val_recall_average_macro': 0.5009720408241972, 'val_f1score_average_macro': 0.48877493289255675, 'val_precision_average_micro': 0.491831769204032, 'val_recall_average_micro': 0.491831769204032, 'val_f1score_average_micro': 0.491831769204032, 'val_precision_average_weighted': 0.5213720053665377, 'val_recall_average_weighted': 0.491831769204032, 'val_f1score_average_weighted': 0.4971388244346048, 'val_acc_average': 0.491831769204032}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.48      0.53      1219\n",
      "           1       0.62      0.51      0.56       412\n",
      "           2       0.46      0.34      0.39       321\n",
      "           3       0.36      0.62      0.46       389\n",
      "           4       0.29      0.39      0.34       310\n",
      "           5       0.66      0.66      0.66       226\n",
      "\n",
      "    accuracy                           0.49      2877\n",
      "   macro avg       0.50      0.50      0.49      2877\n",
      "weighted avg       0.52      0.49      0.50      2877\n",
      "\n",
      "Epoch 7/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1919 | train acc:\t0.5408\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3724\n",
      "           1       0.53      0.42      0.47      1813\n",
      "           2       0.49      0.22      0.30      1218\n",
      "           3       0.47      0.51      0.49      1895\n",
      "           4       0.46      0.33      0.39      1359\n",
      "           5       0.73      0.74      0.74      1072\n",
      "\n",
      "    accuracy                           0.54     11081\n",
      "   macro avg       0.54      0.49      0.50     11081\n",
      "weighted avg       0.53      0.54      0.52     11081\n",
      "\n",
      "val loss:\t1.4064 | val acc:\t0.4818\n",
      "\n",
      "{'val_Other_f1': 0.4935323383084578, 'val_Other_precision': 0.6270543615676359, 'val_Other_recall': 0.40689089417555374, 'val_Saggital_Right_f1': 0.5496598639455782, 'val_Saggital_Right_precision': 0.6253869969040248, 'val_Saggital_Right_recall': 0.49029126213592233, 'val_Transverse_Right_f1': 0.41851851851851846, 'val_Transverse_Right_precision': 0.5159817351598174, 'val_Transverse_Right_recall': 0.35202492211838005, 'val_Saggital_Left_f1': 0.46543330087633883, 'val_Saggital_Left_precision': 0.37460815047021945, 'val_Saggital_Left_recall': 0.6143958868894601, 'val_Transverse_Left_f1': 0.35306553911205074, 'val_Transverse_Left_precision': 0.26257861635220126, 'val_Transverse_Left_recall': 0.5387096774193548, 'val_Bladder_f1': 0.6814516129032259, 'val_Bladder_precision': 0.6259259259259259, 'val_Bladder_recall': 0.7477876106194691, 'val_precision_average_macro': 0.5052559643966374, 'val_recall_average_macro': 0.5250167088930233, 'val_f1score_average_macro': 0.4936101956106951, 'val_precision_average_micro': 0.48175182481751827, 'val_recall_average_micro': 0.48175182481751827, 'val_f1score_average_micro': 0.48175182481751827, 'val_precision_average_weighted': 0.5409280665008914, 'val_recall_average_weighted': 0.48175182481751827, 'val_f1score_average_weighted': 0.4890275163259622, 'val_acc_average': 0.48175182481751827}\n",
      "Epoch 8/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1700 | train acc:\t0.5505\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      3724\n",
      "           1       0.56      0.42      0.48      1813\n",
      "           2       0.48      0.21      0.30      1218\n",
      "           3       0.47      0.50      0.48      1895\n",
      "           4       0.47      0.37      0.42      1359\n",
      "           5       0.74      0.76      0.75      1072\n",
      "\n",
      "    accuracy                           0.55     11081\n",
      "   macro avg       0.55      0.50      0.51     11081\n",
      "weighted avg       0.54      0.55      0.53     11081\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.4037 | val acc:\t0.4703\n",
      "\n",
      "{'val_Other_f1': 0.4686064318529862, 'val_Other_precision': 0.6202702702702703, 'val_Other_recall': 0.37653814602132896, 'val_Saggital_Right_f1': 0.5112359550561798, 'val_Saggital_Right_precision': 0.6066666666666667, 'val_Saggital_Right_recall': 0.441747572815534, 'val_Transverse_Right_f1': 0.41923076923076924, 'val_Transverse_Right_precision': 0.5477386934673367, 'val_Transverse_Right_recall': 0.3395638629283489, 'val_Saggital_Left_f1': 0.44698703279938984, 'val_Saggital_Left_precision': 0.31778741865509763, 'val_Saggital_Left_recall': 0.7532133676092545, 'val_Transverse_Left_f1': 0.36906377204884666, 'val_Transverse_Left_precision': 0.3185011709601874, 'val_Transverse_Left_recall': 0.43870967741935485, 'val_Bladder_f1': 0.6757281553398058, 'val_Bladder_precision': 0.6020761245674741, 'val_Bladder_recall': 0.7699115044247787, 'val_precision_average_macro': 0.5021733907645054, 'val_recall_average_macro': 0.5199473552031, 'val_f1score_average_macro': 0.4818086860546629, 'val_precision_average_micro': 0.470281543274244, 'val_recall_average_micro': 0.470281543274244, 'val_f1score_average_micro': 0.470281543274244, 'val_precision_average_weighted': 0.5353855125950231, 'val_recall_average_weighted': 0.470281543274244, 'val_f1score_average_weighted': 0.47182336428081834, 'val_acc_average': 0.470281543274244}\n",
      "Epoch 9/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1416 | train acc:\t0.5636\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      3724\n",
      "           1       0.57      0.44      0.50      1813\n",
      "           2       0.53      0.29      0.37      1218\n",
      "           3       0.50      0.52      0.51      1895\n",
      "           4       0.48      0.39      0.43      1359\n",
      "           5       0.75      0.77      0.76      1072\n",
      "\n",
      "    accuracy                           0.56     11081\n",
      "   macro avg       0.57      0.52      0.54     11081\n",
      "weighted avg       0.56      0.56      0.55     11081\n",
      "\n",
      "val loss:\t1.3859 | val acc:\t0.4783\n",
      "\n",
      "{'val_Other_f1': 0.516250588789449, 'val_Other_precision': 0.6061946902654868, 'val_Other_recall': 0.4495488105004102, 'val_Saggital_Right_f1': 0.4860426929392446, 'val_Saggital_Right_precision': 0.751269035532995, 'val_Saggital_Right_recall': 0.3592233009708738, 'val_Transverse_Right_f1': 0.44563279857397503, 'val_Transverse_Right_precision': 0.5208333333333334, 'val_Transverse_Right_recall': 0.3894080996884735, 'val_Saggital_Left_f1': 0.42369337979094074, 'val_Saggital_Left_precision': 0.29063097514340347, 'val_Saggital_Left_recall': 0.781491002570694, 'val_Transverse_Left_f1': 0.3552631578947369, 'val_Transverse_Left_precision': 0.3624161073825503, 'val_Transverse_Left_recall': 0.34838709677419355, 'val_Bladder_f1': 0.6842105263157895, 'val_Bladder_precision': 0.7447916666666666, 'val_Bladder_recall': 0.6327433628318584, 'val_precision_average_macro': 0.5460226347207393, 'val_recall_average_macro': 0.4934669455560839, 'val_f1score_average_macro': 0.4851821907173559, 'val_precision_average_micro': 0.478275981925617, 'val_recall_average_micro': 0.478275981925617, 'val_f1score_average_micro': 0.478275981925617, 'val_precision_average_weighted': 0.5593983418002307, 'val_recall_average_weighted': 0.478275981925617, 'val_f1score_average_weighted': 0.48737819541222305, 'val_acc_average': 0.478275981925617}\n",
      "Epoch 10/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1479 | train acc:\t0.5604\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3724\n",
      "           1       0.58      0.44      0.50      1813\n",
      "           2       0.52      0.27      0.35      1218\n",
      "           3       0.49      0.53      0.51      1895\n",
      "           4       0.47      0.38      0.42      1359\n",
      "           5       0.76      0.77      0.76      1072\n",
      "\n",
      "    accuracy                           0.56     11081\n",
      "   macro avg       0.56      0.52      0.53     11081\n",
      "weighted avg       0.56      0.56      0.55     11081\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-06d6654dd9c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                   'conv3_filters': conv3_filters, 'linear1_size': linear1_size }\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain5fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_configs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# model_ft = ViewNet(num_classes, conv1_filters, conv2_filters, conv3_filters, linear1_size, dropout)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-0b58c564014a>\u001b[0m in \u001b[0;36mtrain5fold\u001b[0;34m(network_configs, model_ft, lr, wd, amsgrad, i)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Train & Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_from_best_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"inception\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mbest_metrics_per_fold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_from_best_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-bf1ddefbf274>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs, is_inception, final_testing)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-ad5f21858ff9>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \"\"\"\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;31m# PIL image mode: L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YCbCr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _init_jupyter.<locals>.cleanup at 0x7f15664c4598> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# shutdown async logger because _user_process_finished isn't called in jupyter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mshutdown_async_log_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post_run_cell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36m_stop_jupyter_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jupyter_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmirror_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cloud\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stopping streaming files and file change observer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_file_syncing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36m_end_file_syncing\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;31m# TODO: there was a case where _file_event_handlers was getting modified in the loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_event_handlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m             \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_pusher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tailer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tailer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tailer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/hn/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "repetitions = 1\n",
    "\n",
    "conv1_filters = 8\n",
    "conv2_filters = 16\n",
    "conv3_filters = 32\n",
    "linear1_size = 512\n",
    "\n",
    "dropout = 0.25\n",
    "lr = 0.0005\n",
    "wd = 0.001\n",
    "amsgrad = False\n",
    "for i in range(repetitions):\n",
    "    config_string = f\"{conv1_filters}_{conv2_filters}_{conv3_filters}_{linear1_size}_{dropout}_{lr}_{wd}_{amsgrad}\"\n",
    "    model_ft = ViewNet(num_classes, conv1_filters, conv2_filters, conv3_filters, linear1_size, dropout)\n",
    "    run_configs = {'lr': lr, 'wd': wd, 'amsgrad': amsgrad,'dropout': dropout, \n",
    "                  'conv1_filters': conv1_filters, 'conv2_filters': conv2_filters, \n",
    "                  'conv3_filters': conv3_filters, 'linear1_size': linear1_size }\n",
    "\n",
    "    train5fold(run_configs, model_ft, lr, wd, amsgrad, i)\n",
    "\n",
    "# model_ft = ViewNet(num_classes, conv1_filters, conv2_filters, conv3_filters, linear1_size, dropout)\n",
    "# print(model_ft)\n",
    "# num_parameters = sum(p.numel() for p in model_ft.parameters())\n",
    "# print(num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# conv1_filters_size = [8, 16, 32]\n",
    "# conv2_filters_size = [16, 32, 8]\n",
    "# conv3_filters_size = [16, 32, 8]\n",
    "# linear1_sizes = [512, 1024]\n",
    "# dropouts = [0.25, 0.3]\n",
    "\n",
    "# # lrs = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3]\n",
    "# # weight_decays = [1e-5, 5e-5, 1e-4, 5e-4, 1e-6, 5e-6]\n",
    "# lrs = [1e-3]\n",
    "# weight_decays = [0.0005, 0.005]\n",
    "# i = 0\n",
    "# amsgrads=[False]\n",
    "# for conv1_filters in conv1_filters_size:\n",
    "#     for conv2_filters in conv2_filters_size:\n",
    "#         for conv3_filters in conv3_filters_size:\n",
    "#             for linear1_size in linear1_sizes:\n",
    "#                 for dropout in dropouts:\n",
    "#                     for lr in lrs:\n",
    "#                         for wd in weight_decays:\n",
    "#                             for amsgrad in amsgrads:\n",
    "#                                 if i < 10:\n",
    "#                                     i += 1\n",
    "#                                     continue\n",
    "#                                 config_string = f\"{conv1_filters}_{conv2_filters}_{conv3_filters}_{linear1_size}_{dropout}_{lr}_{wd}_{amsgrad}\"\n",
    "#                                 model_ft = ViewNet(num_classes, conv1_filters, conv2_filters, conv3_filters, linear1_size, dropout)\n",
    "#                                 run_configs = {'lr': lr, 'wd': wd, 'amsgrad': amsgrad,'dropout': dropout, \n",
    "#                                               'conv1_filters': conv1_filters, 'conv2_filters': conv2_filters, \n",
    "#                                               'conv3_filters': conv3_filters, 'linear1_size': linear1_size }\n",
    "\n",
    "#                                 train5fold(run_configs, model_ft, lr, wd, amsgrad, i)\n",
    "#                                 i += 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "image_label.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
