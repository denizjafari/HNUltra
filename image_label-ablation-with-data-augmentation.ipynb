{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZbZRovgc-In"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "import time\n",
    "import os\n",
    "from torch.utils import data\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "import io\n",
    "\n",
    "# Tensorboard for logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2aVSXNyc-Iv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda') \n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "torch.cuda.set_device(1)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylfRtoN9c-Iy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        image_ids     view_label  subj_id  view_train\n",
       "0       1323_2_1        Missing     1323         NaN\n",
       "1       1323_2_2        Missing     1323         NaN\n",
       "2       1323_2_3        Missing     1323         NaN\n",
       "3       1323_2_4        Missing     1323         NaN\n",
       "4       1323_2_5        Missing     1323         NaN\n",
       "...          ...            ...      ...         ...\n",
       "72454  1066_7_50  Saggital_Left     1066         0.0\n",
       "72455  1066_7_51  Saggital_Left     1066         0.0\n",
       "72456  1066_7_52  Saggital_Left     1066         0.0\n",
       "72457  1066_7_53  Saggital_Left     1066         0.0\n",
       "72458  1066_7_54          Other     1066         0.0\n",
       "\n",
       "[72459 rows x 4 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# root directory\n",
    "root_dir = \"/home/andreasabo/Documents/HNProject/\"\n",
    "\n",
    "# data directory on current machine: abhishekmoturu, andreasabo, denizjafari, navidkorhani\n",
    "data_dir = \"/home/andreasabo/Documents/HNProject/all_label_img/\"\n",
    "\n",
    "# read target df\n",
    "csv_path = os.path.join(root_dir, \"all_splits_100000.csv\")\n",
    "data_df = pd.read_csv(csv_path, usecols=['subj_id', 'image_ids', 'view_label', 'view_train'])\n",
    "data_df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72WtxtY8ynt1"
   },
   "source": [
    "### **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYRzqa5Uc-JB"
   },
   "outputs": [],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset: right_sag, right_trav, left_sag, left_trav, bladder, other\n",
    "num_classes = 6\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 48\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 200\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model; when True we only update the reshaped layer params\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_OCR_7uy52w"
   },
   "source": [
    "### **Reading Data Indicies and Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nnwavxcqGBv"
   },
   "outputs": [],
   "source": [
    "label_mapping = {'Other':0, 'Saggital_Right':1, 'Transverse_Right':2, \n",
    "                 'Saggital_Left':3, 'Transverse_Left':4, 'Bladder':5}\n",
    "label_unmapping = {0: 'Other', 1:'Saggital_Right', 2: 'Transverse_Right', \n",
    "                   3:'Saggital_Left', 4:'Transverse_Left', 5: 'Bladder'}\n",
    "\n",
    "data_df['view_label'] = data_df['view_label'].map(label_mapping)\n",
    "\n",
    "train_df = data_df[data_df.view_train == 1]\n",
    "test_df = data_df[data_df.view_train == 0]\n",
    "\n",
    "labels = {}\n",
    "train_and_valid_subj_ids = []\n",
    "train_and_valid_image_ids = []\n",
    "test_ids = []\n",
    "\n",
    "for ind, row in train_df.iterrows():\n",
    "    train_and_valid_subj_ids.append(row['subj_id'])\n",
    "    train_and_valid_image_ids.append(row['image_ids'])\n",
    "    labels[row['image_ids']] = row['view_label']\n",
    "\n",
    "for ind, row in test_df.iterrows():\n",
    "    test_ids.append(row['image_ids'])\n",
    "    labels[row['image_ids']] = row['view_label']\n",
    "\n",
    "s = set()\n",
    "t_v_ids = pd.DataFrame(list(zip(train_and_valid_subj_ids, train_and_valid_image_ids)), columns=['subj_ids', 'image_ids'])\n",
    "id_groups = [t_v_ids for _, t_v_ids in t_v_ids.groupby('subj_ids')]\n",
    "random.shuffle(id_groups)\n",
    "id_groups = pd.concat(id_groups).reset_index(drop=True)\n",
    "train_val_split = int(0.8*len(set(id_groups['subj_ids'].values)))\n",
    "train_val_set = [i for i in id_groups['subj_ids'].values if not (i in s or s.add(i))]\n",
    "cutoff = train_val_set[train_val_split]\n",
    "train_portion = (id_groups['subj_ids'].values == cutoff).argmax()\n",
    "\n",
    "train_ids = id_groups[:train_portion]['image_ids'].tolist()\n",
    "valid_ids = id_groups[train_portion:]['image_ids'].tolist()\n",
    "\n",
    "partition = {'train':train_ids, 'valid':valid_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_local(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=False):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tensorboard/image_summaries\n",
    "# def plot_to_image(figure):\n",
    "#   \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "#   returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "#   # Save the plot to a PNG in memory.\n",
    "#   buf = io.BytesIO()\n",
    "#   plt.savefig(buf, format='png')\n",
    "#   # Closing the figure prevents it from being displayed directly inside\n",
    "#   # the notebook.\n",
    "#   plt.close(figure)\n",
    "#   buf.seek(0)\n",
    "#   # Convert PNG buffer to TF image\n",
    "#   image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "#   # Add the batch dimension\n",
    "#   image = tf.expand_dims(image, 0)\n",
    "#   return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5p70l48MzfMP"
   },
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_fnkR4Tc-JH"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False, writer = None):\n",
    "    since = time.time()\n",
    "    classnames = ['Other', 'Saggital_Right', 'Transverse_Right', 'Saggital_Left','Transverse_Left', 'Bladder']\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 54)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            running_preds = []\n",
    "            running_labels = []\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                labels = labels.type(torch.long)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        labels = torch.argmax(labels, 1)\n",
    "                        running_preds += torch.argmax(outputs, 1).tolist()\n",
    "                        running_labels += labels.tolist()\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    preds = torch.argmax(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print('{} loss:\\t{:.4f} | {} acc:\\t{:.4f}\\n'.format(phase, epoch_loss, phase, epoch_acc))\n",
    "            # Log to tensorboard for visualization\n",
    "            if writer is not None and phase == 'train':\n",
    "                writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "                writer.add_scalar('Accuracy/train', epoch_acc, epoch)\n",
    "#                 cm = confusion_matrix(running_labels, running_preds)\n",
    "#                 figure = plot_confusion_matrix_local(cm, classnames)\n",
    "#                 writer.add_image('confusion_matrix/train', figure, epoch)\n",
    "                \n",
    "                \n",
    "                \n",
    "            if writer is not None and phase == 'val':\n",
    "                writer.add_scalar('Loss/val', epoch_loss, epoch)\n",
    "                writer.add_scalar('Accuracy/val', epoch_acc, epoch)\n",
    "                \n",
    "#                 cm = confusion_matrix(running_labels, running_preds)\n",
    "#                 figure = plot_confusion_matrix_local(cm, classnames)\n",
    "#                 writer.add_image('confusion_matrix/val', figure, epoch)\n",
    "            # deep copy the model\n",
    "            if phase == 'train':\n",
    "                print(classification_report(running_labels, running_preds))\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print(classification_report(running_labels, running_preds))\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72y17dlcc-JL"
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BbgEWoqKc-JO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 256\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=False)\n",
    "\n",
    "class ViewNet(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self, num_classes, conv1_filters, conv2_filters, conv3_filters, linear1_size, dropout):\n",
    "        super(ViewNet, self).__init__()\n",
    "        self.conv1_filters = conv1_filters\n",
    "        self.conv2_filters = conv2_filters\n",
    "        self.conv3_filters = conv3_filters\n",
    "        self.linear1_size = linear1_size\n",
    "        self.drop_percent = dropout\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, self.conv1_filters, 4, padding=2)\n",
    "        self.conv2 = nn.Conv2d(self.conv1_filters, self.conv2_filters, 4, padding=2)\n",
    "        self.conv3 = nn.Conv2d(self.conv2_filters, self.conv3_filters, 4, padding=2)\n",
    "        self.pool = nn.MaxPool2d(4, 4)\n",
    "        self.dropout = nn.Dropout(self.drop_percent)\n",
    "        self.linear1 = nn.Linear(self.conv3_filters*4*4, self.linear1_size)\n",
    "        self.dropout = nn.Dropout(self.drop_percent)\n",
    "        self.linear2 = nn.Linear(self.linear1_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.dropout(F.relu(self.conv1(x))))\n",
    "        x = self.pool(self.dropout(F.relu(self.conv2(x))))\n",
    "        x = self.pool(self.dropout(F.relu(self.conv3(x))))\n",
    "        x = x.view(-1, self.conv3_filters*4*4) ## reshaping \n",
    "        x = self.dropout(F.relu((self.linear1(x))))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qhoGXnXmzjd4"
   },
   "source": [
    "### **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iikM7_G3c-JR"
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, list_IDs, labels, transformations=None):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.transformations = transformations\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        img_path = data_dir + ID + '.jpg'\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        \n",
    "        if self.transformations:\n",
    "            image = self.transformations(image)\n",
    "        \n",
    "        \n",
    "        image = ToTensor()(image)\n",
    "        \n",
    "        \n",
    "        \n",
    "        y = torch.FloatTensor([0]*6)        \n",
    "        y[int(self.labels[ID])] = 1\n",
    "\n",
    "        return image, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DdSxc4Dhc-JT"
   },
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "transformation = transforms.Compose([transforms.RandomAffine(degrees=8, translate=(0.1, 0.1), scale=(0.95,1.25))])\n",
    "# Generators\n",
    "training_set = Dataset(partition['train'], labels, transformations = transformation)\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(partition['valid'], labels)\n",
    "validation_generator = data.DataLoader(validation_set, **params)\n",
    "\n",
    "dataloaders_dict = {'train':training_generator, 'val':validation_generator}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets check that the data augmentation makes sense\n",
    "\n",
    "\n",
    "for inputs, labs in training_generator:\n",
    "#     plt.subplot(2,5)\n",
    "    plt.figure(figsize=(20,10)) \n",
    "    for i in range(10):\n",
    "        im = inputs[i]\n",
    "        im_np = np.asarray(im).squeeze()\n",
    "        plt.subplot(2,5,i+ 1)\n",
    "        plt.imshow(im_np, cmap='gray')\n",
    "        frame1 = plt.gca()\n",
    "        frame1.axes.get_xaxis().set_visible(False)\n",
    "        frame1.axes.get_yaxis().set_visible(False)\n",
    "        label = np.argmax(labs[i]== 1).cpu().item()\n",
    "        plt.title(label_unmapping[label])\n",
    "        \n",
    "        \n",
    "    break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8MJGKSRz-Rz"
   },
   "source": [
    "### **Running the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "07uuFRNNc-JW"
   },
   "outputs": [],
   "source": [
    "def SetupAndRunTest(model_ft, writer, lr, wd, amsgrad):\n",
    "    \n",
    "    input_size = 256\n",
    "\n",
    "    # Print the model we just instantiated\n",
    "#     print(model_ft)\n",
    "\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    # Gather the parameters to be optimized/updated in this run. If we are\n",
    "    #  finetuning we will be updating all parameters. However, if we are\n",
    "    #  doing feature extract method, we will only update the parameters\n",
    "    #  that we have just initialized, i.e. the parameters with requires_grad\n",
    "    #  is True.\n",
    "    params_to_update = model_ft.parameters()\n",
    "#     print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\",name)\n",
    "    else:\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(\"\\t\",name)\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.Adam(params_to_update, lr=lr, weight_decay=wd, amsgrad=amsgrad)\n",
    "\n",
    "    # Setup the loss fxn\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs, is_inception=(model_name==\"inception\"), writer = writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRFzmztTc-Ja"
   },
   "outputs": [],
   "source": [
    "# Train and evaluate\n",
    "# model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs, is_inception=(model_name==\"inception\"), writer = writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t conv1.weight\n",
      "\t conv1.bias\n",
      "\t conv2.weight\n",
      "\t conv2.bias\n",
      "\t conv3.weight\n",
      "\t conv3.bias\n",
      "\t linear1.weight\n",
      "\t linear1.bias\n",
      "\t linear2.weight\n",
      "\t linear2.bias\n",
      "Epoch 1/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.5353 | train acc:\t0.4000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.87      0.55      3943\n",
      "           1       0.33      0.14      0.19      1671\n",
      "           2       0.20      0.00      0.00      1174\n",
      "           3       0.34      0.21      0.26      1778\n",
      "           4       0.38      0.02      0.04      1329\n",
      "           5       0.57      0.29      0.38      1001\n",
      "\n",
      "    accuracy                           0.40     10896\n",
      "   macro avg       0.37      0.25      0.24     10896\n",
      "weighted avg       0.37      0.40      0.31     10896\n",
      "\n",
      "val loss:\t1.4892 | val acc:\t0.5179\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.88      0.69      1518\n",
      "           1       0.49      0.04      0.07       468\n",
      "           2       0.00      0.00      0.00       321\n",
      "           3       0.30      0.53      0.38       397\n",
      "           4       0.46      0.06      0.10       309\n",
      "           5       0.85      0.40      0.55       252\n",
      "\n",
      "    accuracy                           0.52      3265\n",
      "   macro avg       0.44      0.32      0.30      3265\n",
      "weighted avg       0.48      0.52      0.43      3265\n",
      "\n",
      "Epoch 2/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasabo/.conda/envs/hn/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t1.3738 | train acc:\t0.4606\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.77      0.58      3943\n",
      "           1       0.42      0.31      0.36      1671\n",
      "           2       0.44      0.02      0.04      1174\n",
      "           3       0.38      0.38      0.38      1778\n",
      "           4       0.44      0.09      0.15      1329\n",
      "           5       0.63      0.65      0.64      1001\n",
      "\n",
      "    accuracy                           0.46     10896\n",
      "   macro avg       0.46      0.37      0.36     10896\n",
      "weighted avg       0.45      0.46      0.41     10896\n",
      "\n",
      "val loss:\t1.4587 | val acc:\t0.5449\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.94      0.69      1518\n",
      "           1       0.50      0.26      0.35       468\n",
      "           2       1.00      0.01      0.01       321\n",
      "           3       0.40      0.20      0.27       397\n",
      "           4       0.50      0.06      0.11       309\n",
      "           5       0.88      0.51      0.64       252\n",
      "\n",
      "    accuracy                           0.54      3265\n",
      "   macro avg       0.64      0.33      0.34      3265\n",
      "weighted avg       0.59      0.54      0.46      3265\n",
      "\n",
      "Epoch 3/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3274 | train acc:\t0.4823\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.76      0.59      3943\n",
      "           1       0.47      0.38      0.42      1671\n",
      "           2       0.48      0.06      0.11      1174\n",
      "           3       0.41      0.39      0.40      1778\n",
      "           4       0.44      0.15      0.22      1329\n",
      "           5       0.66      0.65      0.65      1001\n",
      "\n",
      "    accuracy                           0.48     10896\n",
      "   macro avg       0.49      0.40      0.40     10896\n",
      "weighted avg       0.48      0.48      0.44     10896\n",
      "\n",
      "val loss:\t1.4238 | val acc:\t0.5544\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.87      0.69      1518\n",
      "           1       0.47      0.29      0.36       468\n",
      "           2       0.79      0.03      0.07       321\n",
      "           3       0.39      0.32      0.35       397\n",
      "           4       0.40      0.24      0.30       309\n",
      "           5       0.88      0.58      0.70       252\n",
      "\n",
      "    accuracy                           0.55      3265\n",
      "   macro avg       0.58      0.39      0.41      3265\n",
      "weighted avg       0.57      0.55      0.50      3265\n",
      "\n",
      "Epoch 4/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2997 | train acc:\t0.4937\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.74      0.59      3943\n",
      "           1       0.49      0.40      0.45      1671\n",
      "           2       0.48      0.09      0.15      1174\n",
      "           3       0.43      0.40      0.41      1778\n",
      "           4       0.44      0.21      0.29      1329\n",
      "           5       0.65      0.68      0.66      1001\n",
      "\n",
      "    accuracy                           0.49     10896\n",
      "   macro avg       0.50      0.42      0.43     10896\n",
      "weighted avg       0.49      0.49      0.46     10896\n",
      "\n",
      "val loss:\t1.4309 | val acc:\t0.5198\n",
      "\n",
      "Epoch 5/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2882 | train acc:\t0.4941\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.74      0.59      3943\n",
      "           1       0.50      0.40      0.45      1671\n",
      "           2       0.46      0.12      0.19      1174\n",
      "           3       0.43      0.40      0.41      1778\n",
      "           4       0.41      0.20      0.27      1329\n",
      "           5       0.68      0.68      0.68      1001\n",
      "\n",
      "    accuracy                           0.49     10896\n",
      "   macro avg       0.49      0.42      0.43     10896\n",
      "weighted avg       0.49      0.49      0.47     10896\n",
      "\n",
      "val loss:\t1.3991 | val acc:\t0.5685\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.91      0.69      1518\n",
      "           1       0.50      0.30      0.38       468\n",
      "           2       0.77      0.13      0.23       321\n",
      "           3       0.44      0.26      0.33       397\n",
      "           4       0.60      0.06      0.11       309\n",
      "           5       0.84      0.66      0.74       252\n",
      "\n",
      "    accuracy                           0.57      3265\n",
      "   macro avg       0.62      0.39      0.41      3265\n",
      "weighted avg       0.58      0.57      0.51      3265\n",
      "\n",
      "Epoch 6/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2598 | train acc:\t0.5102\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.75      0.60      3943\n",
      "           1       0.54      0.40      0.46      1671\n",
      "           2       0.51      0.16      0.24      1174\n",
      "           3       0.44      0.42      0.43      1778\n",
      "           4       0.45      0.26      0.33      1329\n",
      "           5       0.69      0.68      0.69      1001\n",
      "\n",
      "    accuracy                           0.51     10896\n",
      "   macro avg       0.52      0.44      0.46     10896\n",
      "weighted avg       0.51      0.51      0.49     10896\n",
      "\n",
      "val loss:\t1.3791 | val acc:\t0.5783\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.82      0.71      1518\n",
      "           1       0.48      0.39      0.43       468\n",
      "           2       0.75      0.14      0.23       321\n",
      "           3       0.39      0.44      0.41       397\n",
      "           4       0.43      0.25      0.31       309\n",
      "           5       0.88      0.64      0.74       252\n",
      "\n",
      "    accuracy                           0.58      3265\n",
      "   macro avg       0.59      0.45      0.47      3265\n",
      "weighted avg       0.59      0.58      0.55      3265\n",
      "\n",
      "Epoch 7/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2442 | train acc:\t0.5118\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.73      0.60      3943\n",
      "           1       0.55      0.41      0.47      1671\n",
      "           2       0.49      0.17      0.25      1174\n",
      "           3       0.44      0.43      0.44      1778\n",
      "           4       0.45      0.25      0.32      1329\n",
      "           5       0.69      0.69      0.69      1001\n",
      "\n",
      "    accuracy                           0.51     10896\n",
      "   macro avg       0.52      0.45      0.46     10896\n",
      "weighted avg       0.51      0.51      0.49     10896\n",
      "\n",
      "val loss:\t1.3613 | val acc:\t0.5764\n",
      "\n",
      "Epoch 8/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2286 | train acc:\t0.5167\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.72      0.59      3943\n",
      "           1       0.57      0.42      0.48      1671\n",
      "           2       0.52      0.19      0.28      1174\n",
      "           3       0.45      0.44      0.45      1778\n",
      "           4       0.45      0.29      0.35      1329\n",
      "           5       0.68      0.70      0.69      1001\n",
      "\n",
      "    accuracy                           0.52     10896\n",
      "   macro avg       0.53      0.46      0.47     10896\n",
      "weighted avg       0.52      0.52      0.50     10896\n",
      "\n",
      "val loss:\t1.3747 | val acc:\t0.5703\n",
      "\n",
      "Epoch 9/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2190 | train acc:\t0.5225\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.72      0.60      3943\n",
      "           1       0.56      0.42      0.48      1671\n",
      "           2       0.52      0.22      0.31      1174\n",
      "           3       0.46      0.44      0.45      1778\n",
      "           4       0.46      0.30      0.36      1329\n",
      "           5       0.69      0.70      0.69      1001\n",
      "\n",
      "    accuracy                           0.52     10896\n",
      "   macro avg       0.53      0.47      0.48     10896\n",
      "weighted avg       0.52      0.52      0.51     10896\n",
      "\n",
      "val loss:\t1.3049 | val acc:\t0.5703\n",
      "\n",
      "Epoch 10/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2092 | train acc:\t0.5230\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.72      0.60      3943\n",
      "           1       0.57      0.43      0.49      1671\n",
      "           2       0.53      0.21      0.30      1174\n",
      "           3       0.46      0.44      0.45      1778\n",
      "           4       0.45      0.30      0.36      1329\n",
      "           5       0.69      0.73      0.71      1001\n",
      "\n",
      "    accuracy                           0.52     10896\n",
      "   macro avg       0.53      0.47      0.48     10896\n",
      "weighted avg       0.52      0.52      0.51     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.3134 | val acc:\t0.5669\n",
      "\n",
      "Epoch 11/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1997 | train acc:\t0.5339\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.74      0.61      3943\n",
      "           1       0.57      0.42      0.48      1671\n",
      "           2       0.59      0.26      0.36      1174\n",
      "           3       0.46      0.44      0.45      1778\n",
      "           4       0.49      0.31      0.38      1329\n",
      "           5       0.70      0.73      0.71      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.56      0.48      0.50     10896\n",
      "weighted avg       0.54      0.53      0.52     10896\n",
      "\n",
      "val loss:\t1.3590 | val acc:\t0.5752\n",
      "\n",
      "Epoch 12/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1916 | train acc:\t0.5387\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.74      0.61      3943\n",
      "           1       0.58      0.44      0.50      1671\n",
      "           2       0.56      0.25      0.35      1174\n",
      "           3       0.49      0.45      0.47      1778\n",
      "           4       0.47      0.31      0.38      1329\n",
      "           5       0.71      0.73      0.72      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.56      0.49      0.50     10896\n",
      "weighted avg       0.54      0.54      0.52     10896\n",
      "\n",
      "val loss:\t1.3671 | val acc:\t0.5810\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.71      0.69      1518\n",
      "           1       0.63      0.26      0.37       468\n",
      "           2       0.47      0.37      0.42       321\n",
      "           3       0.41      0.59      0.48       397\n",
      "           4       0.39      0.52      0.45       309\n",
      "           5       0.83      0.72      0.77       252\n",
      "\n",
      "    accuracy                           0.58      3265\n",
      "   macro avg       0.57      0.53      0.53      3265\n",
      "weighted avg       0.60      0.58      0.57      3265\n",
      "\n",
      "Epoch 13/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1843 | train acc:\t0.5396\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.72      0.61      3943\n",
      "           1       0.59      0.43      0.49      1671\n",
      "           2       0.54      0.25      0.34      1174\n",
      "           3       0.48      0.48      0.48      1778\n",
      "           4       0.48      0.34      0.39      1329\n",
      "           5       0.70      0.74      0.72      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.55      0.49      0.51     10896\n",
      "weighted avg       0.54      0.54      0.53     10896\n",
      "\n",
      "val loss:\t1.3479 | val acc:\t0.5645\n",
      "\n",
      "Epoch 14/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1618 | train acc:\t0.5426\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.72      0.61      3943\n",
      "           1       0.59      0.43      0.50      1671\n",
      "           2       0.54      0.28      0.37      1174\n",
      "           3       0.48      0.46      0.47      1778\n",
      "           4       0.49      0.33      0.40      1329\n",
      "           5       0.72      0.76      0.74      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.56      0.50      0.51     10896\n",
      "weighted avg       0.54      0.54      0.53     10896\n",
      "\n",
      "val loss:\t1.3185 | val acc:\t0.5868\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.87      0.71      1518\n",
      "           1       0.45      0.47      0.46       468\n",
      "           2       0.80      0.13      0.22       321\n",
      "           3       0.45      0.28      0.34       397\n",
      "           4       0.66      0.14      0.23       309\n",
      "           5       0.84      0.70      0.76       252\n",
      "\n",
      "    accuracy                           0.59      3265\n",
      "   macro avg       0.63      0.43      0.45      3265\n",
      "weighted avg       0.60      0.59      0.54      3265\n",
      "\n",
      "Epoch 15/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1587 | train acc:\t0.5423\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.61      3943\n",
      "           1       0.59      0.44      0.50      1671\n",
      "           2       0.55      0.26      0.36      1174\n",
      "           3       0.48      0.46      0.47      1778\n",
      "           4       0.50      0.33      0.40      1329\n",
      "           5       0.70      0.74      0.72      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.56      0.49      0.51     10896\n",
      "weighted avg       0.54      0.54      0.53     10896\n",
      "\n",
      "val loss:\t1.3125 | val acc:\t0.5963\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.81      0.71      1518\n",
      "           1       0.52      0.37      0.43       468\n",
      "           2       0.55      0.32      0.40       321\n",
      "           3       0.43      0.49      0.46       397\n",
      "           4       0.55      0.27      0.36       309\n",
      "           5       0.87      0.67      0.76       252\n",
      "\n",
      "    accuracy                           0.60      3265\n",
      "   macro avg       0.59      0.49      0.52      3265\n",
      "weighted avg       0.59      0.60      0.58      3265\n",
      "\n",
      "Epoch 16/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1449 | train acc:\t0.5517\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.72      0.61      3943\n",
      "           1       0.60      0.43      0.50      1671\n",
      "           2       0.61      0.32      0.42      1174\n",
      "           3       0.50      0.49      0.49      1778\n",
      "           4       0.48      0.36      0.41      1329\n",
      "           5       0.71      0.74      0.72      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.57      0.51      0.53     10896\n",
      "weighted avg       0.56      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.3227 | val acc:\t0.6003\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.82      0.71      1518\n",
      "           1       0.51      0.40      0.45       468\n",
      "           2       0.60      0.31      0.40       321\n",
      "           3       0.45      0.41      0.43       397\n",
      "           4       0.50      0.35      0.41       309\n",
      "           5       0.90      0.65      0.75       252\n",
      "\n",
      "    accuracy                           0.60      3265\n",
      "   macro avg       0.60      0.49      0.53      3265\n",
      "weighted avg       0.60      0.60      0.58      3265\n",
      "\n",
      "Epoch 17/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1401 | train acc:\t0.5508\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.72      0.61      3943\n",
      "           1       0.60      0.43      0.50      1671\n",
      "           2       0.59      0.31      0.40      1174\n",
      "           3       0.48      0.49      0.49      1778\n",
      "           4       0.48      0.37      0.42      1329\n",
      "           5       0.72      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.57      0.51      0.53     10896\n",
      "weighted avg       0.55      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.2927 | val acc:\t0.5807\n",
      "\n",
      "Epoch 18/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1283 | train acc:\t0.5618\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.62      3943\n",
      "           1       0.61      0.45      0.52      1671\n",
      "           2       0.59      0.31      0.41      1174\n",
      "           3       0.51      0.48      0.49      1778\n",
      "           4       0.52      0.36      0.43      1329\n",
      "           5       0.75      0.75      0.75      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.59      0.51      0.54     10896\n",
      "weighted avg       0.57      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.2875 | val acc:\t0.6086\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.88      0.72      1518\n",
      "           1       0.56      0.37      0.44       468\n",
      "           2       0.67      0.29      0.41       321\n",
      "           3       0.50      0.35      0.41       397\n",
      "           4       0.54      0.25      0.34       309\n",
      "           5       0.86      0.68      0.76       252\n",
      "\n",
      "    accuracy                           0.61      3265\n",
      "   macro avg       0.62      0.47      0.51      3265\n",
      "weighted avg       0.61      0.61      0.58      3265\n",
      "\n",
      "Epoch 19/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t1.1097 | train acc:\t0.5647\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63      3943\n",
      "           1       0.60      0.44      0.51      1671\n",
      "           2       0.61      0.33      0.43      1174\n",
      "           3       0.51      0.49      0.50      1778\n",
      "           4       0.52      0.39      0.44      1329\n",
      "           5       0.74      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.59      0.52      0.54     10896\n",
      "weighted avg       0.57      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.2704 | val acc:\t0.5838\n",
      "\n",
      "Epoch 20/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1220 | train acc:\t0.5616\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.62      0.45      0.52      1671\n",
      "           2       0.59      0.31      0.41      1174\n",
      "           3       0.50      0.50      0.50      1778\n",
      "           4       0.50      0.37      0.43      1329\n",
      "           5       0.73      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.52      0.54     10896\n",
      "weighted avg       0.57      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.2492 | val acc:\t0.5966\n",
      "\n",
      "Epoch 21/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1003 | train acc:\t0.5700\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.62      3943\n",
      "           1       0.63      0.48      0.55      1671\n",
      "           2       0.59      0.34      0.43      1174\n",
      "           3       0.52      0.49      0.50      1778\n",
      "           4       0.52      0.39      0.45      1329\n",
      "           5       0.73      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.53      0.55     10896\n",
      "weighted avg       0.57      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.2876 | val acc:\t0.6089\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.81      0.71      1518\n",
      "           1       0.51      0.45      0.48       468\n",
      "           2       0.55      0.41      0.47       321\n",
      "           3       0.47      0.41      0.44       397\n",
      "           4       0.55      0.33      0.41       309\n",
      "           5       0.92      0.61      0.74       252\n",
      "\n",
      "    accuracy                           0.61      3265\n",
      "   macro avg       0.61      0.50      0.54      3265\n",
      "weighted avg       0.61      0.61      0.60      3265\n",
      "\n",
      "Epoch 22/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0917 | train acc:\t0.5755\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.61      0.44      0.51      1671\n",
      "           2       0.60      0.36      0.45      1174\n",
      "           3       0.51      0.51      0.51      1778\n",
      "           4       0.54      0.40      0.46      1329\n",
      "           5       0.76      0.76      0.76      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.53      0.55     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2694 | val acc:\t0.6172\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.87      0.73      1518\n",
      "           1       0.70      0.24      0.36       468\n",
      "           2       0.63      0.30      0.41       321\n",
      "           3       0.49      0.46      0.47       397\n",
      "           4       0.48      0.37      0.42       309\n",
      "           5       0.84      0.74      0.79       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.63      0.50      0.53      3265\n",
      "weighted avg       0.62      0.62      0.59      3265\n",
      "\n",
      "Epoch 23/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0886 | train acc:\t0.5763\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.65      0.46      0.54      1671\n",
      "           2       0.60      0.34      0.44      1174\n",
      "           3       0.52      0.50      0.51      1778\n",
      "           4       0.53      0.40      0.46      1329\n",
      "           5       0.74      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.55     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2983 | val acc:\t0.6138\n",
      "\n",
      "Epoch 24/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0909 | train acc:\t0.5807\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.63      3943\n",
      "           1       0.65      0.47      0.54      1671\n",
      "           2       0.61      0.35      0.45      1174\n",
      "           3       0.52      0.51      0.52      1778\n",
      "           4       0.55      0.43      0.48      1329\n",
      "           5       0.73      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.59      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2803 | val acc:\t0.6003\n",
      "\n",
      "Epoch 25/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0770 | train acc:\t0.5838\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.65      0.48      0.55      1671\n",
      "           2       0.63      0.37      0.46      1174\n",
      "           3       0.53      0.51      0.52      1778\n",
      "           4       0.54      0.40      0.46      1329\n",
      "           5       0.75      0.75      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.61      0.54      0.56     10896\n",
      "weighted avg       0.59      0.58      0.58     10896\n",
      "\n",
      "val loss:\t1.2752 | val acc:\t0.6110\n",
      "\n",
      "Epoch 26/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0743 | train acc:\t0.5791\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63      3943\n",
      "           1       0.64      0.48      0.55      1671\n",
      "           2       0.62      0.37      0.46      1174\n",
      "           3       0.51      0.49      0.50      1778\n",
      "           4       0.55      0.43      0.48      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2894 | val acc:\t0.6052\n",
      "\n",
      "Epoch 27/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0607 | train acc:\t0.5937\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.75      0.64      3943\n",
      "           1       0.65      0.49      0.56      1671\n",
      "           2       0.64      0.39      0.48      1174\n",
      "           3       0.54      0.51      0.52      1778\n",
      "           4       0.56      0.43      0.49      1329\n",
      "           5       0.75      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.59      0.59     10896\n",
      "\n",
      "val loss:\t1.2331 | val acc:\t0.6230\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.86      0.73      1518\n",
      "           1       0.56      0.40      0.46       468\n",
      "           2       0.56      0.41      0.48       321\n",
      "           3       0.51      0.30      0.38       397\n",
      "           4       0.60      0.37      0.45       309\n",
      "           5       0.85      0.72      0.78       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.62      0.51      0.55      3265\n",
      "weighted avg       0.61      0.62      0.60      3265\n",
      "\n",
      "Epoch 28/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0608 | train acc:\t0.5864\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.64      3943\n",
      "           1       0.64      0.48      0.55      1671\n",
      "           2       0.63      0.38      0.47      1174\n",
      "           3       0.52      0.51      0.51      1778\n",
      "           4       0.57      0.43      0.49      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.2081 | val acc:\t0.6175\n",
      "\n",
      "Epoch 29/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0504 | train acc:\t0.5897\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.73      0.64      3943\n",
      "           1       0.63      0.47      0.54      1671\n",
      "           2       0.63      0.39      0.48      1174\n",
      "           3       0.53      0.52      0.53      1778\n",
      "           4       0.55      0.44      0.49      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.56      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2559 | val acc:\t0.6221\n",
      "\n",
      "Epoch 30/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0500 | train acc:\t0.5918\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      3943\n",
      "           1       0.66      0.49      0.56      1671\n",
      "           2       0.62      0.38      0.47      1174\n",
      "           3       0.52      0.52      0.52      1778\n",
      "           4       0.55      0.44      0.49      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.56      0.57     10896\n",
      "weighted avg       0.60      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2020 | val acc:\t0.6312\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.87      0.73      1518\n",
      "           1       0.66      0.37      0.48       468\n",
      "           2       0.68      0.38      0.49       321\n",
      "           3       0.49      0.46      0.47       397\n",
      "           4       0.55      0.37      0.44       309\n",
      "           5       0.91      0.58      0.71       252\n",
      "\n",
      "    accuracy                           0.63      3265\n",
      "   macro avg       0.65      0.51      0.55      3265\n",
      "weighted avg       0.64      0.63      0.61      3265\n",
      "\n",
      "Epoch 31/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0460 | train acc:\t0.5954\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65      3943\n",
      "           1       0.64      0.49      0.56      1671\n",
      "           2       0.63      0.39      0.49      1174\n",
      "           3       0.54      0.52      0.53      1778\n",
      "           4       0.55      0.44      0.49      1329\n",
      "           5       0.76      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2681 | val acc:\t0.6196\n",
      "\n",
      "Epoch 32/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0416 | train acc:\t0.5982\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65      3943\n",
      "           1       0.66      0.48      0.55      1671\n",
      "           2       0.64      0.40      0.49      1174\n",
      "           3       0.54      0.53      0.54      1778\n",
      "           4       0.57      0.44      0.50      1329\n",
      "           5       0.74      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2118 | val acc:\t0.6196\n",
      "\n",
      "Epoch 33/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0321 | train acc:\t0.6025\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.63      0.41      0.49      1174\n",
      "           3       0.54      0.53      0.54      1778\n",
      "           4       0.56      0.45      0.50      1329\n",
      "           5       0.76      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.2178 | val acc:\t0.6187\n",
      "\n",
      "Epoch 34/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0273 | train acc:\t0.6010\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.66      0.50      0.57      1671\n",
      "           2       0.63      0.41      0.49      1174\n",
      "           3       0.53      0.52      0.52      1778\n",
      "           4       0.58      0.45      0.51      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.1994 | val acc:\t0.6205\n",
      "\n",
      "Epoch 35/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0091 | train acc:\t0.6062\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.64      0.42      0.51      1174\n",
      "           3       0.56      0.53      0.54      1778\n",
      "           4       0.56      0.46      0.50      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1764 | val acc:\t0.6239\n",
      "\n",
      "Epoch 36/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0081 | train acc:\t0.6112\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.66      3943\n",
      "           1       0.66      0.50      0.57      1671\n",
      "           2       0.66      0.44      0.52      1174\n",
      "           3       0.56      0.52      0.54      1778\n",
      "           4       0.57      0.47      0.52      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2231 | val acc:\t0.6276\n",
      "\n",
      "Epoch 37/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0058 | train acc:\t0.6139\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.68      0.52      0.59      1671\n",
      "           2       0.62      0.41      0.50      1174\n",
      "           3       0.56      0.55      0.56      1778\n",
      "           4       0.58      0.46      0.51      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.2359 | val acc:\t0.6395\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.82      0.73      1518\n",
      "           1       0.61      0.48      0.54       468\n",
      "           2       0.61      0.44      0.51       321\n",
      "           3       0.53      0.47      0.50       397\n",
      "           4       0.57      0.39      0.46       309\n",
      "           5       0.88      0.65      0.75       252\n",
      "\n",
      "    accuracy                           0.64      3265\n",
      "   macro avg       0.64      0.54      0.58      3265\n",
      "weighted avg       0.64      0.64      0.63      3265\n",
      "\n",
      "Epoch 38/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0057 | train acc:\t0.6148\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.68      0.52      0.59      1671\n",
      "           2       0.65      0.43      0.51      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.57      0.47      0.52      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.1764 | val acc:\t0.6153\n",
      "\n",
      "Epoch 39/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9973 | train acc:\t0.6173\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.65      0.45      0.53      1174\n",
      "           3       0.57      0.54      0.56      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1807 | val acc:\t0.6297\n",
      "\n",
      "Epoch 40/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9949 | train acc:\t0.6189\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.68      0.52      0.59      1671\n",
      "           2       0.65      0.45      0.53      1174\n",
      "           3       0.57      0.54      0.55      1778\n",
      "           4       0.58      0.49      0.53      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1857 | val acc:\t0.6319\n",
      "\n",
      "Epoch 41/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9902 | train acc:\t0.6166\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.66      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.63      0.43      0.51      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.76      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.63      0.59      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2040 | val acc:\t0.6392\n",
      "\n",
      "Epoch 42/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9905 | train acc:\t0.6177\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.67      0.52      0.59      1671\n",
      "           2       0.65      0.46      0.54      1174\n",
      "           3       0.56      0.53      0.55      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.78      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2265 | val acc:\t0.6214\n",
      "\n",
      "Epoch 43/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9939 | train acc:\t0.6194\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.67      0.52      0.58      1671\n",
      "           2       0.67      0.45      0.54      1174\n",
      "           3       0.56      0.53      0.55      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2011 | val acc:\t0.6291\n",
      "\n",
      "Epoch 44/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9780 | train acc:\t0.6177\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.67      0.52      0.59      1671\n",
      "           2       0.66      0.47      0.54      1174\n",
      "           3       0.56      0.53      0.54      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.76      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1597 | val acc:\t0.6276\n",
      "\n",
      "Epoch 45/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9790 | train acc:\t0.6253\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.69      0.53      0.60      1671\n",
      "           2       0.64      0.48      0.55      1174\n",
      "           3       0.58      0.54      0.56      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.2135 | val acc:\t0.6328\n",
      "\n",
      "Epoch 46/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9737 | train acc:\t0.6239\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.68      0.53      0.60      1671\n",
      "           2       0.65      0.45      0.53      1174\n",
      "           3       0.58      0.54      0.56      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.79      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1951 | val acc:\t0.6358\n",
      "\n",
      "Epoch 47/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9600 | train acc:\t0.6345\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.68      0.53      0.60      1671\n",
      "           2       0.67      0.48      0.56      1174\n",
      "           3       0.59      0.56      0.58      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.2081 | val acc:\t0.6364\n",
      "\n",
      "Epoch 48/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9617 | train acc:\t0.6277\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.66      0.48      0.55      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.59      0.49      0.53      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1902 | val acc:\t0.6325\n",
      "\n",
      "Epoch 49/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9623 | train acc:\t0.6244\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.65      0.48      0.55      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1753 | val acc:\t0.6306\n",
      "\n",
      "Epoch 50/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9613 | train acc:\t0.6280\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.68      0.51      0.58      1671\n",
      "           2       0.66      0.47      0.55      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.59      0.50      0.54      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1745 | val acc:\t0.6319\n",
      "\n",
      "Epoch 51/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9551 | train acc:\t0.6307\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.67      0.49      0.57      1174\n",
      "           3       0.59      0.55      0.57      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1453 | val acc:\t0.6398\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.87      0.73      1518\n",
      "           1       0.71      0.36      0.48       468\n",
      "           2       0.59      0.43      0.50       321\n",
      "           3       0.57      0.42      0.48       397\n",
      "           4       0.61      0.32      0.42       309\n",
      "           5       0.79      0.78      0.79       252\n",
      "\n",
      "    accuracy                           0.64      3265\n",
      "   macro avg       0.65      0.53      0.57      3265\n",
      "weighted avg       0.64      0.64      0.62      3265\n",
      "\n",
      "Epoch 52/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9589 | train acc:\t0.6257\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.52      0.58      1671\n",
      "           2       0.65      0.50      0.56      1174\n",
      "           3       0.58      0.54      0.56      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.2012 | val acc:\t0.6380\n",
      "\n",
      "Epoch 53/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9499 | train acc:\t0.6293\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      3943\n",
      "           1       0.69      0.54      0.61      1671\n",
      "           2       0.64      0.47      0.54      1174\n",
      "           3       0.57      0.56      0.57      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1926 | val acc:\t0.6282\n",
      "\n",
      "Epoch 54/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9521 | train acc:\t0.6392\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      3943\n",
      "           1       0.70      0.55      0.61      1671\n",
      "           2       0.67      0.50      0.57      1174\n",
      "           3       0.59      0.56      0.58      1778\n",
      "           4       0.61      0.51      0.56      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.2398 | val acc:\t0.6306\n",
      "\n",
      "Epoch 55/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9361 | train acc:\t0.6387\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.70      0.55      0.62      1671\n",
      "           2       0.69      0.51      0.59      1174\n",
      "           3       0.58      0.57      0.58      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.1655 | val acc:\t0.6260\n",
      "\n",
      "Epoch 56/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9433 | train acc:\t0.6375\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.68      3943\n",
      "           1       0.69      0.55      0.61      1671\n",
      "           2       0.65      0.49      0.56      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.61      0.51      0.56      1329\n",
      "           5       0.78      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.1717 | val acc:\t0.6214\n",
      "\n",
      "Epoch 57/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9260 | train acc:\t0.6399\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.67      0.53      0.59      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.63      0.51      0.56      1329\n",
      "           5       0.77      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.1787 | val acc:\t0.6334\n",
      "\n",
      "Epoch 58/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9374 | train acc:\t0.6392\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.67      3943\n",
      "           1       0.69      0.55      0.61      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.58      0.55      0.57      1778\n",
      "           4       0.62      0.51      0.56      1329\n",
      "           5       0.80      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.1917 | val acc:\t0.6355\n",
      "\n",
      "Epoch 59/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9380 | train acc:\t0.6448\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.68      0.51      0.58      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.1780 | val acc:\t0.6328\n",
      "\n",
      "Epoch 60/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9308 | train acc:\t0.6457\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.69      3943\n",
      "           1       0.69      0.57      0.62      1671\n",
      "           2       0.68      0.49      0.57      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.62      0.51      0.56      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.2021 | val acc:\t0.6270\n",
      "\n",
      "Epoch 61/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9240 | train acc:\t0.6478\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.69      0.50      0.58      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.1643 | val acc:\t0.6401\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.88      0.74      1518\n",
      "           1       0.60      0.43      0.50       468\n",
      "           2       0.70      0.40      0.51       321\n",
      "           3       0.57      0.32      0.41       397\n",
      "           4       0.53      0.36      0.43       309\n",
      "           5       0.87      0.71      0.78       252\n",
      "\n",
      "    accuracy                           0.64      3265\n",
      "   macro avg       0.65      0.52      0.56      3265\n",
      "weighted avg       0.64      0.64      0.62      3265\n",
      "\n",
      "Epoch 62/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9250 | train acc:\t0.6420\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.55      0.61      1671\n",
      "           2       0.67      0.50      0.57      1174\n",
      "           3       0.60      0.56      0.58      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1579 | val acc:\t0.6263\n",
      "\n",
      "Epoch 63/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9205 | train acc:\t0.6500\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      3943\n",
      "           1       0.69      0.57      0.63      1671\n",
      "           2       0.67      0.52      0.59      1174\n",
      "           3       0.61      0.58      0.60      1778\n",
      "           4       0.61      0.53      0.57      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1543 | val acc:\t0.6129\n",
      "\n",
      "Epoch 64/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9204 | train acc:\t0.6490\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.69      3943\n",
      "           1       0.71      0.57      0.63      1671\n",
      "           2       0.67      0.50      0.57      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1625 | val acc:\t0.6319\n",
      "\n",
      "Epoch 65/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9275 | train acc:\t0.6425\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.55      0.61      1671\n",
      "           2       0.67      0.52      0.58      1174\n",
      "           3       0.59      0.58      0.59      1778\n",
      "           4       0.61      0.51      0.56      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.1776 | val acc:\t0.6270\n",
      "\n",
      "Epoch 66/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9145 | train acc:\t0.6488\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.70      0.56      0.62      1671\n",
      "           2       0.67      0.52      0.59      1174\n",
      "           3       0.61      0.58      0.60      1778\n",
      "           4       0.61      0.51      0.56      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.1820 | val acc:\t0.6404\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.80      0.73      1518\n",
      "           1       0.75      0.37      0.49       468\n",
      "           2       0.55      0.43      0.48       321\n",
      "           3       0.48      0.60      0.53       397\n",
      "           4       0.54      0.51      0.52       309\n",
      "           5       0.87      0.71      0.78       252\n",
      "\n",
      "    accuracy                           0.64      3265\n",
      "   macro avg       0.64      0.57      0.59      3265\n",
      "weighted avg       0.65      0.64      0.63      3265\n",
      "\n",
      "Epoch 67/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9181 | train acc:\t0.6508\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.77      0.69      3943\n",
      "           1       0.71      0.56      0.62      1671\n",
      "           2       0.67      0.52      0.58      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.63      0.51      0.56      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1613 | val acc:\t0.6322\n",
      "\n",
      "Epoch 68/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9054 | train acc:\t0.6512\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.69      3943\n",
      "           1       0.69      0.57      0.62      1671\n",
      "           2       0.66      0.52      0.58      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.63      0.51      0.57      1329\n",
      "           5       0.82      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1733 | val acc:\t0.6315\n",
      "\n",
      "Epoch 69/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9041 | train acc:\t0.6523\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.70      0.53      0.60      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.63      0.52      0.57      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1610 | val acc:\t0.6450\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.85      0.74      1518\n",
      "           1       0.62      0.47      0.53       468\n",
      "           2       0.70      0.34      0.46       321\n",
      "           3       0.50      0.48      0.49       397\n",
      "           4       0.61      0.41      0.49       309\n",
      "           5       0.89      0.69      0.77       252\n",
      "\n",
      "    accuracy                           0.65      3265\n",
      "   macro avg       0.66      0.54      0.58      3265\n",
      "weighted avg       0.65      0.65      0.63      3265\n",
      "\n",
      "Epoch 70/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9000 | train acc:\t0.6535\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.68      0.53      0.60      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1607 | val acc:\t0.6392\n",
      "\n",
      "Epoch 71/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9081 | train acc:\t0.6514\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.68      0.56      0.62      1671\n",
      "           2       0.69      0.51      0.59      1174\n",
      "           3       0.60      0.59      0.59      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1828 | val acc:\t0.6383\n",
      "\n",
      "Epoch 72/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9058 | train acc:\t0.6580\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.72      0.58      0.64      1671\n",
      "           2       0.67      0.52      0.59      1174\n",
      "           3       0.62      0.61      0.61      1778\n",
      "           4       0.62      0.52      0.57      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1522 | val acc:\t0.6288\n",
      "\n",
      "Epoch 73/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9017 | train acc:\t0.6571\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.67      0.52      0.58      1174\n",
      "           3       0.62      0.58      0.60      1778\n",
      "           4       0.64      0.54      0.58      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1315 | val acc:\t0.6328\n",
      "\n",
      "Epoch 74/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8915 | train acc:\t0.6575\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.77      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.64      0.52      0.58      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1735 | val acc:\t0.6263\n",
      "\n",
      "Epoch 75/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9076 | train acc:\t0.6569\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.68      0.53      0.60      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.63      0.53      0.58      1329\n",
      "           5       0.80      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1708 | val acc:\t0.6322\n",
      "\n",
      "Epoch 76/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8997 | train acc:\t0.6530\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      3943\n",
      "           1       0.70      0.58      0.63      1671\n",
      "           2       0.67      0.53      0.59      1174\n",
      "           3       0.60      0.60      0.60      1778\n",
      "           4       0.64      0.52      0.57      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1453 | val acc:\t0.6285\n",
      "\n",
      "Epoch 77/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8879 | train acc:\t0.6620\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.71      0.58      0.64      1671\n",
      "           2       0.69      0.54      0.61      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.65      0.55      0.60      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1578 | val acc:\t0.6285\n",
      "\n",
      "Epoch 78/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8968 | train acc:\t0.6566\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.67      0.52      0.59      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.79      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1435 | val acc:\t0.6282\n",
      "\n",
      "Epoch 79/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8980 | train acc:\t0.6539\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.58      0.63      1671\n",
      "           2       0.67      0.53      0.59      1174\n",
      "           3       0.61      0.58      0.60      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1546 | val acc:\t0.6230\n",
      "\n",
      "Epoch 80/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8920 | train acc:\t0.6600\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.72      0.58      0.64      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.61      0.60      0.61      1778\n",
      "           4       0.64      0.53      0.58      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1321 | val acc:\t0.6257\n",
      "\n",
      "Epoch 81/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8860 | train acc:\t0.6606\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.70      0.53      0.61      1174\n",
      "           3       0.62      0.59      0.60      1778\n",
      "           4       0.63      0.55      0.58      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1269 | val acc:\t0.6429\n",
      "\n",
      "Epoch 82/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8918 | train acc:\t0.6583\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.57      0.63      1671\n",
      "           2       0.67      0.53      0.60      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.63      0.53      0.58      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1385 | val acc:\t0.6456\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.86      0.74      1518\n",
      "           1       0.76      0.34      0.47       468\n",
      "           2       0.73      0.39      0.51       321\n",
      "           3       0.49      0.50      0.49       397\n",
      "           4       0.56      0.45      0.50       309\n",
      "           5       0.90      0.69      0.78       252\n",
      "\n",
      "    accuracy                           0.65      3265\n",
      "   macro avg       0.68      0.54      0.58      3265\n",
      "weighted avg       0.66      0.65      0.63      3265\n",
      "\n",
      "Epoch 83/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8911 | train acc:\t0.6586\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.70      0.53      0.60      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.64      0.55      0.59      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1605 | val acc:\t0.6395\n",
      "\n",
      "Epoch 84/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9000 | train acc:\t0.6528\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.57      0.62      1671\n",
      "           2       0.66      0.52      0.58      1174\n",
      "           3       0.62      0.59      0.60      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.78      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1495 | val acc:\t0.6426\n",
      "\n",
      "Epoch 85/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.8795 | train acc:\t0.6615\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.70      0.55      0.61      1174\n",
      "           3       0.62      0.61      0.61      1778\n",
      "           4       0.63      0.53      0.58      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1493 | val acc:\t0.6315\n",
      "\n",
      "Epoch 86/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8802 | train acc:\t0.6638\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.68      0.56      0.62      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.64      0.57      0.60      1329\n",
      "           5       0.80      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.65      0.66     10896\n",
      "weighted avg       0.67      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1690 | val acc:\t0.6257\n",
      "\n",
      "Epoch 87/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8706 | train acc:\t0.6669\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.71      0.58      0.64      1671\n",
      "           2       0.71      0.55      0.62      1174\n",
      "           3       0.63      0.62      0.62      1778\n",
      "           4       0.64      0.55      0.59      1329\n",
      "           5       0.81      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.1263 | val acc:\t0.6319\n",
      "\n",
      "Epoch 88/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8819 | train acc:\t0.6635\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.71      0.57      0.63      1671\n",
      "           2       0.66      0.54      0.60      1174\n",
      "           3       0.64      0.60      0.62      1778\n",
      "           4       0.64      0.54      0.58      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1156 | val acc:\t0.6104\n",
      "\n",
      "Epoch 89/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8759 | train acc:\t0.6658\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.71      0.59      0.64      1671\n",
      "           2       0.68      0.53      0.59      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.80      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.1258 | val acc:\t0.6217\n",
      "\n",
      "Epoch 90/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8708 | train acc:\t0.6681\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.71      0.59      0.65      1671\n",
      "           2       0.69      0.54      0.60      1174\n",
      "           3       0.62      0.62      0.62      1778\n",
      "           4       0.64      0.55      0.59      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1097 | val acc:\t0.6328\n",
      "\n",
      "Epoch 91/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8736 | train acc:\t0.6712\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.78      0.71      3943\n",
      "           1       0.69      0.59      0.63      1671\n",
      "           2       0.72      0.56      0.63      1174\n",
      "           3       0.63      0.60      0.61      1778\n",
      "           4       0.65      0.54      0.59      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1628 | val acc:\t0.6325\n",
      "\n",
      "Epoch 92/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8675 | train acc:\t0.6717\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.70      0.60      0.64      1671\n",
      "           2       0.68      0.54      0.61      1174\n",
      "           3       0.64      0.61      0.62      1778\n",
      "           4       0.65      0.56      0.60      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1570 | val acc:\t0.6429\n",
      "\n",
      "Epoch 93/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8762 | train acc:\t0.6659\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.69      0.55      0.62      1174\n",
      "           3       0.62      0.59      0.61      1778\n",
      "           4       0.65      0.55      0.59      1329\n",
      "           5       0.80      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.1463 | val acc:\t0.6257\n",
      "\n",
      "Epoch 94/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8693 | train acc:\t0.6702\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.78      0.70      3943\n",
      "           1       0.72      0.60      0.66      1671\n",
      "           2       0.69      0.54      0.60      1174\n",
      "           3       0.64      0.60      0.62      1778\n",
      "           4       0.66      0.54      0.60      1329\n",
      "           5       0.80      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1275 | val acc:\t0.6279\n",
      "\n",
      "Epoch 95/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8693 | train acc:\t0.6674\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.71      0.59      0.65      1671\n",
      "           2       0.67      0.54      0.60      1174\n",
      "           3       0.63      0.60      0.61      1778\n",
      "           4       0.66      0.55      0.60      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.64      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.1233 | val acc:\t0.6426\n",
      "\n",
      "Epoch 96/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8729 | train acc:\t0.6719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.71      0.60      0.65      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.64      0.61      0.62      1778\n",
      "           4       0.64      0.54      0.59      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1564 | val acc:\t0.6270\n",
      "\n",
      "Epoch 97/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.8567 | train acc:\t0.6694\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.71      0.59      0.65      1671\n",
      "           2       0.68      0.54      0.60      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.67      0.56      0.61      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1301 | val acc:\t0.6423\n",
      "\n",
      "Epoch 98/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8733 | train acc:\t0.6633\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.70      0.58      0.63      1671\n",
      "           2       0.69      0.52      0.59      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.63      0.55      0.59      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1515 | val acc:\t0.6270\n",
      "\n",
      "Epoch 99/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8599 | train acc:\t0.6724\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.72      0.60      0.65      1671\n",
      "           2       0.68      0.56      0.62      1174\n",
      "           3       0.64      0.61      0.62      1778\n",
      "           4       0.65      0.55      0.60      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1245 | val acc:\t0.6395\n",
      "\n",
      "Epoch 100/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8644 | train acc:\t0.6714\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.71      3943\n",
      "           1       0.70      0.60      0.65      1671\n",
      "           2       0.68      0.53      0.60      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.64      0.57      0.60      1329\n",
      "           5       0.79      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1780 | val acc:\t0.6361\n",
      "\n",
      "Epoch 101/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8523 | train acc:\t0.6752\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.72      0.60      0.65      1671\n",
      "           2       0.70      0.58      0.64      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.66      0.56      0.60      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.1454 | val acc:\t0.6346\n",
      "\n",
      "Epoch 102/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8623 | train acc:\t0.6752\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.70      0.61      0.65      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.65      0.62      0.63      1778\n",
      "           4       0.64      0.57      0.60      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.1288 | val acc:\t0.6361\n",
      "\n",
      "Epoch 103/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8475 | train acc:\t0.6773\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.78      0.71      3943\n",
      "           1       0.73      0.60      0.66      1671\n",
      "           2       0.71      0.57      0.63      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.67      0.58      0.62      1329\n",
      "           5       0.81      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.65      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.1254 | val acc:\t0.6426\n",
      "\n",
      "Epoch 104/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8629 | train acc:\t0.6678\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.69      0.61      0.65      1671\n",
      "           2       0.67      0.53      0.59      1174\n",
      "           3       0.63      0.60      0.62      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.80      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.1260 | val acc:\t0.6312\n",
      "\n",
      "Epoch 105/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8673 | train acc:\t0.6700\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.64      0.55      0.59      1329\n",
      "           5       0.80      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1347 | val acc:\t0.6349\n",
      "\n",
      "Epoch 106/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8531 | train acc:\t0.6714\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.69      0.55      0.62      1174\n",
      "           3       0.63      0.60      0.62      1778\n",
      "           4       0.64      0.56      0.59      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1395 | val acc:\t0.6392\n",
      "\n",
      "Epoch 107/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8549 | train acc:\t0.6740\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.71      0.57      0.63      1174\n",
      "           3       0.63      0.60      0.62      1778\n",
      "           4       0.65      0.57      0.61      1329\n",
      "           5       0.79      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.68      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1539 | val acc:\t0.6276\n",
      "\n",
      "Epoch 108/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8487 | train acc:\t0.6794\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.78      0.71      3943\n",
      "           1       0.73      0.59      0.65      1671\n",
      "           2       0.68      0.56      0.61      1174\n",
      "           3       0.65      0.62      0.63      1778\n",
      "           4       0.65      0.57      0.61      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1679 | val acc:\t0.6251\n",
      "\n",
      "Epoch 109/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.8429 | train acc:\t0.6795\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.71      0.61      0.66      1671\n",
      "           2       0.71      0.57      0.64      1174\n",
      "           3       0.65      0.61      0.63      1778\n",
      "           4       0.64      0.58      0.61      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1361 | val acc:\t0.6279\n",
      "\n",
      "Epoch 110/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8539 | train acc:\t0.6808\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.71      3943\n",
      "           1       0.73      0.62      0.67      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.66      0.56      0.61      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1480 | val acc:\t0.6263\n",
      "\n",
      "Epoch 111/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8419 | train acc:\t0.6754\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.73      0.62      0.67      1671\n",
      "           2       0.69      0.57      0.63      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.65      0.56      0.60      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.1396 | val acc:\t0.6294\n",
      "\n",
      "Epoch 112/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8471 | train acc:\t0.6782\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.72      0.62      0.66      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.65      0.56      0.60      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1363 | val acc:\t0.6444\n",
      "\n",
      "Epoch 113/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8463 | train acc:\t0.6787\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.65      0.58      0.61      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1461 | val acc:\t0.6101\n",
      "\n",
      "Epoch 114/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8460 | train acc:\t0.6804\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.71      0.57      0.63      1174\n",
      "           3       0.64      0.63      0.64      1778\n",
      "           4       0.65      0.56      0.60      1329\n",
      "           5       0.81      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1405 | val acc:\t0.6303\n",
      "\n",
      "Epoch 115/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8359 | train acc:\t0.6881\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.71      0.61      0.66      1671\n",
      "           2       0.70      0.61      0.65      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.68      0.58      0.62      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1129 | val acc:\t0.6315\n",
      "\n",
      "Epoch 116/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8392 | train acc:\t0.6797\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.71      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.69      0.56      0.62      1174\n",
      "           3       0.66      0.62      0.64      1778\n",
      "           4       0.65      0.57      0.61      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0999 | val acc:\t0.6319\n",
      "\n",
      "Epoch 117/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8453 | train acc:\t0.6749\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.73      0.61      0.67      1671\n",
      "           2       0.68      0.58      0.62      1174\n",
      "           3       0.64      0.63      0.63      1778\n",
      "           4       0.64      0.53      0.58      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.68      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1536 | val acc:\t0.6426\n",
      "\n",
      "Epoch 118/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8351 | train acc:\t0.6810\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.71      3943\n",
      "           1       0.73      0.62      0.67      1671\n",
      "           2       0.70      0.55      0.62      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.65      0.57      0.61      1329\n",
      "           5       0.82      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1307 | val acc:\t0.6282\n",
      "\n",
      "Epoch 119/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8303 | train acc:\t0.6857\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.73      0.59      0.65      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.66      0.57      0.62      1329\n",
      "           5       0.83      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.1328 | val acc:\t0.6291\n",
      "\n",
      "Epoch 120/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8313 | train acc:\t0.6804\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.70      0.62      0.66      1671\n",
      "           2       0.69      0.56      0.62      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.65      0.56      0.60      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1351 | val acc:\t0.6273\n",
      "\n",
      "Epoch 121/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.8373 | train acc:\t0.6791\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.71      0.61      0.66      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.65      0.55      0.59      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1203 | val acc:\t0.6266\n",
      "\n",
      "Epoch 122/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8372 | train acc:\t0.6822\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.70      0.59      0.64      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.64      0.55      0.59      1329\n",
      "           5       0.79      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1414 | val acc:\t0.6294\n",
      "\n",
      "Epoch 123/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8327 | train acc:\t0.6794\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.71      0.61      0.66      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.64      0.63      0.64      1778\n",
      "           4       0.66      0.55      0.60      1329\n",
      "           5       0.82      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0945 | val acc:\t0.6165\n",
      "\n",
      "Epoch 124/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8389 | train acc:\t0.6791\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.69      0.55      0.62      1174\n",
      "           3       0.64      0.61      0.62      1778\n",
      "           4       0.65      0.57      0.61      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1159 | val acc:\t0.6199\n",
      "\n",
      "Epoch 125/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8284 | train acc:\t0.6832\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.69      0.57      0.63      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1329 | val acc:\t0.6297\n",
      "\n",
      "Epoch 126/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8306 | train acc:\t0.6848\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.72      3943\n",
      "           1       0.71      0.61      0.66      1671\n",
      "           2       0.72      0.58      0.64      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.66      0.56      0.61      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.69      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1264 | val acc:\t0.6395\n",
      "\n",
      "Epoch 127/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8251 | train acc:\t0.6878\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.65      0.62      0.63      1778\n",
      "           4       0.67      0.58      0.62      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1115 | val acc:\t0.6236\n",
      "\n",
      "Epoch 128/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8239 | train acc:\t0.6836\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71      3943\n",
      "           1       0.73      0.62      0.67      1671\n",
      "           2       0.72      0.59      0.65      1174\n",
      "           3       0.65      0.62      0.63      1778\n",
      "           4       0.64      0.54      0.59      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.69      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1374 | val acc:\t0.6423\n",
      "\n",
      "Epoch 129/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8314 | train acc:\t0.6814\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.76      0.71      3943\n",
      "           1       0.71      0.63      0.66      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.65      0.62      0.63      1778\n",
      "           4       0.64      0.59      0.61      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1359 | val acc:\t0.6386\n",
      "\n",
      "Epoch 130/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8331 | train acc:\t0.6849\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.71      0.61      0.66      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.66      0.63      0.64      1778\n",
      "           4       0.65      0.59      0.62      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1349 | val acc:\t0.6300\n",
      "\n",
      "Epoch 131/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8164 | train acc:\t0.6938\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.74      0.63      0.68      1671\n",
      "           2       0.70      0.58      0.63      1174\n",
      "           3       0.64      0.63      0.64      1778\n",
      "           4       0.69      0.59      0.63      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.70      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1397 | val acc:\t0.6270\n",
      "\n",
      "Epoch 132/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8213 | train acc:\t0.6862\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.71      0.62      0.67      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.64      0.58      0.61      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.1072 | val acc:\t0.6337\n",
      "\n",
      "Epoch 133/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.8143 | train acc:\t0.6940\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.73      0.63      0.67      1671\n",
      "           2       0.71      0.59      0.65      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.68      0.58      0.62      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.0997 | val acc:\t0.6358\n",
      "\n",
      "Epoch 134/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8293 | train acc:\t0.6805\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.70      0.61      0.66      1671\n",
      "           2       0.72      0.58      0.64      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.65      0.57      0.61      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1243 | val acc:\t0.6328\n",
      "\n",
      "Epoch 135/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8153 | train acc:\t0.6927\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.73      0.63      0.68      1671\n",
      "           2       0.73      0.61      0.67      1174\n",
      "           3       0.66      0.65      0.65      1778\n",
      "           4       0.65      0.59      0.62      1329\n",
      "           5       0.80      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1129 | val acc:\t0.6303\n",
      "\n",
      "Epoch 136/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8169 | train acc:\t0.6895\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.73      0.63      0.68      1671\n",
      "           2       0.69      0.56      0.62      1174\n",
      "           3       0.67      0.66      0.67      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1626 | val acc:\t0.6349\n",
      "\n",
      "Epoch 137/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8183 | train acc:\t0.6881\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.79      0.72      3943\n",
      "           1       0.73      0.62      0.67      1671\n",
      "           2       0.70      0.56      0.63      1174\n",
      "           3       0.67      0.62      0.64      1778\n",
      "           4       0.66      0.57      0.61      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1303 | val acc:\t0.6230\n",
      "\n",
      "Epoch 138/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8150 | train acc:\t0.6925\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.68      0.64      0.66      1778\n",
      "           4       0.66      0.57      0.61      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1395 | val acc:\t0.6260\n",
      "\n",
      "Epoch 139/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8260 | train acc:\t0.6856\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71      3943\n",
      "           1       0.71      0.63      0.66      1671\n",
      "           2       0.71      0.57      0.63      1174\n",
      "           3       0.66      0.62      0.64      1778\n",
      "           4       0.67      0.59      0.63      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.1575 | val acc:\t0.6282\n",
      "\n",
      "Epoch 140/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8134 | train acc:\t0.6847\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.71      3943\n",
      "           1       0.70      0.63      0.66      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.65      0.62      0.64      1778\n",
      "           4       0.66      0.58      0.62      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1251 | val acc:\t0.6285\n",
      "\n",
      "Epoch 141/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8139 | train acc:\t0.6928\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.74      0.64      0.68      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.66      0.63      0.64      1778\n",
      "           4       0.65      0.58      0.61      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1251 | val acc:\t0.6297\n",
      "\n",
      "Epoch 142/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8181 | train acc:\t0.6870\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.71      0.63      0.67      1671\n",
      "           2       0.71      0.57      0.63      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.1357 | val acc:\t0.6217\n",
      "\n",
      "Epoch 143/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8126 | train acc:\t0.6908\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.72      3943\n",
      "           1       0.73      0.62      0.67      1671\n",
      "           2       0.72      0.60      0.65      1174\n",
      "           3       0.66      0.63      0.65      1778\n",
      "           4       0.67      0.58      0.62      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1808 | val acc:\t0.6236\n",
      "\n",
      "Epoch 144/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8132 | train acc:\t0.6922\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.73      0.62      0.67      1671\n",
      "           2       0.71      0.61      0.66      1174\n",
      "           3       0.65      0.64      0.65      1778\n",
      "           4       0.67      0.57      0.62      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1177 | val acc:\t0.6245\n",
      "\n",
      "Epoch 145/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.8132 | train acc:\t0.6895\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.76      0.71      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.66      0.66      0.66      1778\n",
      "           4       0.67      0.59      0.63      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1423 | val acc:\t0.6266\n",
      "\n",
      "Epoch 146/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8138 | train acc:\t0.6914\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.70      0.60      0.65      1174\n",
      "           3       0.65      0.64      0.65      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.81      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.68      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1211 | val acc:\t0.6386\n",
      "\n",
      "Epoch 147/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8173 | train acc:\t0.6860\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.71      0.59      0.64      1174\n",
      "           3       0.64      0.63      0.64      1778\n",
      "           4       0.65      0.58      0.61      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.1445 | val acc:\t0.6334\n",
      "\n",
      "Epoch 148/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8134 | train acc:\t0.6877\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71      3943\n",
      "           1       0.73      0.62      0.67      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.66      0.63      0.65      1778\n",
      "           4       0.66      0.57      0.61      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.0951 | val acc:\t0.6300\n",
      "\n",
      "Epoch 149/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8162 | train acc:\t0.6871\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.70      0.59      0.64      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.65      0.56      0.60      1329\n",
      "           5       0.81      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1355 | val acc:\t0.6221\n",
      "\n",
      "Epoch 150/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8025 | train acc:\t0.6978\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.75      0.64      0.69      1671\n",
      "           2       0.72      0.60      0.65      1174\n",
      "           3       0.65      0.64      0.65      1778\n",
      "           4       0.67      0.58      0.62      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1158 | val acc:\t0.6242\n",
      "\n",
      "Epoch 151/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8197 | train acc:\t0.6868\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.70      0.59      0.64      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.66      0.56      0.61      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.1569 | val acc:\t0.6230\n",
      "\n",
      "Epoch 152/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8129 | train acc:\t0.6914\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.72      0.59      0.65      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.66      0.57      0.61      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1307 | val acc:\t0.6297\n",
      "\n",
      "Epoch 153/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8278 | train acc:\t0.6858\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.66      0.63      0.64      1778\n",
      "           4       0.67      0.58      0.62      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.1319 | val acc:\t0.6346\n",
      "\n",
      "Epoch 154/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8013 | train acc:\t0.6975\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.72      3943\n",
      "           1       0.72      0.63      0.68      1671\n",
      "           2       0.70      0.60      0.65      1174\n",
      "           3       0.67      0.64      0.65      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1034 | val acc:\t0.6202\n",
      "\n",
      "Epoch 155/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8083 | train acc:\t0.6902\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.73      0.63      0.68      1671\n",
      "           2       0.69      0.59      0.64      1174\n",
      "           3       0.66      0.62      0.64      1778\n",
      "           4       0.66      0.57      0.61      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1102 | val acc:\t0.6398\n",
      "\n",
      "Epoch 156/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7936 | train acc:\t0.7001\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.72      3943\n",
      "           1       0.72      0.64      0.68      1671\n",
      "           2       0.73      0.61      0.67      1174\n",
      "           3       0.66      0.65      0.65      1778\n",
      "           4       0.67      0.58      0.62      1329\n",
      "           5       0.84      0.85      0.85      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1328 | val acc:\t0.6368\n",
      "\n",
      "Epoch 157/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.7984 | train acc:\t0.6925\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.65      0.64      0.64      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1212 | val acc:\t0.6325\n",
      "\n",
      "Epoch 158/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8178 | train acc:\t0.6901\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.72      0.63      0.68      1671\n",
      "           2       0.70      0.59      0.64      1174\n",
      "           3       0.65      0.64      0.65      1778\n",
      "           4       0.66      0.57      0.61      1329\n",
      "           5       0.81      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1154 | val acc:\t0.6343\n",
      "\n",
      "Epoch 159/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8040 | train acc:\t0.6941\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.72      0.65      0.68      1671\n",
      "           2       0.72      0.58      0.64      1174\n",
      "           3       0.66      0.63      0.65      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.70      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1452 | val acc:\t0.6349\n",
      "\n",
      "Epoch 160/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8081 | train acc:\t0.6909\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.71      0.63      0.67      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.67      0.57      0.61      1329\n",
      "           5       0.81      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1184 | val acc:\t0.6401\n",
      "\n",
      "Epoch 161/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8165 | train acc:\t0.6921\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.67      0.64      0.65      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.82      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1332 | val acc:\t0.6374\n",
      "\n",
      "Epoch 162/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7971 | train acc:\t0.6981\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      3943\n",
      "           1       0.72      0.64      0.68      1671\n",
      "           2       0.71      0.61      0.66      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.67      0.59      0.63      1329\n",
      "           5       0.83      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1273 | val acc:\t0.6202\n",
      "\n",
      "Epoch 163/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8149 | train acc:\t0.6873\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.71      0.61      0.66      1671\n",
      "           2       0.70      0.58      0.63      1174\n",
      "           3       0.66      0.63      0.64      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1215 | val acc:\t0.6364\n",
      "\n",
      "Epoch 164/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8116 | train acc:\t0.6925\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.72      0.59      0.65      1174\n",
      "           3       0.67      0.64      0.65      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1306 | val acc:\t0.6294\n",
      "\n",
      "Epoch 165/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8063 | train acc:\t0.6927\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.72      0.64      0.68      1671\n",
      "           2       0.71      0.59      0.64      1174\n",
      "           3       0.67      0.64      0.65      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1386 | val acc:\t0.6309\n",
      "\n",
      "Epoch 166/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8012 | train acc:\t0.6961\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.69      0.59      0.64      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.68      0.59      0.63      1329\n",
      "           5       0.83      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.69     10896\n",
      "\n",
      "val loss:\t1.1477 | val acc:\t0.6297\n",
      "\n",
      "Epoch 167/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7890 | train acc:\t0.7009\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.72      0.59      0.65      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.68      0.59      0.63      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1213 | val acc:\t0.6355\n",
      "\n",
      "Epoch 168/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7948 | train acc:\t0.6988\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.72      3943\n",
      "           1       0.74      0.63      0.68      1671\n",
      "           2       0.71      0.61      0.66      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.67      0.60      0.64      1329\n",
      "           5       0.83      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1255 | val acc:\t0.6285\n",
      "\n",
      "Epoch 169/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.7992 | train acc:\t0.6997\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73      3943\n",
      "           1       0.74      0.63      0.68      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.67      0.64      0.65      1778\n",
      "           4       0.67      0.61      0.64      1329\n",
      "           5       0.82      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1198 | val acc:\t0.6343\n",
      "\n",
      "Epoch 170/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7968 | train acc:\t0.6970\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.72      0.64      0.68      1671\n",
      "           2       0.70      0.59      0.64      1174\n",
      "           3       0.68      0.64      0.66      1778\n",
      "           4       0.68      0.61      0.64      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1021 | val acc:\t0.6199\n",
      "\n",
      "Epoch 171/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8098 | train acc:\t0.6952\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.72      0.64      0.68      1671\n",
      "           2       0.70      0.59      0.64      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.69     10896\n",
      "\n",
      "val loss:\t1.0955 | val acc:\t0.6349\n",
      "\n",
      "Epoch 172/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8154 | train acc:\t0.6886\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.72      3943\n",
      "           1       0.71      0.63      0.67      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.66      0.63      0.64      1778\n",
      "           4       0.68      0.59      0.63      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1384 | val acc:\t0.6325\n",
      "\n",
      "Epoch 173/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7954 | train acc:\t0.6922\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.71      0.63      0.67      1671\n",
      "           2       0.72      0.59      0.65      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.66      0.58      0.62      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1304 | val acc:\t0.6306\n",
      "\n",
      "Epoch 174/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8038 | train acc:\t0.7004\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.79      0.73      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.67      0.63      0.65      1778\n",
      "           4       0.70      0.59      0.64      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1088 | val acc:\t0.6297\n",
      "\n",
      "Epoch 175/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7865 | train acc:\t0.7025\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.73      0.63      0.68      1671\n",
      "           2       0.72      0.59      0.65      1174\n",
      "           3       0.67      0.66      0.67      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1340 | val acc:\t0.6331\n",
      "\n",
      "Epoch 176/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7973 | train acc:\t0.6949\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.71      0.63      0.67      1671\n",
      "           2       0.71      0.59      0.65      1174\n",
      "           3       0.67      0.66      0.66      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1254 | val acc:\t0.6199\n",
      "\n",
      "Epoch 177/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8025 | train acc:\t0.6931\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.71      0.64      0.68      1671\n",
      "           2       0.70      0.58      0.63      1174\n",
      "           3       0.67      0.64      0.65      1778\n",
      "           4       0.67      0.59      0.63      1329\n",
      "           5       0.83      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1555 | val acc:\t0.6276\n",
      "\n",
      "Epoch 178/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7880 | train acc:\t0.6945\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.72      0.64      0.68      1671\n",
      "           2       0.72      0.59      0.65      1174\n",
      "           3       0.66      0.63      0.65      1778\n",
      "           4       0.67      0.59      0.63      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1328 | val acc:\t0.6297\n",
      "\n",
      "Epoch 179/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8014 | train acc:\t0.6914\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.71      0.63      0.66      1671\n",
      "           2       0.70      0.59      0.64      1174\n",
      "           3       0.68      0.63      0.65      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.81      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1354 | val acc:\t0.6343\n",
      "\n",
      "Epoch 180/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7924 | train acc:\t0.7006\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.73      0.65      0.69      1671\n",
      "           2       0.69      0.58      0.63      1174\n",
      "           3       0.68      0.66      0.67      1778\n",
      "           4       0.68      0.60      0.64      1329\n",
      "           5       0.82      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1157 | val acc:\t0.6285\n",
      "\n",
      "Epoch 181/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.7977 | train acc:\t0.6957\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.71      0.59      0.64      1174\n",
      "           3       0.67      0.64      0.65      1778\n",
      "           4       0.67      0.58      0.62      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.69     10896\n",
      "\n",
      "val loss:\t1.1219 | val acc:\t0.6239\n",
      "\n",
      "Epoch 182/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7921 | train acc:\t0.6998\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.72      3943\n",
      "           1       0.73      0.65      0.69      1671\n",
      "           2       0.73      0.60      0.66      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.67      0.59      0.63      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1480 | val acc:\t0.6227\n",
      "\n",
      "Epoch 183/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8047 | train acc:\t0.6933\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.79      0.72      3943\n",
      "           1       0.71      0.63      0.67      1671\n",
      "           2       0.70      0.58      0.63      1174\n",
      "           3       0.67      0.62      0.65      1778\n",
      "           4       0.68      0.58      0.63      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1454 | val acc:\t0.6340\n",
      "\n",
      "Epoch 184/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7862 | train acc:\t0.7044\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.75      0.65      0.70      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.69      0.65      0.67      1778\n",
      "           4       0.69      0.61      0.64      1329\n",
      "           5       0.80      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1355 | val acc:\t0.6300\n",
      "\n",
      "Epoch 185/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8027 | train acc:\t0.6962\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.79      0.73      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.74      0.59      0.66      1174\n",
      "           3       0.66      0.62      0.64      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.82      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.69     10896\n",
      "\n",
      "val loss:\t1.1317 | val acc:\t0.6312\n",
      "\n",
      "Epoch 186/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7929 | train acc:\t0.6980\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.73      0.65      0.69      1671\n",
      "           2       0.72      0.59      0.65      1174\n",
      "           3       0.67      0.64      0.65      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1339 | val acc:\t0.6242\n",
      "\n",
      "Epoch 187/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7914 | train acc:\t0.7018\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.73      0.65      0.68      1671\n",
      "           2       0.73      0.61      0.66      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.68      0.60      0.64      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1174 | val acc:\t0.6260\n",
      "\n",
      "Epoch 188/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7926 | train acc:\t0.6985\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.79      0.73      3943\n",
      "           1       0.73      0.63      0.68      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.66      0.57      0.61      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1459 | val acc:\t0.6374\n",
      "\n",
      "Epoch 189/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7915 | train acc:\t0.7027\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.72      3943\n",
      "           1       0.75      0.64      0.69      1671\n",
      "           2       0.72      0.60      0.65      1174\n",
      "           3       0.67      0.66      0.67      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1260 | val acc:\t0.6043\n",
      "\n",
      "Epoch 190/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7876 | train acc:\t0.7044\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.73      0.66      0.69      1671\n",
      "           2       0.72      0.60      0.65      1174\n",
      "           3       0.68      0.66      0.67      1778\n",
      "           4       0.68      0.60      0.64      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1363 | val acc:\t0.6312\n",
      "\n",
      "Epoch 191/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7913 | train acc:\t0.7008\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.72      0.65      0.68      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.67      0.63      0.65      1778\n",
      "           4       0.69      0.60      0.64      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1025 | val acc:\t0.6300\n",
      "\n",
      "Epoch 192/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8011 | train acc:\t0.6997\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.73      0.63      0.68      1671\n",
      "           2       0.72      0.60      0.65      1174\n",
      "           3       0.66      0.65      0.66      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1235 | val acc:\t0.6248\n",
      "\n",
      "Epoch 193/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.7934 | train acc:\t0.6997\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.74      0.60      0.66      1174\n",
      "           3       0.68      0.65      0.67      1778\n",
      "           4       0.67      0.61      0.64      1329\n",
      "           5       0.82      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1201 | val acc:\t0.6441\n",
      "\n",
      "Epoch 194/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7882 | train acc:\t0.6996\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.72      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.71      0.59      0.65      1174\n",
      "           3       0.68      0.64      0.66      1778\n",
      "           4       0.68      0.60      0.64      1329\n",
      "           5       0.82      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1357 | val acc:\t0.6300\n",
      "\n",
      "Epoch 195/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7922 | train acc:\t0.7013\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.72      3943\n",
      "           1       0.74      0.64      0.69      1671\n",
      "           2       0.71      0.62      0.66      1174\n",
      "           3       0.67      0.64      0.65      1778\n",
      "           4       0.69      0.60      0.65      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1495 | val acc:\t0.6242\n",
      "\n",
      "Epoch 196/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7850 | train acc:\t0.7045\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.74      0.65      0.69      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.69      0.66      0.67      1778\n",
      "           4       0.67      0.61      0.64      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1518 | val acc:\t0.6217\n",
      "\n",
      "Epoch 197/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7874 | train acc:\t0.6999\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.74      0.65      0.69      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.67      0.59      0.63      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1269 | val acc:\t0.6334\n",
      "\n",
      "Epoch 198/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7960 | train acc:\t0.6940\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.71      0.59      0.65      1174\n",
      "           3       0.66      0.63      0.64      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.83      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1484 | val acc:\t0.6190\n",
      "\n",
      "Epoch 199/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7858 | train acc:\t0.7023\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.72      0.65      0.68      1671\n",
      "           2       0.72      0.60      0.65      1174\n",
      "           3       0.67      0.66      0.66      1778\n",
      "           4       0.68      0.59      0.63      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1336 | val acc:\t0.6312\n",
      "\n",
      "Epoch 200/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7881 | train acc:\t0.6989\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.72      0.64      0.68      1671\n",
      "           2       0.69      0.58      0.63      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.66      0.60      0.63      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1208 | val acc:\t0.6303\n",
      "\n",
      "Training complete in 30m 10s\n",
      "Best val acc: 0.645636\n",
      "\t conv1.weight\n",
      "\t conv1.bias\n",
      "\t conv2.weight\n",
      "\t conv2.bias\n",
      "\t conv3.weight\n",
      "\t conv3.bias\n",
      "\t linear1.weight\n",
      "\t linear1.bias\n",
      "\t linear2.weight\n",
      "\t linear2.bias\n",
      "Epoch 1/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.5495 | train acc:\t0.3905\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.86      0.54      3943\n",
      "           1       0.34      0.12      0.18      1671\n",
      "           2       0.19      0.01      0.01      1174\n",
      "           3       0.33      0.20      0.25      1778\n",
      "           4       0.19      0.01      0.01      1329\n",
      "           5       0.52      0.29      0.37      1001\n",
      "\n",
      "    accuracy                           0.39     10896\n",
      "   macro avg       0.33      0.25      0.23     10896\n",
      "weighted avg       0.34      0.39      0.30     10896\n",
      "\n",
      "val loss:\t1.5744 | val acc:\t0.4864\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.81      0.65      1518\n",
      "           1       0.57      0.06      0.10       468\n",
      "           2       0.58      0.04      0.08       321\n",
      "           3       0.35      0.25      0.29       397\n",
      "           4       0.45      0.03      0.05       309\n",
      "           5       0.34      0.82      0.48       252\n",
      "\n",
      "    accuracy                           0.49      3265\n",
      "   macro avg       0.47      0.34      0.28      3265\n",
      "weighted avg       0.50      0.49      0.40      3265\n",
      "\n",
      "Epoch 2/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3943 | train acc:\t0.4504\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.74      0.56      3943\n",
      "           1       0.43      0.34      0.38      1671\n",
      "           2       0.37      0.03      0.06      1174\n",
      "           3       0.37      0.35      0.36      1778\n",
      "           4       0.41      0.12      0.19      1329\n",
      "           5       0.58      0.61      0.60      1001\n",
      "\n",
      "    accuracy                           0.45     10896\n",
      "   macro avg       0.44      0.36      0.36     10896\n",
      "weighted avg       0.44      0.45      0.40     10896\n",
      "\n",
      "val loss:\t1.4447 | val acc:\t0.5280\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      1518\n",
      "           1       0.34      0.56      0.42       468\n",
      "           2       0.89      0.02      0.05       321\n",
      "           3       0.37      0.18      0.24       397\n",
      "           4       0.39      0.16      0.22       309\n",
      "           5       0.73      0.74      0.74       252\n",
      "\n",
      "    accuracy                           0.53      3265\n",
      "   macro avg       0.55      0.40      0.39      3265\n",
      "weighted avg       0.55      0.53      0.48      3265\n",
      "\n",
      "Epoch 3/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3439 | train acc:\t0.4712\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.74      0.57      3943\n",
      "           1       0.49      0.35      0.41      1671\n",
      "           2       0.46      0.08      0.13      1174\n",
      "           3       0.40      0.38      0.39      1778\n",
      "           4       0.41      0.19      0.26      1329\n",
      "           5       0.62      0.63      0.62      1001\n",
      "\n",
      "    accuracy                           0.47     10896\n",
      "   macro avg       0.48      0.39      0.40     10896\n",
      "weighted avg       0.47      0.47      0.44     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.4503 | val acc:\t0.5568\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.83      0.69      1518\n",
      "           1       0.44      0.45      0.44       468\n",
      "           2       0.85      0.03      0.07       321\n",
      "           3       0.39      0.30      0.34       397\n",
      "           4       0.47      0.14      0.22       309\n",
      "           5       0.84      0.67      0.74       252\n",
      "\n",
      "    accuracy                           0.56      3265\n",
      "   macro avg       0.59      0.40      0.42      3265\n",
      "weighted avg       0.57      0.56      0.51      3265\n",
      "\n",
      "Epoch 4/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3013 | train acc:\t0.4915\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.74      0.58      3943\n",
      "           1       0.51      0.38      0.44      1671\n",
      "           2       0.50      0.11      0.18      1174\n",
      "           3       0.43      0.39      0.41      1778\n",
      "           4       0.44      0.24      0.31      1329\n",
      "           5       0.65      0.66      0.65      1001\n",
      "\n",
      "    accuracy                           0.49     10896\n",
      "   macro avg       0.50      0.42      0.43     10896\n",
      "weighted avg       0.49      0.49      0.46     10896\n",
      "\n",
      "val loss:\t1.3917 | val acc:\t0.5479\n",
      "\n",
      "Epoch 5/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2844 | train acc:\t0.4945\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.73      0.58      3943\n",
      "           1       0.54      0.40      0.46      1671\n",
      "           2       0.50      0.13      0.21      1174\n",
      "           3       0.43      0.39      0.41      1778\n",
      "           4       0.41      0.26      0.32      1329\n",
      "           5       0.67      0.67      0.67      1001\n",
      "\n",
      "    accuracy                           0.49     10896\n",
      "   macro avg       0.51      0.43      0.44     10896\n",
      "weighted avg       0.49      0.49      0.47     10896\n",
      "\n",
      "val loss:\t1.3945 | val acc:\t0.5694\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.85      0.70      1518\n",
      "           1       0.60      0.19      0.29       468\n",
      "           2       0.91      0.03      0.06       321\n",
      "           3       0.40      0.50      0.44       397\n",
      "           4       0.43      0.35      0.38       309\n",
      "           5       0.85      0.65      0.74       252\n",
      "\n",
      "    accuracy                           0.57      3265\n",
      "   macro avg       0.63      0.43      0.44      3265\n",
      "weighted avg       0.61      0.57      0.52      3265\n",
      "\n",
      "Epoch 6/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2482 | train acc:\t0.5093\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.72      0.59      3943\n",
      "           1       0.55      0.40      0.46      1671\n",
      "           2       0.50      0.18      0.27      1174\n",
      "           3       0.44      0.42      0.43      1778\n",
      "           4       0.45      0.29      0.35      1329\n",
      "           5       0.68      0.69      0.68      1001\n",
      "\n",
      "    accuracy                           0.51     10896\n",
      "   macro avg       0.52      0.45      0.47     10896\n",
      "weighted avg       0.51      0.51      0.49     10896\n",
      "\n",
      "val loss:\t1.3852 | val acc:\t0.5538\n",
      "\n",
      "Epoch 7/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2541 | train acc:\t0.5111\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.72      0.59      3943\n",
      "           1       0.57      0.38      0.46      1671\n",
      "           2       0.53      0.19      0.28      1174\n",
      "           3       0.45      0.44      0.44      1778\n",
      "           4       0.46      0.30      0.36      1329\n",
      "           5       0.66      0.68      0.67      1001\n",
      "\n",
      "    accuracy                           0.51     10896\n",
      "   macro avg       0.53      0.45      0.47     10896\n",
      "weighted avg       0.52      0.51      0.49     10896\n",
      "\n",
      "val loss:\t1.3709 | val acc:\t0.5614\n",
      "\n",
      "Epoch 8/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2244 | train acc:\t0.5195\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.72      0.59      3943\n",
      "           1       0.57      0.40      0.47      1671\n",
      "           2       0.53      0.21      0.30      1174\n",
      "           3       0.45      0.45      0.45      1778\n",
      "           4       0.46      0.29      0.36      1329\n",
      "           5       0.71      0.71      0.71      1001\n",
      "\n",
      "    accuracy                           0.52     10896\n",
      "   macro avg       0.54      0.46      0.48     10896\n",
      "weighted avg       0.52      0.52      0.50     10896\n",
      "\n",
      "val loss:\t1.3794 | val acc:\t0.5737\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.84      0.69      1518\n",
      "           1       0.48      0.44      0.46       468\n",
      "           2       0.51      0.30      0.38       321\n",
      "           3       0.43      0.26      0.32       397\n",
      "           4       0.60      0.09      0.16       309\n",
      "           5       0.86      0.67      0.75       252\n",
      "\n",
      "    accuracy                           0.57      3265\n",
      "   macro avg       0.58      0.43      0.46      3265\n",
      "weighted avg       0.57      0.57      0.54      3265\n",
      "\n",
      "Epoch 9/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2050 | train acc:\t0.5276\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.72      0.60      3943\n",
      "           1       0.60      0.42      0.50      1671\n",
      "           2       0.54      0.23      0.32      1174\n",
      "           3       0.46      0.44      0.45      1778\n",
      "           4       0.47      0.33      0.39      1329\n",
      "           5       0.70      0.72      0.71      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.55      0.48      0.49     10896\n",
      "weighted avg       0.53      0.53      0.51     10896\n",
      "\n",
      "val loss:\t1.3207 | val acc:\t0.5810\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.84      0.70      1518\n",
      "           1       0.54      0.35      0.43       468\n",
      "           2       0.53      0.29      0.37       321\n",
      "           3       0.43      0.30      0.36       397\n",
      "           4       0.46      0.21      0.29       309\n",
      "           5       0.87      0.68      0.76       252\n",
      "\n",
      "    accuracy                           0.58      3265\n",
      "   macro avg       0.57      0.45      0.48      3265\n",
      "weighted avg       0.57      0.58      0.55      3265\n",
      "\n",
      "Epoch 10/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2035 | train acc:\t0.5277\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.73      0.60      3943\n",
      "           1       0.59      0.42      0.49      1671\n",
      "           2       0.51      0.24      0.32      1174\n",
      "           3       0.47      0.43      0.45      1778\n",
      "           4       0.47      0.32      0.38      1329\n",
      "           5       0.70      0.71      0.71      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.54      0.47      0.49     10896\n",
      "weighted avg       0.53      0.53      0.51     10896\n",
      "\n",
      "val loss:\t1.3562 | val acc:\t0.5746\n",
      "\n",
      "Epoch 11/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1864 | train acc:\t0.5349\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.72      0.60      3943\n",
      "           1       0.60      0.41      0.48      1671\n",
      "           2       0.55      0.26      0.35      1174\n",
      "           3       0.47      0.45      0.46      1778\n",
      "           4       0.49      0.34      0.40      1329\n",
      "           5       0.71      0.74      0.72      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.56      0.49      0.50     10896\n",
      "weighted avg       0.54      0.53      0.52     10896\n",
      "\n",
      "val loss:\t1.3696 | val acc:\t0.5703\n",
      "\n",
      "Epoch 12/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1849 | train acc:\t0.5380\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.73      0.60      3943\n",
      "           1       0.59      0.43      0.50      1671\n",
      "           2       0.55      0.26      0.35      1174\n",
      "           3       0.48      0.45      0.47      1778\n",
      "           4       0.49      0.34      0.40      1329\n",
      "           5       0.72      0.73      0.72      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.56      0.49      0.51     10896\n",
      "weighted avg       0.54      0.54      0.52     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.3025 | val acc:\t0.5761\n",
      "\n",
      "Epoch 13/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1665 | train acc:\t0.5473\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.73      0.61      3943\n",
      "           1       0.62      0.41      0.50      1671\n",
      "           2       0.58      0.29      0.39      1174\n",
      "           3       0.49      0.48      0.49      1778\n",
      "           4       0.49      0.34      0.40      1329\n",
      "           5       0.72      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.57      0.50      0.52     10896\n",
      "weighted avg       0.55      0.55      0.53     10896\n",
      "\n",
      "val loss:\t1.2766 | val acc:\t0.5752\n",
      "\n",
      "Epoch 14/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1545 | train acc:\t0.5482\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.72      0.61      3943\n",
      "           1       0.61      0.43      0.50      1671\n",
      "           2       0.56      0.30      0.39      1174\n",
      "           3       0.50      0.46      0.48      1778\n",
      "           4       0.51      0.37      0.43      1329\n",
      "           5       0.71      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.57      0.50      0.52     10896\n",
      "weighted avg       0.55      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.2979 | val acc:\t0.5807\n",
      "\n",
      "Epoch 15/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1455 | train acc:\t0.5526\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.61      3943\n",
      "           1       0.62      0.44      0.52      1671\n",
      "           2       0.57      0.29      0.39      1174\n",
      "           3       0.49      0.46      0.48      1778\n",
      "           4       0.50      0.35      0.41      1329\n",
      "           5       0.72      0.76      0.74      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.57      0.51      0.53     10896\n",
      "weighted avg       0.56      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.2825 | val acc:\t0.5789\n",
      "\n",
      "Epoch 16/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1468 | train acc:\t0.5540\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.61      3943\n",
      "           1       0.62      0.45      0.52      1671\n",
      "           2       0.58      0.31      0.41      1174\n",
      "           3       0.50      0.47      0.48      1778\n",
      "           4       0.51      0.37      0.43      1329\n",
      "           5       0.72      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.58      0.51      0.53     10896\n",
      "weighted avg       0.56      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.3061 | val acc:\t0.5770\n",
      "\n",
      "Epoch 17/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1449 | train acc:\t0.5538\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.61      3943\n",
      "           1       0.63      0.42      0.51      1671\n",
      "           2       0.58      0.32      0.41      1174\n",
      "           3       0.50      0.47      0.48      1778\n",
      "           4       0.52      0.37      0.43      1329\n",
      "           5       0.72      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.58      0.51      0.53     10896\n",
      "weighted avg       0.56      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.2847 | val acc:\t0.5819\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.90      0.70      1518\n",
      "           1       0.58      0.29      0.39       468\n",
      "           2       0.66      0.18      0.29       321\n",
      "           3       0.44      0.28      0.34       397\n",
      "           4       0.62      0.17      0.27       309\n",
      "           5       0.82      0.71      0.76       252\n",
      "\n",
      "    accuracy                           0.58      3265\n",
      "   macro avg       0.61      0.42      0.46      3265\n",
      "weighted avg       0.59      0.58      0.53      3265\n",
      "\n",
      "Epoch 18/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1399 | train acc:\t0.5583\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.62      3943\n",
      "           1       0.60      0.44      0.51      1671\n",
      "           2       0.61      0.32      0.42      1174\n",
      "           3       0.50      0.47      0.48      1778\n",
      "           4       0.49      0.37      0.42      1329\n",
      "           5       0.73      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.51      0.53     10896\n",
      "weighted avg       0.56      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.2392 | val acc:\t0.5856\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.90      0.70      1518\n",
      "           1       0.60      0.32      0.41       468\n",
      "           2       0.59      0.25      0.35       321\n",
      "           3       0.47      0.26      0.34       397\n",
      "           4       0.56      0.10      0.17       309\n",
      "           5       0.84      0.72      0.78       252\n",
      "\n",
      "    accuracy                           0.59      3265\n",
      "   macro avg       0.61      0.43      0.46      3265\n",
      "weighted avg       0.58      0.59      0.54      3265\n",
      "\n",
      "Epoch 19/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1257 | train acc:\t0.5584\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.61      3943\n",
      "           1       0.62      0.44      0.51      1671\n",
      "           2       0.56      0.32      0.40      1174\n",
      "           3       0.52      0.48      0.50      1778\n",
      "           4       0.51      0.35      0.42      1329\n",
      "           5       0.73      0.78      0.75      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.52      0.53     10896\n",
      "weighted avg       0.56      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.2877 | val acc:\t0.5920\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.89      0.70      1518\n",
      "           1       0.61      0.31      0.41       468\n",
      "           2       0.64      0.21      0.32       321\n",
      "           3       0.50      0.35      0.41       397\n",
      "           4       0.53      0.19      0.28       309\n",
      "           5       0.89      0.65      0.75       252\n",
      "\n",
      "    accuracy                           0.59      3265\n",
      "   macro avg       0.62      0.43      0.48      3265\n",
      "weighted avg       0.60      0.59      0.55      3265\n",
      "\n",
      "Epoch 20/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1198 | train acc:\t0.5607\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.72      0.61      3943\n",
      "           1       0.61      0.45      0.52      1671\n",
      "           2       0.61      0.34      0.44      1174\n",
      "           3       0.50      0.47      0.49      1778\n",
      "           4       0.54      0.40      0.46      1329\n",
      "           5       0.73      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.59      0.52      0.54     10896\n",
      "weighted avg       0.57      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.2575 | val acc:\t0.5945\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.89      0.71      1518\n",
      "           1       0.55      0.39      0.46       468\n",
      "           2       0.63      0.25      0.36       321\n",
      "           3       0.46      0.22      0.30       397\n",
      "           4       0.58      0.19      0.29       309\n",
      "           5       0.84      0.73      0.78       252\n",
      "\n",
      "    accuracy                           0.59      3265\n",
      "   macro avg       0.61      0.45      0.48      3265\n",
      "weighted avg       0.59      0.59      0.55      3265\n",
      "\n",
      "Epoch 21/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1182 | train acc:\t0.5631\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.72      0.62      3943\n",
      "           1       0.61      0.44      0.51      1671\n",
      "           2       0.59      0.33      0.42      1174\n",
      "           3       0.52      0.48      0.50      1778\n",
      "           4       0.53      0.40      0.45      1329\n",
      "           5       0.74      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.59      0.52      0.54     10896\n",
      "weighted avg       0.57      0.56      0.55     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.2737 | val acc:\t0.5841\n",
      "\n",
      "Epoch 22/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1080 | train acc:\t0.5721\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63      3943\n",
      "           1       0.63      0.46      0.53      1671\n",
      "           2       0.61      0.33      0.43      1174\n",
      "           3       0.51      0.49      0.50      1778\n",
      "           4       0.52      0.40      0.45      1329\n",
      "           5       0.75      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.60      0.53      0.55     10896\n",
      "weighted avg       0.58      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.2834 | val acc:\t0.5645\n",
      "\n",
      "Epoch 23/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1099 | train acc:\t0.5711\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.64      0.45      0.53      1671\n",
      "           2       0.61      0.35      0.45      1174\n",
      "           3       0.52      0.50      0.51      1778\n",
      "           4       0.53      0.41      0.46      1329\n",
      "           5       0.74      0.74      0.74      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.60      0.53      0.55     10896\n",
      "weighted avg       0.58      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.2272 | val acc:\t0.6012\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.87      0.71      1518\n",
      "           1       0.56      0.40      0.47       468\n",
      "           2       0.74      0.18      0.29       321\n",
      "           3       0.46      0.39      0.42       397\n",
      "           4       0.55      0.26      0.35       309\n",
      "           5       0.90      0.63      0.74       252\n",
      "\n",
      "    accuracy                           0.60      3265\n",
      "   macro avg       0.64      0.46      0.50      3265\n",
      "weighted avg       0.61      0.60      0.57      3265\n",
      "\n",
      "Epoch 24/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1016 | train acc:\t0.5718\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.62      3943\n",
      "           1       0.63      0.45      0.52      1671\n",
      "           2       0.60      0.36      0.45      1174\n",
      "           3       0.51      0.49      0.50      1778\n",
      "           4       0.54      0.42      0.47      1329\n",
      "           5       0.75      0.75      0.75      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.60      0.53      0.55     10896\n",
      "weighted avg       0.58      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.2248 | val acc:\t0.5727\n",
      "\n",
      "Epoch 25/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0987 | train acc:\t0.5732\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63      3943\n",
      "           1       0.63      0.45      0.53      1671\n",
      "           2       0.59      0.34      0.44      1174\n",
      "           3       0.51      0.49      0.50      1778\n",
      "           4       0.55      0.41      0.47      1329\n",
      "           5       0.74      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.60      0.54      0.55     10896\n",
      "weighted avg       0.58      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.2216 | val acc:\t0.5954\n",
      "\n",
      "Epoch 26/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1037 | train acc:\t0.5716\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.75      0.63      3943\n",
      "           1       0.62      0.45      0.52      1671\n",
      "           2       0.62      0.34      0.44      1174\n",
      "           3       0.51      0.48      0.49      1778\n",
      "           4       0.54      0.39      0.45      1329\n",
      "           5       0.76      0.75      0.76      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.60      0.53      0.55     10896\n",
      "weighted avg       0.58      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.3075 | val acc:\t0.5632\n",
      "\n",
      "Epoch 27/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1026 | train acc:\t0.5696\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63      3943\n",
      "           1       0.63      0.45      0.52      1671\n",
      "           2       0.60      0.35      0.44      1174\n",
      "           3       0.52      0.50      0.51      1778\n",
      "           4       0.51      0.39      0.44      1329\n",
      "           5       0.74      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.53      0.55     10896\n",
      "weighted avg       0.57      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.2456 | val acc:\t0.5865\n",
      "\n",
      "Epoch 28/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0882 | train acc:\t0.5753\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.64      0.45      0.53      1671\n",
      "           2       0.60      0.37      0.46      1174\n",
      "           3       0.51      0.48      0.50      1778\n",
      "           4       0.54      0.40      0.46      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2707 | val acc:\t0.5908\n",
      "\n",
      "Epoch 29/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0867 | train acc:\t0.5743\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.62      3943\n",
      "           1       0.63      0.44      0.52      1671\n",
      "           2       0.60      0.36      0.45      1174\n",
      "           3       0.53      0.51      0.52      1778\n",
      "           4       0.55      0.41      0.47      1329\n",
      "           5       0.75      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.58      0.57      0.57     10896\n",
      "\n",
      "val loss:\t1.2582 | val acc:\t0.5804\n",
      "\n",
      "Epoch 30/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0817 | train acc:\t0.5836\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.63      0.46      0.53      1671\n",
      "           2       0.61      0.36      0.45      1174\n",
      "           3       0.53      0.51      0.52      1778\n",
      "           4       0.55      0.42      0.48      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2352 | val acc:\t0.5813\n",
      "\n",
      "Epoch 31/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0767 | train acc:\t0.5815\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.64      0.46      0.54      1671\n",
      "           2       0.62      0.38      0.47      1174\n",
      "           3       0.53      0.50      0.51      1778\n",
      "           4       0.54      0.41      0.47      1329\n",
      "           5       0.75      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.61      0.54      0.56     10896\n",
      "weighted avg       0.59      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2714 | val acc:\t0.5890\n",
      "\n",
      "Epoch 32/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0770 | train acc:\t0.5800\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.63      3943\n",
      "           1       0.62      0.48      0.54      1671\n",
      "           2       0.61      0.36      0.45      1174\n",
      "           3       0.52      0.50      0.51      1778\n",
      "           4       0.55      0.43      0.48      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2674 | val acc:\t0.5743\n",
      "\n",
      "Epoch 33/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t1.0706 | train acc:\t0.5850\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      3943\n",
      "           1       0.66      0.46      0.54      1671\n",
      "           2       0.61      0.37      0.46      1174\n",
      "           3       0.52      0.50      0.51      1778\n",
      "           4       0.56      0.43      0.49      1329\n",
      "           5       0.75      0.76      0.76      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.58      0.58     10896\n",
      "\n",
      "val loss:\t1.2500 | val acc:\t0.5963\n",
      "\n",
      "Epoch 34/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0732 | train acc:\t0.5839\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.63      0.46      0.54      1671\n",
      "           2       0.60      0.37      0.46      1174\n",
      "           3       0.53      0.51      0.52      1778\n",
      "           4       0.55      0.41      0.47      1329\n",
      "           5       0.77      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.58      0.58     10896\n",
      "\n",
      "val loss:\t1.2347 | val acc:\t0.6006\n",
      "\n",
      "Epoch 35/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0640 | train acc:\t0.5890\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.64      3943\n",
      "           1       0.64      0.47      0.55      1671\n",
      "           2       0.61      0.39      0.48      1174\n",
      "           3       0.54      0.52      0.53      1778\n",
      "           4       0.54      0.43      0.48      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2104 | val acc:\t0.5761\n",
      "\n",
      "Epoch 36/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0751 | train acc:\t0.5855\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63      3943\n",
      "           1       0.65      0.47      0.54      1671\n",
      "           2       0.59      0.39      0.47      1174\n",
      "           3       0.54      0.53      0.54      1778\n",
      "           4       0.56      0.42      0.48      1329\n",
      "           5       0.76      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2292 | val acc:\t0.5844\n",
      "\n",
      "Epoch 37/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0615 | train acc:\t0.5878\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.63      3943\n",
      "           1       0.65      0.47      0.55      1671\n",
      "           2       0.63      0.40      0.49      1174\n",
      "           3       0.53      0.52      0.53      1778\n",
      "           4       0.56      0.40      0.47      1329\n",
      "           5       0.77      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.62      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2110 | val acc:\t0.5776\n",
      "\n",
      "Epoch 38/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0598 | train acc:\t0.5870\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      3943\n",
      "           1       0.64      0.47      0.54      1671\n",
      "           2       0.61      0.38      0.47      1174\n",
      "           3       0.53      0.50      0.51      1778\n",
      "           4       0.54      0.42      0.47      1329\n",
      "           5       0.76      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2689 | val acc:\t0.5859\n",
      "\n",
      "Epoch 39/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0566 | train acc:\t0.5891\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.64      3943\n",
      "           1       0.64      0.48      0.55      1671\n",
      "           2       0.62      0.40      0.49      1174\n",
      "           3       0.53      0.52      0.52      1778\n",
      "           4       0.55      0.41      0.47      1329\n",
      "           5       0.76      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.56      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2525 | val acc:\t0.5755\n",
      "\n",
      "Epoch 40/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0495 | train acc:\t0.5910\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.64      0.49      0.55      1671\n",
      "           2       0.61      0.39      0.48      1174\n",
      "           3       0.55      0.52      0.53      1778\n",
      "           4       0.56      0.41      0.48      1329\n",
      "           5       0.77      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.56      0.58     10896\n",
      "weighted avg       0.60      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2236 | val acc:\t0.5877\n",
      "\n",
      "Epoch 41/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0469 | train acc:\t0.5987\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      3943\n",
      "           1       0.66      0.50      0.57      1671\n",
      "           2       0.63      0.43      0.51      1174\n",
      "           3       0.55      0.52      0.53      1778\n",
      "           4       0.56      0.43      0.49      1329\n",
      "           5       0.76      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2478 | val acc:\t0.5709\n",
      "\n",
      "Epoch 42/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0492 | train acc:\t0.5909\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.65      0.49      0.56      1671\n",
      "           2       0.62      0.38      0.47      1174\n",
      "           3       0.54      0.52      0.53      1778\n",
      "           4       0.55      0.41      0.47      1329\n",
      "           5       0.77      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.60      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2113 | val acc:\t0.5877\n",
      "\n",
      "Epoch 43/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0502 | train acc:\t0.5926\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.65      0.49      0.56      1671\n",
      "           2       0.62      0.39      0.48      1174\n",
      "           3       0.55      0.53      0.54      1778\n",
      "           4       0.55      0.43      0.48      1329\n",
      "           5       0.75      0.76      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.56      0.58     10896\n",
      "weighted avg       0.60      0.59      0.59     10896\n",
      "\n",
      "val loss:\t1.2373 | val acc:\t0.5960\n",
      "\n",
      "Epoch 44/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0501 | train acc:\t0.5977\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.73      0.64      3943\n",
      "           1       0.65      0.49      0.56      1671\n",
      "           2       0.62      0.40      0.49      1174\n",
      "           3       0.55      0.56      0.56      1778\n",
      "           4       0.55      0.41      0.47      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2367 | val acc:\t0.5838\n",
      "\n",
      "Epoch 45/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t1.0490 | train acc:\t0.5929\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      3943\n",
      "           1       0.63      0.49      0.55      1671\n",
      "           2       0.64      0.40      0.49      1174\n",
      "           3       0.55      0.52      0.53      1778\n",
      "           4       0.55      0.42      0.47      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.59      0.59     10896\n",
      "\n",
      "val loss:\t1.2158 | val acc:\t0.5954\n",
      "\n",
      "Epoch 46/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0321 | train acc:\t0.5981\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.73      0.64      3943\n",
      "           1       0.66      0.50      0.57      1671\n",
      "           2       0.62      0.41      0.50      1174\n",
      "           3       0.54      0.52      0.53      1778\n",
      "           4       0.55      0.44      0.49      1329\n",
      "           5       0.76      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2388 | val acc:\t0.5758\n",
      "\n",
      "Epoch 47/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0452 | train acc:\t0.5965\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65      3943\n",
      "           1       0.64      0.49      0.56      1671\n",
      "           2       0.61      0.41      0.49      1174\n",
      "           3       0.55      0.52      0.53      1778\n",
      "           4       0.57      0.43      0.49      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.3063 | val acc:\t0.5617\n",
      "\n",
      "Epoch 48/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0397 | train acc:\t0.6010\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.66      0.49      0.56      1671\n",
      "           2       0.63      0.41      0.50      1174\n",
      "           3       0.55      0.55      0.55      1778\n",
      "           4       0.56      0.44      0.49      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2118 | val acc:\t0.5819\n",
      "\n",
      "Epoch 49/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0371 | train acc:\t0.5963\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.75      0.65      3943\n",
      "           1       0.64      0.48      0.55      1671\n",
      "           2       0.60      0.40      0.48      1174\n",
      "           3       0.54      0.52      0.53      1778\n",
      "           4       0.56      0.43      0.49      1329\n",
      "           5       0.79      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.1883 | val acc:\t0.5930\n",
      "\n",
      "Epoch 50/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0391 | train acc:\t0.5999\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.64      0.50      0.57      1671\n",
      "           2       0.62      0.41      0.49      1174\n",
      "           3       0.57      0.54      0.55      1778\n",
      "           4       0.56      0.44      0.49      1329\n",
      "           5       0.75      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2208 | val acc:\t0.5988\n",
      "\n",
      "Epoch 51/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0340 | train acc:\t0.5997\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65      3943\n",
      "           1       0.65      0.49      0.56      1671\n",
      "           2       0.63      0.41      0.49      1174\n",
      "           3       0.55      0.53      0.54      1778\n",
      "           4       0.56      0.45      0.50      1329\n",
      "           5       0.77      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2310 | val acc:\t0.6077\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.87      0.71      1518\n",
      "           1       0.68      0.32      0.44       468\n",
      "           2       0.73      0.24      0.36       321\n",
      "           3       0.45      0.50      0.47       397\n",
      "           4       0.58      0.22      0.32       309\n",
      "           5       0.88      0.67      0.76       252\n",
      "\n",
      "    accuracy                           0.61      3265\n",
      "   macro avg       0.65      0.47      0.51      3265\n",
      "weighted avg       0.63      0.61      0.58      3265\n",
      "\n",
      "Epoch 52/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0294 | train acc:\t0.6041\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.64      0.49      0.55      1671\n",
      "           2       0.65      0.43      0.52      1174\n",
      "           3       0.55      0.55      0.55      1778\n",
      "           4       0.57      0.45      0.50      1329\n",
      "           5       0.77      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.2248 | val acc:\t0.5908\n",
      "\n",
      "Epoch 53/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0341 | train acc:\t0.6030\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65      3943\n",
      "           1       0.65      0.49      0.56      1671\n",
      "           2       0.63      0.42      0.51      1174\n",
      "           3       0.55      0.53      0.54      1778\n",
      "           4       0.57      0.45      0.51      1329\n",
      "           5       0.78      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.2368 | val acc:\t0.5905\n",
      "\n",
      "Epoch 54/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0302 | train acc:\t0.6027\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.64      0.42      0.51      1174\n",
      "           3       0.55      0.53      0.54      1778\n",
      "           4       0.56      0.43      0.49      1329\n",
      "           5       0.76      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.2044 | val acc:\t0.5957\n",
      "\n",
      "Epoch 55/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0346 | train acc:\t0.5999\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.63      0.49      0.55      1671\n",
      "           2       0.63      0.42      0.51      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.57      0.43      0.49      1329\n",
      "           5       0.76      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2618 | val acc:\t0.5899\n",
      "\n",
      "Epoch 56/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0293 | train acc:\t0.6037\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      3943\n",
      "           1       0.66      0.50      0.57      1671\n",
      "           2       0.62      0.43      0.51      1174\n",
      "           3       0.56      0.53      0.54      1778\n",
      "           4       0.56      0.43      0.48      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1810 | val acc:\t0.5838\n",
      "\n",
      "Epoch 57/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0272 | train acc:\t0.6058\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.64      0.51      0.57      1671\n",
      "           2       0.63      0.43      0.51      1174\n",
      "           3       0.57      0.54      0.55      1778\n",
      "           4       0.57      0.45      0.50      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2283 | val acc:\t0.5740\n",
      "\n",
      "Epoch 58/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0387 | train acc:\t0.5965\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65      3943\n",
      "           1       0.66      0.49      0.56      1671\n",
      "           2       0.62      0.40      0.49      1174\n",
      "           3       0.55      0.53      0.54      1778\n",
      "           4       0.55      0.43      0.48      1329\n",
      "           5       0.77      0.76      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.1877 | val acc:\t0.5933\n",
      "\n",
      "Epoch 59/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0199 | train acc:\t0.6118\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.65      0.45      0.53      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.59      0.45      0.51      1329\n",
      "           5       0.76      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.64      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.2026 | val acc:\t0.5933\n",
      "\n",
      "Epoch 60/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0200 | train acc:\t0.6035\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.65      0.49      0.56      1671\n",
      "           2       0.63      0.44      0.52      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.55      0.43      0.48      1329\n",
      "           5       0.78      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.2295 | val acc:\t0.5881\n",
      "\n",
      "Epoch 61/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0219 | train acc:\t0.6054\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.65      0.51      0.57      1671\n",
      "           2       0.63      0.44      0.52      1174\n",
      "           3       0.54      0.54      0.54      1778\n",
      "           4       0.57      0.42      0.48      1329\n",
      "           5       0.77      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1846 | val acc:\t0.6040\n",
      "\n",
      "Epoch 62/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0170 | train acc:\t0.6045\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.65      0.51      0.57      1671\n",
      "           2       0.61      0.42      0.50      1174\n",
      "           3       0.55      0.54      0.54      1778\n",
      "           4       0.57      0.44      0.49      1329\n",
      "           5       0.78      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.2194 | val acc:\t0.5847\n",
      "\n",
      "Epoch 63/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0211 | train acc:\t0.6055\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.73      0.65      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.63      0.43      0.51      1174\n",
      "           3       0.56      0.55      0.55      1778\n",
      "           4       0.56      0.43      0.49      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1946 | val acc:\t0.5957\n",
      "\n",
      "Epoch 64/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0224 | train acc:\t0.6028\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.66      0.49      0.56      1671\n",
      "           2       0.62      0.42      0.50      1174\n",
      "           3       0.56      0.55      0.55      1778\n",
      "           4       0.58      0.46      0.51      1329\n",
      "           5       0.76      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.2008 | val acc:\t0.5828\n",
      "\n",
      "Epoch 65/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0130 | train acc:\t0.6034\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.64      0.49      0.56      1671\n",
      "           2       0.61      0.44      0.51      1174\n",
      "           3       0.56      0.55      0.55      1778\n",
      "           4       0.56      0.43      0.49      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.1629 | val acc:\t0.5871\n",
      "\n",
      "Epoch 66/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0175 | train acc:\t0.6071\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.65      3943\n",
      "           1       0.65      0.51      0.57      1671\n",
      "           2       0.61      0.44      0.51      1174\n",
      "           3       0.56      0.53      0.54      1778\n",
      "           4       0.58      0.46      0.51      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.62      0.58      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2125 | val acc:\t0.6049\n",
      "\n",
      "Epoch 67/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0188 | train acc:\t0.6079\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      3943\n",
      "           1       0.64      0.50      0.56      1671\n",
      "           2       0.63      0.43      0.51      1174\n",
      "           3       0.56      0.53      0.54      1778\n",
      "           4       0.59      0.46      0.51      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2113 | val acc:\t0.5933\n",
      "\n",
      "Epoch 68/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0183 | train acc:\t0.6039\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.73      0.64      3943\n",
      "           1       0.66      0.50      0.57      1671\n",
      "           2       0.63      0.44      0.52      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.56      0.44      0.50      1329\n",
      "           5       0.76      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1899 | val acc:\t0.6052\n",
      "\n",
      "Epoch 69/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0068 | train acc:\t0.6112\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.65      0.52      0.58      1671\n",
      "           2       0.62      0.45      0.53      1174\n",
      "           3       0.57      0.54      0.56      1778\n",
      "           4       0.58      0.44      0.50      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.2088 | val acc:\t0.5933\n",
      "\n",
      "Epoch 70/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0038 | train acc:\t0.6090\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.65      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.63      0.44      0.52      1174\n",
      "           3       0.55      0.54      0.55      1778\n",
      "           4       0.56      0.46      0.51      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2124 | val acc:\t0.5795\n",
      "\n",
      "Epoch 71/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0149 | train acc:\t0.6082\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.66      0.53      0.59      1671\n",
      "           2       0.65      0.43      0.52      1174\n",
      "           3       0.56      0.53      0.54      1778\n",
      "           4       0.57      0.44      0.50      1329\n",
      "           5       0.76      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2394 | val acc:\t0.6074\n",
      "\n",
      "Epoch 72/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0102 | train acc:\t0.6063\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.73      0.65      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.62      0.45      0.52      1174\n",
      "           3       0.56      0.53      0.54      1778\n",
      "           4       0.56      0.45      0.50      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2069 | val acc:\t0.5942\n",
      "\n",
      "Epoch 73/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0142 | train acc:\t0.6055\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.73      0.65      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.63      0.43      0.51      1174\n",
      "           3       0.55      0.53      0.54      1778\n",
      "           4       0.56      0.46      0.51      1329\n",
      "           5       0.75      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.62      0.58      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2196 | val acc:\t0.5822\n",
      "\n",
      "Epoch 74/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0080 | train acc:\t0.6057\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.66      0.51      0.57      1671\n",
      "           2       0.65      0.44      0.53      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.55      0.43      0.48      1329\n",
      "           5       0.77      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2195 | val acc:\t0.5954\n",
      "\n",
      "Epoch 75/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0056 | train acc:\t0.6105\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.65      3943\n",
      "           1       0.65      0.51      0.57      1671\n",
      "           2       0.66      0.46      0.54      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.56      0.45      0.50      1329\n",
      "           5       0.77      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.2065 | val acc:\t0.5835\n",
      "\n",
      "Epoch 76/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0061 | train acc:\t0.6084\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.63      0.43      0.51      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.56      0.45      0.50      1329\n",
      "           5       0.78      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1956 | val acc:\t0.5991\n",
      "\n",
      "Epoch 77/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9991 | train acc:\t0.6139\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.63      0.45      0.52      1174\n",
      "           3       0.58      0.55      0.56      1778\n",
      "           4       0.58      0.44      0.50      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.2226 | val acc:\t0.5816\n",
      "\n",
      "Epoch 78/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0087 | train acc:\t0.6121\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.65      0.52      0.58      1671\n",
      "           2       0.65      0.45      0.53      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.56      0.44      0.49      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.1979 | val acc:\t0.6086\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.87      0.72      1518\n",
      "           1       0.61      0.43      0.50       468\n",
      "           2       0.65      0.33      0.43       321\n",
      "           3       0.44      0.35      0.39       397\n",
      "           4       0.51      0.20      0.28       309\n",
      "           5       0.89      0.66      0.76       252\n",
      "\n",
      "    accuracy                           0.61      3265\n",
      "   macro avg       0.62      0.47      0.51      3265\n",
      "weighted avg       0.61      0.61      0.58      3265\n",
      "\n",
      "Epoch 79/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9926 | train acc:\t0.6181\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.67      0.52      0.59      1671\n",
      "           2       0.63      0.45      0.52      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.59      0.47      0.52      1329\n",
      "           5       0.78      0.76      0.77      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.58      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1923 | val acc:\t0.5902\n",
      "\n",
      "Epoch 80/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t1.0059 | train acc:\t0.6128\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.63      0.46      0.53      1174\n",
      "           3       0.57      0.56      0.56      1778\n",
      "           4       0.57      0.44      0.50      1329\n",
      "           5       0.76      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.1732 | val acc:\t0.5930\n",
      "\n",
      "Epoch 81/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9994 | train acc:\t0.6120\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.65      0.51      0.57      1671\n",
      "           2       0.63      0.45      0.53      1174\n",
      "           3       0.58      0.54      0.56      1778\n",
      "           4       0.59      0.47      0.52      1329\n",
      "           5       0.77      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.2210 | val acc:\t0.6040\n",
      "\n",
      "Epoch 82/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0077 | train acc:\t0.6115\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.63      0.43      0.51      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.59      0.47      0.52      1329\n",
      "           5       0.77      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.2190 | val acc:\t0.5982\n",
      "\n",
      "Epoch 83/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9954 | train acc:\t0.6171\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.69      0.54      0.60      1671\n",
      "           2       0.62      0.45      0.52      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.59      0.46      0.52      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2400 | val acc:\t0.5893\n",
      "\n",
      "Epoch 84/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9947 | train acc:\t0.6159\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.66      0.45      0.53      1174\n",
      "           3       0.57      0.56      0.56      1778\n",
      "           4       0.56      0.46      0.51      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1644 | val acc:\t0.5819\n",
      "\n",
      "Epoch 85/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9945 | train acc:\t0.6163\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.67      0.52      0.59      1671\n",
      "           2       0.62      0.47      0.54      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.57      0.44      0.50      1329\n",
      "           5       0.79      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2099 | val acc:\t0.5979\n",
      "\n",
      "Epoch 86/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9939 | train acc:\t0.6152\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.65      0.52      0.58      1671\n",
      "           2       0.63      0.45      0.52      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.57      0.44      0.49      1329\n",
      "           5       0.78      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1978 | val acc:\t0.5902\n",
      "\n",
      "Epoch 87/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9869 | train acc:\t0.6176\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.66      0.53      0.59      1671\n",
      "           2       0.63      0.47      0.54      1174\n",
      "           3       0.58      0.55      0.56      1778\n",
      "           4       0.56      0.44      0.49      1329\n",
      "           5       0.79      0.78      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2179 | val acc:\t0.5933\n",
      "\n",
      "Epoch 88/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9817 | train acc:\t0.6231\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.66      3943\n",
      "           1       0.68      0.52      0.59      1671\n",
      "           2       0.65      0.46      0.54      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.59      0.47      0.52      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.65      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1698 | val acc:\t0.5972\n",
      "\n",
      "Epoch 89/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0012 | train acc:\t0.6089\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.65      0.51      0.57      1671\n",
      "           2       0.62      0.45      0.52      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.57      0.44      0.50      1329\n",
      "           5       0.77      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1922 | val acc:\t0.5963\n",
      "\n",
      "Epoch 90/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9912 | train acc:\t0.6173\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.66      0.45      0.53      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.57      0.47      0.51      1329\n",
      "           5       0.77      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2235 | val acc:\t0.5789\n",
      "\n",
      "Epoch 91/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9942 | train acc:\t0.6122\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.73      0.65      3943\n",
      "           1       0.68      0.53      0.59      1671\n",
      "           2       0.65      0.45      0.53      1174\n",
      "           3       0.56      0.55      0.56      1778\n",
      "           4       0.56      0.45      0.50      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.2057 | val acc:\t0.5930\n",
      "\n",
      "Epoch 92/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.9888 | train acc:\t0.6194\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.66      3943\n",
      "           1       0.67      0.52      0.59      1671\n",
      "           2       0.63      0.44      0.52      1174\n",
      "           3       0.57      0.56      0.56      1778\n",
      "           4       0.59      0.46      0.51      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2112 | val acc:\t0.5942\n",
      "\n",
      "Epoch 93/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9882 | train acc:\t0.6175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.66      0.53      0.58      1671\n",
      "           2       0.63      0.43      0.51      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.57      0.45      0.50      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2329 | val acc:\t0.5942\n",
      "\n",
      "Epoch 94/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9775 | train acc:\t0.6231\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.66      0.46      0.54      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.58      0.49      0.53      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1937 | val acc:\t0.6208\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.86      0.72      1518\n",
      "           1       0.67      0.41      0.51       468\n",
      "           2       0.61      0.34      0.44       321\n",
      "           3       0.50      0.34      0.41       397\n",
      "           4       0.55      0.38      0.45       309\n",
      "           5       0.88      0.65      0.75       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.64      0.50      0.54      3265\n",
      "weighted avg       0.62      0.62      0.60      3265\n",
      "\n",
      "Epoch 95/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9877 | train acc:\t0.6160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.64      0.48      0.55      1174\n",
      "           3       0.56      0.56      0.56      1778\n",
      "           4       0.57      0.45      0.51      1329\n",
      "           5       0.77      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2063 | val acc:\t0.5945\n",
      "\n",
      "Epoch 96/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9907 | train acc:\t0.6192\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.68      0.53      0.59      1671\n",
      "           2       0.63      0.48      0.55      1174\n",
      "           3       0.57      0.54      0.56      1778\n",
      "           4       0.58      0.46      0.51      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1782 | val acc:\t0.5844\n",
      "\n",
      "Epoch 97/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9909 | train acc:\t0.6115\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.65      3943\n",
      "           1       0.67      0.52      0.58      1671\n",
      "           2       0.64      0.47      0.54      1174\n",
      "           3       0.57      0.54      0.55      1778\n",
      "           4       0.56      0.43      0.49      1329\n",
      "           5       0.76      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.1852 | val acc:\t0.5859\n",
      "\n",
      "Epoch 98/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9979 | train acc:\t0.6104\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.65      0.51      0.57      1671\n",
      "           2       0.63      0.44      0.52      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.77      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1816 | val acc:\t0.6052\n",
      "\n",
      "Epoch 99/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9925 | train acc:\t0.6155\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.65      0.52      0.58      1671\n",
      "           2       0.63      0.47      0.54      1174\n",
      "           3       0.57      0.54      0.56      1778\n",
      "           4       0.58      0.45      0.51      1329\n",
      "           5       0.78      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2096 | val acc:\t0.5920\n",
      "\n",
      "Epoch 100/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9772 | train acc:\t0.6281\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.66      0.46      0.54      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.78      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1896 | val acc:\t0.6000\n",
      "\n",
      "Epoch 101/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9884 | train acc:\t0.6234\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.66      0.53      0.59      1671\n",
      "           2       0.67      0.45      0.54      1174\n",
      "           3       0.59      0.56      0.58      1778\n",
      "           4       0.59      0.47      0.52      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.65      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2088 | val acc:\t0.6080\n",
      "\n",
      "Epoch 102/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9943 | train acc:\t0.6144\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.65      0.50      0.57      1671\n",
      "           2       0.63      0.47      0.54      1174\n",
      "           3       0.56      0.55      0.56      1778\n",
      "           4       0.59      0.46      0.52      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.1937 | val acc:\t0.5972\n",
      "\n",
      "Epoch 103/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9885 | train acc:\t0.6166\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.67      0.54      0.60      1671\n",
      "           2       0.62      0.43      0.51      1174\n",
      "           3       0.57      0.56      0.57      1778\n",
      "           4       0.59      0.45      0.51      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1938 | val acc:\t0.6034\n",
      "\n",
      "Epoch 104/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9838 | train acc:\t0.6226\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.66      0.54      0.59      1671\n",
      "           2       0.65      0.46      0.54      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.59      0.46      0.52      1329\n",
      "           5       0.78      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1526 | val acc:\t0.6074\n",
      "\n",
      "Epoch 105/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9761 | train acc:\t0.6211\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.66      0.54      0.60      1671\n",
      "           2       0.65      0.47      0.54      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.58      0.48      0.53      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1958 | val acc:\t0.6165\n",
      "\n",
      "Epoch 106/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9843 | train acc:\t0.6211\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.65      0.46      0.54      1174\n",
      "           3       0.57      0.54      0.56      1778\n",
      "           4       0.58      0.50      0.54      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2446 | val acc:\t0.5926\n",
      "\n",
      "Epoch 107/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0000 | train acc:\t0.6134\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.64      0.44      0.52      1174\n",
      "           3       0.57      0.56      0.56      1778\n",
      "           4       0.57      0.47      0.52      1329\n",
      "           5       0.76      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.1716 | val acc:\t0.5783\n",
      "\n",
      "Epoch 108/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9791 | train acc:\t0.6244\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.68      0.47      0.56      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.78      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.65      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1472 | val acc:\t0.6092\n",
      "\n",
      "Epoch 109/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9805 | train acc:\t0.6217\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.66      0.54      0.59      1671\n",
      "           2       0.65      0.47      0.55      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2035 | val acc:\t0.6086\n",
      "\n",
      "Epoch 110/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9826 | train acc:\t0.6205\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.66      0.44      0.53      1174\n",
      "           3       0.58      0.58      0.58      1778\n",
      "           4       0.58      0.46      0.51      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1792 | val acc:\t0.5966\n",
      "\n",
      "Epoch 111/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9749 | train acc:\t0.6258\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.63      0.47      0.54      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.59      0.47      0.53      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.2127 | val acc:\t0.5939\n",
      "\n",
      "Epoch 112/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9844 | train acc:\t0.6222\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.64      0.47      0.54      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.59      0.47      0.52      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2052 | val acc:\t0.5786\n",
      "\n",
      "Epoch 113/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9808 | train acc:\t0.6240\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.66      0.47      0.55      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.58      0.48      0.53      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1619 | val acc:\t0.5853\n",
      "\n",
      "Epoch 114/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9775 | train acc:\t0.6212\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.66      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.64      0.45      0.53      1174\n",
      "           3       0.58      0.55      0.56      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.77      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1837 | val acc:\t0.6119\n",
      "\n",
      "Epoch 115/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9837 | train acc:\t0.6210\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.66      3943\n",
      "           1       0.67      0.52      0.58      1671\n",
      "           2       0.62      0.45      0.53      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.78      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.2222 | val acc:\t0.6175\n",
      "\n",
      "Epoch 116/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9888 | train acc:\t0.6210\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.62      0.45      0.52      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.60      0.47      0.53      1329\n",
      "           5       0.78      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2228 | val acc:\t0.5636\n",
      "\n",
      "Epoch 117/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9847 | train acc:\t0.6193\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.66      3943\n",
      "           1       0.66      0.53      0.59      1671\n",
      "           2       0.63      0.45      0.53      1174\n",
      "           3       0.57      0.54      0.56      1778\n",
      "           4       0.60      0.48      0.54      1329\n",
      "           5       0.75      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1935 | val acc:\t0.5887\n",
      "\n",
      "Epoch 118/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9854 | train acc:\t0.6237\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.69      0.52      0.60      1671\n",
      "           2       0.66      0.47      0.55      1174\n",
      "           3       0.58      0.59      0.59      1778\n",
      "           4       0.56      0.45      0.50      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2037 | val acc:\t0.6049\n",
      "\n",
      "Epoch 119/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9791 | train acc:\t0.6289\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.64      0.47      0.54      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.78      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.2095 | val acc:\t0.5896\n",
      "\n",
      "Epoch 120/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9809 | train acc:\t0.6233\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.67      3943\n",
      "           1       0.67      0.54      0.59      1671\n",
      "           2       0.64      0.48      0.55      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.59      0.47      0.52      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2012 | val acc:\t0.5960\n",
      "\n",
      "Epoch 121/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9786 | train acc:\t0.6219\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.64      0.47      0.54      1174\n",
      "           3       0.58      0.55      0.57      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2134 | val acc:\t0.6074\n",
      "\n",
      "Epoch 122/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9821 | train acc:\t0.6201\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.64      0.46      0.54      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.59      0.46      0.52      1329\n",
      "           5       0.78      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1881 | val acc:\t0.5917\n",
      "\n",
      "Epoch 123/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9820 | train acc:\t0.6217\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.68      0.48      0.56      1174\n",
      "           3       0.56      0.56      0.56      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.77      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1728 | val acc:\t0.5994\n",
      "\n",
      "Epoch 124/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9785 | train acc:\t0.6277\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.65      0.48      0.55      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.61      0.47      0.53      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1830 | val acc:\t0.5822\n",
      "\n",
      "Epoch 125/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9853 | train acc:\t0.6277\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.68      0.52      0.59      1671\n",
      "           2       0.67      0.48      0.56      1174\n",
      "           3       0.59      0.58      0.59      1778\n",
      "           4       0.59      0.46      0.51      1329\n",
      "           5       0.79      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1821 | val acc:\t0.5890\n",
      "\n",
      "Epoch 126/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9801 | train acc:\t0.6232\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.69      0.54      0.60      1671\n",
      "           2       0.67      0.47      0.55      1174\n",
      "           3       0.57      0.56      0.57      1778\n",
      "           4       0.59      0.47      0.52      1329\n",
      "           5       0.77      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.65      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2392 | val acc:\t0.5930\n",
      "\n",
      "Epoch 127/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9690 | train acc:\t0.6265\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.52      0.58      1671\n",
      "           2       0.65      0.47      0.55      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.60      0.48      0.53      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1568 | val acc:\t0.5966\n",
      "\n",
      "Epoch 128/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9812 | train acc:\t0.6183\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.65      0.52      0.58      1671\n",
      "           2       0.64      0.46      0.53      1174\n",
      "           3       0.58      0.55      0.56      1778\n",
      "           4       0.59      0.49      0.53      1329\n",
      "           5       0.76      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1928 | val acc:\t0.6031\n",
      "\n",
      "Epoch 129/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9709 | train acc:\t0.6242\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.67      3943\n",
      "           1       0.67      0.54      0.60      1671\n",
      "           2       0.63      0.46      0.53      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.58      0.48      0.53      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1612 | val acc:\t0.5917\n",
      "\n",
      "Epoch 130/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9916 | train acc:\t0.6188\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.66      0.53      0.59      1671\n",
      "           2       0.65      0.47      0.54      1174\n",
      "           3       0.56      0.56      0.56      1778\n",
      "           4       0.59      0.46      0.52      1329\n",
      "           5       0.77      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2046 | val acc:\t0.5816\n",
      "\n",
      "Epoch 131/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9709 | train acc:\t0.6245\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.66      3943\n",
      "           1       0.67      0.53      0.60      1671\n",
      "           2       0.66      0.47      0.55      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.60      0.47      0.53      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.65      0.60      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1826 | val acc:\t0.5972\n",
      "\n",
      "Epoch 132/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9714 | train acc:\t0.6232\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.64      0.45      0.53      1174\n",
      "           3       0.58      0.55      0.57      1778\n",
      "           4       0.59      0.47      0.53      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1752 | val acc:\t0.5730\n",
      "\n",
      "Epoch 133/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9807 | train acc:\t0.6244\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.63      0.45      0.53      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1740 | val acc:\t0.6052\n",
      "\n",
      "Epoch 134/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9719 | train acc:\t0.6241\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.66      0.54      0.59      1671\n",
      "           2       0.68      0.49      0.57      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.77      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1639 | val acc:\t0.5991\n",
      "\n",
      "Epoch 135/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9741 | train acc:\t0.6311\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.69      0.55      0.61      1671\n",
      "           2       0.64      0.48      0.55      1174\n",
      "           3       0.59      0.55      0.57      1778\n",
      "           4       0.60      0.47      0.53      1329\n",
      "           5       0.79      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1775 | val acc:\t0.5807\n",
      "\n",
      "Epoch 136/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9852 | train acc:\t0.6165\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.63      0.44      0.52      1174\n",
      "           3       0.56      0.56      0.56      1778\n",
      "           4       0.58      0.46      0.51      1329\n",
      "           5       0.77      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.63      0.59      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2462 | val acc:\t0.6153\n",
      "\n",
      "Epoch 137/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9720 | train acc:\t0.6227\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.66      0.46      0.54      1174\n",
      "           3       0.57      0.57      0.57      1778\n",
      "           4       0.59      0.47      0.52      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2015 | val acc:\t0.6025\n",
      "\n",
      "Epoch 138/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9692 | train acc:\t0.6274\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.65      0.50      0.56      1174\n",
      "           3       0.59      0.56      0.58      1778\n",
      "           4       0.58      0.45      0.51      1329\n",
      "           5       0.78      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1598 | val acc:\t0.5819\n",
      "\n",
      "Epoch 139/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9761 | train acc:\t0.6240\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.66      0.53      0.59      1671\n",
      "           2       0.66      0.48      0.55      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.78      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1814 | val acc:\t0.6046\n",
      "\n",
      "Epoch 140/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9610 | train acc:\t0.6288\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.64      0.45      0.53      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.59      0.50      0.54      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1579 | val acc:\t0.6095\n",
      "\n",
      "Epoch 141/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9735 | train acc:\t0.6210\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.64      0.46      0.54      1174\n",
      "           3       0.58      0.58      0.58      1778\n",
      "           4       0.59      0.49      0.53      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1814 | val acc:\t0.5942\n",
      "\n",
      "Epoch 142/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9650 | train acc:\t0.6318\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.66      0.54      0.59      1671\n",
      "           2       0.66      0.49      0.57      1174\n",
      "           3       0.60      0.57      0.58      1778\n",
      "           4       0.62      0.49      0.55      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1669 | val acc:\t0.5975\n",
      "\n",
      "Epoch 143/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9670 | train acc:\t0.6264\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.52      0.59      1671\n",
      "           2       0.65      0.48      0.55      1174\n",
      "           3       0.59      0.58      0.58      1778\n",
      "           4       0.59      0.47      0.52      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.2007 | val acc:\t0.5930\n",
      "\n",
      "Epoch 144/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9762 | train acc:\t0.6264\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.66      0.54      0.59      1671\n",
      "           2       0.65      0.45      0.53      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.60      0.48      0.53      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.59      0.61     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1881 | val acc:\t0.5865\n",
      "\n",
      "Epoch 145/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9680 | train acc:\t0.6305\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.68      0.52      0.59      1671\n",
      "           2       0.67      0.48      0.56      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.60      0.47      0.53      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1806 | val acc:\t0.5966\n",
      "\n",
      "Epoch 146/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9632 | train acc:\t0.6267\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.53      0.60      1671\n",
      "           2       0.64      0.48      0.55      1174\n",
      "           3       0.58      0.55      0.56      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.78      0.77      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1898 | val acc:\t0.6025\n",
      "\n",
      "Epoch 147/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9714 | train acc:\t0.6249\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.65      0.47      0.55      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.59      0.49      0.54      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2305 | val acc:\t0.6000\n",
      "\n",
      "Epoch 148/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9645 | train acc:\t0.6345\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      3943\n",
      "           1       0.67      0.55      0.60      1671\n",
      "           2       0.65      0.49      0.56      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.62      0.49      0.55      1329\n",
      "           5       0.78      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1874 | val acc:\t0.6009\n",
      "\n",
      "Epoch 149/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9700 | train acc:\t0.6273\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.67      3943\n",
      "           1       0.67      0.52      0.59      1671\n",
      "           2       0.65      0.47      0.54      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.59      0.49      0.54      1329\n",
      "           5       0.78      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1710 | val acc:\t0.5761\n",
      "\n",
      "Epoch 150/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9656 | train acc:\t0.6295\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.54      0.60      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.59      0.55      0.57      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1683 | val acc:\t0.5905\n",
      "\n",
      "Epoch 151/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9690 | train acc:\t0.6266\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.66      0.53      0.59      1671\n",
      "           2       0.64      0.47      0.54      1174\n",
      "           3       0.58      0.55      0.56      1778\n",
      "           4       0.60      0.48      0.53      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.2016 | val acc:\t0.6156\n",
      "\n",
      "Epoch 152/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9719 | train acc:\t0.6211\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.65      0.47      0.54      1174\n",
      "           3       0.57      0.56      0.57      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.77      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2622 | val acc:\t0.5972\n",
      "\n",
      "Epoch 153/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9682 | train acc:\t0.6256\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.66      0.48      0.55      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.62      0.47      0.53      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1896 | val acc:\t0.5942\n",
      "\n",
      "Epoch 154/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9646 | train acc:\t0.6293\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.66      0.48      0.56      1174\n",
      "           3       0.60      0.57      0.59      1778\n",
      "           4       0.58      0.48      0.53      1329\n",
      "           5       0.78      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1632 | val acc:\t0.6236\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.87      0.72      1518\n",
      "           1       0.66      0.40      0.50       468\n",
      "           2       0.76      0.26      0.39       321\n",
      "           3       0.49      0.45      0.47       397\n",
      "           4       0.58      0.37      0.45       309\n",
      "           5       0.90      0.63      0.74       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.67      0.49      0.54      3265\n",
      "weighted avg       0.64      0.62      0.60      3265\n",
      "\n",
      "Epoch 155/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9537 | train acc:\t0.6338\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.64      0.49      0.56      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.59      0.49      0.54      1329\n",
      "           5       0.79      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.2080 | val acc:\t0.6009\n",
      "\n",
      "Epoch 156/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9653 | train acc:\t0.6303\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.55      0.61      1671\n",
      "           2       0.67      0.48      0.56      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1649 | val acc:\t0.5902\n",
      "\n",
      "Epoch 157/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9695 | train acc:\t0.6262\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.68      0.54      0.61      1671\n",
      "           2       0.66      0.48      0.56      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.78      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1665 | val acc:\t0.5960\n",
      "\n",
      "Epoch 158/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9738 | train acc:\t0.6241\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.64      0.46      0.54      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.58      0.50      0.54      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1579 | val acc:\t0.5828\n",
      "\n",
      "Epoch 159/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9712 | train acc:\t0.6288\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.69      0.53      0.60      1671\n",
      "           2       0.68      0.48      0.56      1174\n",
      "           3       0.59      0.55      0.57      1778\n",
      "           4       0.60      0.48      0.53      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1701 | val acc:\t0.5807\n",
      "\n",
      "Epoch 160/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9502 | train acc:\t0.6371\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      3943\n",
      "           1       0.69      0.55      0.61      1671\n",
      "           2       0.67      0.50      0.57      1174\n",
      "           3       0.59      0.56      0.58      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.78      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.1674 | val acc:\t0.5975\n",
      "\n",
      "Epoch 161/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9663 | train acc:\t0.6318\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.67      3943\n",
      "           1       0.68      0.53      0.59      1671\n",
      "           2       0.64      0.47      0.54      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.62      0.49      0.55      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1847 | val acc:\t0.5988\n",
      "\n",
      "Epoch 162/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9743 | train acc:\t0.6256\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.68      0.53      0.59      1671\n",
      "           2       0.65      0.47      0.55      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.78      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1995 | val acc:\t0.5985\n",
      "\n",
      "Epoch 163/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.9643 | train acc:\t0.6299\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.66      0.48      0.55      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.60      0.48      0.53      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1532 | val acc:\t0.5856\n",
      "\n",
      "Epoch 164/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9610 | train acc:\t0.6334\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      3943\n",
      "           1       0.67      0.54      0.60      1671\n",
      "           2       0.66      0.47      0.55      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.61      0.50      0.55      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.2049 | val acc:\t0.6015\n",
      "\n",
      "Epoch 165/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9735 | train acc:\t0.6258\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.65      0.48      0.55      1174\n",
      "           3       0.58      0.55      0.57      1778\n",
      "           4       0.59      0.46      0.52      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.61     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1962 | val acc:\t0.5926\n",
      "\n",
      "Epoch 166/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9611 | train acc:\t0.6299\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      3943\n",
      "           1       0.67      0.54      0.60      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.58      0.58      0.58      1778\n",
      "           4       0.58      0.49      0.53      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.2054 | val acc:\t0.6129\n",
      "\n",
      "Epoch 167/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9732 | train acc:\t0.6254\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.66      0.47      0.55      1174\n",
      "           3       0.58      0.55      0.56      1778\n",
      "           4       0.60      0.48      0.54      1329\n",
      "           5       0.76      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.61     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.2406 | val acc:\t0.5917\n",
      "\n",
      "Epoch 168/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9624 | train acc:\t0.6320\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.69      0.53      0.60      1671\n",
      "           2       0.68      0.50      0.58      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.76      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1501 | val acc:\t0.5917\n",
      "\n",
      "Epoch 169/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9644 | train acc:\t0.6287\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.63      0.44      0.52      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.58      0.48      0.53      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.64      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1496 | val acc:\t0.5884\n",
      "\n",
      "Epoch 170/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9631 | train acc:\t0.6313\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.65      0.47      0.55      1174\n",
      "           3       0.58      0.57      0.58      1778\n",
      "           4       0.60      0.48      0.53      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1871 | val acc:\t0.5994\n",
      "\n",
      "Epoch 171/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9612 | train acc:\t0.6316\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.66      0.47      0.55      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.59      0.47      0.52      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.2204 | val acc:\t0.5923\n",
      "\n",
      "Epoch 172/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9575 | train acc:\t0.6331\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.64      0.48      0.55      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.60      0.50      0.54      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1854 | val acc:\t0.5960\n",
      "\n",
      "Epoch 173/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9675 | train acc:\t0.6262\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.65      0.48      0.55      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.60      0.47      0.53      1329\n",
      "           5       0.78      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1638 | val acc:\t0.5896\n",
      "\n",
      "Epoch 174/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9715 | train acc:\t0.6238\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.68      0.52      0.59      1671\n",
      "           2       0.66      0.47      0.55      1174\n",
      "           3       0.58      0.57      0.58      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.65      0.60      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1101 | val acc:\t0.5923\n",
      "\n",
      "Epoch 175/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.9674 | train acc:\t0.6281\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.66      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.67      0.49      0.56      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.60      0.47      0.53      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1572 | val acc:\t0.6095\n",
      "\n",
      "Epoch 176/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9615 | train acc:\t0.6320\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.68      0.52      0.59      1671\n",
      "           2       0.69      0.49      0.57      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.66      0.60      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1694 | val acc:\t0.6049\n",
      "\n",
      "Epoch 177/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9684 | train acc:\t0.6274\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.66      0.48      0.56      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.56      0.46      0.51      1329\n",
      "           5       0.77      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.61     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1966 | val acc:\t0.6092\n",
      "\n",
      "Epoch 178/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9553 | train acc:\t0.6316\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.77      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1785 | val acc:\t0.5994\n",
      "\n",
      "Epoch 179/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9710 | train acc:\t0.6212\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.65      0.48      0.56      1174\n",
      "           3       0.57      0.56      0.57      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1911 | val acc:\t0.5994\n",
      "\n",
      "Epoch 180/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9677 | train acc:\t0.6311\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.66      0.48      0.56      1174\n",
      "           3       0.60      0.59      0.60      1778\n",
      "           4       0.60      0.47      0.53      1329\n",
      "           5       0.77      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1979 | val acc:\t0.6083\n",
      "\n",
      "Epoch 181/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9594 | train acc:\t0.6267\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.54      0.60      1671\n",
      "           2       0.63      0.46      0.53      1174\n",
      "           3       0.60      0.57      0.58      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1725 | val acc:\t0.6003\n",
      "\n",
      "Epoch 182/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9667 | train acc:\t0.6297\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.54      0.60      1671\n",
      "           2       0.69      0.50      0.58      1174\n",
      "           3       0.58      0.55      0.57      1778\n",
      "           4       0.58      0.48      0.52      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1884 | val acc:\t0.5988\n",
      "\n",
      "Epoch 183/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9660 | train acc:\t0.6311\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      3943\n",
      "           1       0.67      0.55      0.60      1671\n",
      "           2       0.63      0.48      0.54      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.59      0.49      0.54      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1855 | val acc:\t0.6000\n",
      "\n",
      "Epoch 184/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9556 | train acc:\t0.6357\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      3943\n",
      "           1       0.67      0.56      0.61      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.61      0.51      0.56      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.1967 | val acc:\t0.5767\n",
      "\n",
      "Epoch 185/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9731 | train acc:\t0.6293\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.60      0.48      0.53      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1584 | val acc:\t0.5917\n",
      "\n",
      "Epoch 186/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9599 | train acc:\t0.6332\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.67      3943\n",
      "           1       0.69      0.54      0.61      1671\n",
      "           2       0.64      0.49      0.55      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.61      0.49      0.54      1329\n",
      "           5       0.79      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1781 | val acc:\t0.5991\n",
      "\n",
      "Epoch 187/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.9567 | train acc:\t0.6333\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.69      0.53      0.60      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1484 | val acc:\t0.5954\n",
      "\n",
      "Epoch 188/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9666 | train acc:\t0.6298\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.65      0.47      0.54      1174\n",
      "           3       0.59      0.58      0.58      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.79      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1855 | val acc:\t0.5991\n",
      "\n",
      "Epoch 189/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9699 | train acc:\t0.6292\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.65      0.52      0.58      1671\n",
      "           2       0.64      0.49      0.55      1174\n",
      "           3       0.59      0.56      0.58      1778\n",
      "           4       0.60      0.48      0.53      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1837 | val acc:\t0.6034\n",
      "\n",
      "Epoch 190/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9690 | train acc:\t0.6269\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.68      0.48      0.57      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.60      0.47      0.53      1329\n",
      "           5       0.76      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1550 | val acc:\t0.5887\n",
      "\n",
      "Epoch 191/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9532 | train acc:\t0.6372\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      3943\n",
      "           1       0.70      0.56      0.62      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.59      0.58      0.59      1778\n",
      "           4       0.60      0.46      0.52      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.1521 | val acc:\t0.6012\n",
      "\n",
      "Epoch 192/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9559 | train acc:\t0.6324\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.67      3943\n",
      "           1       0.70      0.56      0.62      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.58      0.54      0.56      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1586 | val acc:\t0.6116\n",
      "\n",
      "Epoch 193/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9642 | train acc:\t0.6246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.67      3943\n",
      "           1       0.68      0.52      0.59      1671\n",
      "           2       0.64      0.48      0.55      1174\n",
      "           3       0.58      0.57      0.58      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1702 | val acc:\t0.5881\n",
      "\n",
      "Epoch 194/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9593 | train acc:\t0.6313\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.66      0.48      0.55      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.60      0.50      0.54      1329\n",
      "           5       0.77      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1573 | val acc:\t0.5985\n",
      "\n",
      "Epoch 195/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9570 | train acc:\t0.6322\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.54      0.60      1671\n",
      "           2       0.65      0.49      0.56      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.60      0.48      0.54      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1756 | val acc:\t0.5966\n",
      "\n",
      "Epoch 196/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9564 | train acc:\t0.6315\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.55      0.60      1671\n",
      "           2       0.67      0.48      0.56      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1834 | val acc:\t0.5988\n",
      "\n",
      "Epoch 197/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9595 | train acc:\t0.6314\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.65      0.52      0.58      1671\n",
      "           2       0.65      0.48      0.55      1174\n",
      "           3       0.58      0.57      0.58      1778\n",
      "           4       0.61      0.48      0.54      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.2007 | val acc:\t0.5982\n",
      "\n",
      "Epoch 198/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9643 | train acc:\t0.6274\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.66      0.53      0.59      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.60      0.55      0.57      1778\n",
      "           4       0.58      0.48      0.53      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1745 | val acc:\t0.6135\n",
      "\n",
      "Epoch 199/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.9611 | train acc:\t0.6311\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.65      0.46      0.54      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.60      0.47      0.53      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1987 | val acc:\t0.6086\n",
      "\n",
      "Epoch 200/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9585 | train acc:\t0.6340\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.69      0.55      0.61      1671\n",
      "           2       0.65      0.50      0.56      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.79      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1457 | val acc:\t0.6067\n",
      "\n",
      "Training complete in 30m 3s\n",
      "Best val acc: 0.623583\n",
      "\t conv1.weight\n",
      "\t conv1.bias\n",
      "\t conv2.weight\n",
      "\t conv2.bias\n",
      "\t conv3.weight\n",
      "\t conv3.bias\n",
      "\t linear1.weight\n",
      "\t linear1.bias\n",
      "\t linear2.weight\n",
      "\t linear2.bias\n",
      "Epoch 1/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.6396 | train acc:\t0.3602\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.98      0.53      3943\n",
      "           1       0.15      0.01      0.02      1671\n",
      "           2       0.00      0.00      0.00      1174\n",
      "           3       0.29      0.01      0.02      1778\n",
      "           4       0.00      0.00      0.00      1329\n",
      "           5       0.50      0.00      0.01      1001\n",
      "\n",
      "    accuracy                           0.36     10896\n",
      "   macro avg       0.22      0.17      0.10     10896\n",
      "weighted avg       0.25      0.36      0.20     10896\n",
      "\n",
      "val loss:\t1.6160 | val acc:\t0.4674\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      1.00      0.64      1518\n",
      "           1       0.00      0.00      0.00       468\n",
      "           2       0.00      0.00      0.00       321\n",
      "           3       0.38      0.03      0.05       397\n",
      "           4       0.00      0.00      0.00       309\n",
      "           5       1.00      0.02      0.03       252\n",
      "\n",
      "    accuracy                           0.47      3265\n",
      "   macro avg       0.31      0.17      0.12      3265\n",
      "weighted avg       0.34      0.47      0.30      3265\n",
      "\n",
      "Epoch 2/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.5040 | train acc:\t0.4008\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.82      0.55      3943\n",
      "           1       0.30      0.13      0.18      1671\n",
      "           2       0.60      0.00      0.01      1174\n",
      "           3       0.31      0.28      0.29      1778\n",
      "           4       0.33      0.01      0.02      1329\n",
      "           5       0.60      0.42      0.49      1001\n",
      "\n",
      "    accuracy                           0.40     10896\n",
      "   macro avg       0.42      0.28      0.26     10896\n",
      "weighted avg       0.41      0.40      0.32     10896\n",
      "\n",
      "val loss:\t1.5852 | val acc:\t0.4882\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.77      0.66      1518\n",
      "           1       0.32      0.19      0.24       468\n",
      "           2       0.00      0.00      0.00       321\n",
      "           3       0.24      0.44      0.32       397\n",
      "           4       0.61      0.04      0.07       309\n",
      "           5       0.67      0.58      0.62       252\n",
      "\n",
      "    accuracy                           0.49      3265\n",
      "   macro avg       0.40      0.34      0.32      3265\n",
      "weighted avg       0.45      0.49      0.43      3265\n",
      "\n",
      "Epoch 3/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.4416 | train acc:\t0.4323\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.78      0.56      3943\n",
      "           1       0.37      0.23      0.28      1671\n",
      "           2       0.28      0.01      0.01      1174\n",
      "           3       0.34      0.34      0.34      1778\n",
      "           4       0.48      0.05      0.09      1329\n",
      "           5       0.59      0.59      0.59      1001\n",
      "\n",
      "    accuracy                           0.43     10896\n",
      "   macro avg       0.42      0.33      0.31     10896\n",
      "weighted avg       0.42      0.43      0.37     10896\n",
      "\n",
      "val loss:\t1.5185 | val acc:\t0.5317\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.90      0.68      1518\n",
      "           1       0.45      0.24      0.32       468\n",
      "           2       1.00      0.01      0.02       321\n",
      "           3       0.31      0.25      0.28       397\n",
      "           4       0.38      0.02      0.03       309\n",
      "           5       0.70      0.61      0.65       252\n",
      "\n",
      "    accuracy                           0.53      3265\n",
      "   macro avg       0.57      0.34      0.33      3265\n",
      "weighted avg       0.55      0.53      0.45      3265\n",
      "\n",
      "Epoch 4/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.4092 | train acc:\t0.4431\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.77      0.57      3943\n",
      "           1       0.40      0.29      0.34      1671\n",
      "           2       0.33      0.01      0.02      1174\n",
      "           3       0.36      0.33      0.34      1778\n",
      "           4       0.44      0.08      0.13      1329\n",
      "           5       0.61      0.60      0.60      1001\n",
      "\n",
      "    accuracy                           0.44     10896\n",
      "   macro avg       0.43      0.35      0.33     10896\n",
      "weighted avg       0.43      0.44      0.39     10896\n",
      "\n",
      "val loss:\t1.5212 | val acc:\t0.5219\n",
      "\n",
      "Epoch 5/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3715 | train acc:\t0.4648\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.77      0.58      3943\n",
      "           1       0.45      0.35      0.39      1671\n",
      "           2       0.40      0.02      0.05      1174\n",
      "           3       0.39      0.37      0.38      1778\n",
      "           4       0.40      0.11      0.17      1329\n",
      "           5       0.64      0.63      0.64      1001\n",
      "\n",
      "    accuracy                           0.46     10896\n",
      "   macro avg       0.46      0.37      0.37     10896\n",
      "weighted avg       0.45      0.46      0.42     10896\n",
      "\n",
      "val loss:\t1.5039 | val acc:\t0.5473\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.78      0.69      1518\n",
      "           1       0.40      0.41      0.40       468\n",
      "           2       0.56      0.13      0.21       321\n",
      "           3       0.33      0.33      0.33       397\n",
      "           4       0.37      0.25      0.30       309\n",
      "           5       0.73      0.66      0.70       252\n",
      "\n",
      "    accuracy                           0.55      3265\n",
      "   macro avg       0.50      0.43      0.44      3265\n",
      "weighted avg       0.54      0.55      0.52      3265\n",
      "\n",
      "Epoch 6/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3568 | train acc:\t0.4674\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.75      0.58      3943\n",
      "           1       0.45      0.37      0.41      1671\n",
      "           2       0.45      0.05      0.09      1174\n",
      "           3       0.38      0.37      0.37      1778\n",
      "           4       0.41      0.13      0.20      1329\n",
      "           5       0.62      0.63      0.63      1001\n",
      "\n",
      "    accuracy                           0.47     10896\n",
      "   macro avg       0.47      0.38      0.38     10896\n",
      "weighted avg       0.46      0.47      0.42     10896\n",
      "\n",
      "val loss:\t1.4878 | val acc:\t0.5495\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.81      0.70      1518\n",
      "           1       0.41      0.45      0.43       468\n",
      "           2       0.72      0.07      0.13       321\n",
      "           3       0.34      0.30      0.32       397\n",
      "           4       0.37      0.21      0.27       309\n",
      "           5       0.80      0.62      0.70       252\n",
      "\n",
      "    accuracy                           0.55      3265\n",
      "   macro avg       0.54      0.41      0.42      3265\n",
      "weighted avg       0.55      0.55      0.52      3265\n",
      "\n",
      "Epoch 7/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t1.3381 | train acc:\t0.4781\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.75      0.58      3943\n",
      "           1       0.47      0.39      0.43      1671\n",
      "           2       0.47      0.05      0.08      1174\n",
      "           3       0.41      0.38      0.40      1778\n",
      "           4       0.43      0.16      0.24      1329\n",
      "           5       0.63      0.66      0.65      1001\n",
      "\n",
      "    accuracy                           0.48     10896\n",
      "   macro avg       0.48      0.40      0.40     10896\n",
      "weighted avg       0.47      0.48      0.44     10896\n",
      "\n",
      "val loss:\t1.4549 | val acc:\t0.5501\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.85      0.69      1518\n",
      "           1       0.49      0.29      0.37       468\n",
      "           2       0.69      0.06      0.11       321\n",
      "           3       0.35      0.34      0.34       397\n",
      "           4       0.36      0.23      0.28       309\n",
      "           5       0.83      0.58      0.68       252\n",
      "\n",
      "    accuracy                           0.55      3265\n",
      "   macro avg       0.55      0.39      0.41      3265\n",
      "weighted avg       0.55      0.55      0.51      3265\n",
      "\n",
      "Epoch 8/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3150 | train acc:\t0.4845\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.74      0.58      3943\n",
      "           1       0.49      0.42      0.45      1671\n",
      "           2       0.45      0.07      0.11      1174\n",
      "           3       0.41      0.37      0.39      1778\n",
      "           4       0.44      0.19      0.26      1329\n",
      "           5       0.65      0.66      0.65      1001\n",
      "\n",
      "    accuracy                           0.48     10896\n",
      "   macro avg       0.49      0.41      0.41     10896\n",
      "weighted avg       0.48      0.48      0.45     10896\n",
      "\n",
      "val loss:\t1.4540 | val acc:\t0.5642\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.82      0.70      1518\n",
      "           1       0.48      0.31      0.37       468\n",
      "           2       0.55      0.20      0.30       321\n",
      "           3       0.36      0.34      0.35       397\n",
      "           4       0.39      0.25      0.31       309\n",
      "           5       0.79      0.67      0.73       252\n",
      "\n",
      "    accuracy                           0.56      3265\n",
      "   macro avg       0.53      0.43      0.46      3265\n",
      "weighted avg       0.55      0.56      0.54      3265\n",
      "\n",
      "Epoch 9/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3060 | train acc:\t0.4880\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.75      0.59      3943\n",
      "           1       0.49      0.39      0.44      1671\n",
      "           2       0.49      0.07      0.13      1174\n",
      "           3       0.41      0.38      0.39      1778\n",
      "           4       0.45      0.21      0.28      1329\n",
      "           5       0.66      0.67      0.67      1001\n",
      "\n",
      "    accuracy                           0.49     10896\n",
      "   macro avg       0.50      0.41      0.42     10896\n",
      "weighted avg       0.48      0.49      0.45     10896\n",
      "\n",
      "val loss:\t1.4352 | val acc:\t0.5703\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.83      0.70      1518\n",
      "           1       0.44      0.45      0.44       468\n",
      "           2       0.55      0.21      0.30       321\n",
      "           3       0.38      0.20      0.26       397\n",
      "           4       0.43      0.19      0.26       309\n",
      "           5       0.76      0.73      0.74       252\n",
      "\n",
      "    accuracy                           0.57      3265\n",
      "   macro avg       0.53      0.43      0.45      3265\n",
      "weighted avg       0.55      0.57      0.53      3265\n",
      "\n",
      "Epoch 10/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3013 | train acc:\t0.4905\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.74      0.59      3943\n",
      "           1       0.49      0.41      0.44      1671\n",
      "           2       0.52      0.12      0.20      1174\n",
      "           3       0.42      0.39      0.41      1778\n",
      "           4       0.41      0.20      0.27      1329\n",
      "           5       0.65      0.66      0.65      1001\n",
      "\n",
      "    accuracy                           0.49     10896\n",
      "   macro avg       0.50      0.42      0.43     10896\n",
      "weighted avg       0.49      0.49      0.46     10896\n",
      "\n",
      "val loss:\t1.4304 | val acc:\t0.5629\n",
      "\n",
      "Epoch 11/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2879 | train acc:\t0.4971\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.75      0.60      3943\n",
      "           1       0.51      0.42      0.46      1671\n",
      "           2       0.55      0.10      0.17      1174\n",
      "           3       0.42      0.40      0.41      1778\n",
      "           4       0.40      0.19      0.26      1329\n",
      "           5       0.66      0.68      0.67      1001\n",
      "\n",
      "    accuracy                           0.50     10896\n",
      "   macro avg       0.51      0.42      0.43     10896\n",
      "weighted avg       0.49      0.50      0.47     10896\n",
      "\n",
      "val loss:\t1.4270 | val acc:\t0.5737\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.83      0.70      1518\n",
      "           1       0.48      0.38      0.42       468\n",
      "           2       0.61      0.19      0.29       321\n",
      "           3       0.37      0.33      0.35       397\n",
      "           4       0.42      0.19      0.27       309\n",
      "           5       0.77      0.73      0.75       252\n",
      "\n",
      "    accuracy                           0.57      3265\n",
      "   macro avg       0.54      0.44      0.46      3265\n",
      "weighted avg       0.56      0.57      0.54      3265\n",
      "\n",
      "Epoch 12/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2761 | train acc:\t0.5018\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.74      0.60      3943\n",
      "           1       0.51      0.41      0.45      1671\n",
      "           2       0.47      0.11      0.18      1174\n",
      "           3       0.42      0.39      0.40      1778\n",
      "           4       0.46      0.24      0.32      1329\n",
      "           5       0.69      0.70      0.70      1001\n",
      "\n",
      "    accuracy                           0.50     10896\n",
      "   macro avg       0.51      0.43      0.44     10896\n",
      "weighted avg       0.50      0.50      0.47     10896\n",
      "\n",
      "val loss:\t1.4116 | val acc:\t0.5681\n",
      "\n",
      "Epoch 13/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2735 | train acc:\t0.5087\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.74      0.60      3943\n",
      "           1       0.53      0.44      0.48      1671\n",
      "           2       0.52      0.12      0.19      1174\n",
      "           3       0.44      0.41      0.42      1778\n",
      "           4       0.47      0.24      0.32      1329\n",
      "           5       0.67      0.70      0.69      1001\n",
      "\n",
      "    accuracy                           0.51     10896\n",
      "   macro avg       0.52      0.44      0.45     10896\n",
      "weighted avg       0.51      0.51      0.48     10896\n",
      "\n",
      "val loss:\t1.4258 | val acc:\t0.5746\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.78      0.70      1518\n",
      "           1       0.47      0.36      0.41       468\n",
      "           2       0.48      0.34      0.40       321\n",
      "           3       0.36      0.35      0.36       397\n",
      "           4       0.42      0.27      0.33       309\n",
      "           5       0.74      0.77      0.75       252\n",
      "\n",
      "    accuracy                           0.57      3265\n",
      "   macro avg       0.52      0.48      0.49      3265\n",
      "weighted avg       0.55      0.57      0.56      3265\n",
      "\n",
      "Epoch 14/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2541 | train acc:\t0.5123\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.73      0.60      3943\n",
      "           1       0.54      0.43      0.48      1671\n",
      "           2       0.52      0.15      0.23      1174\n",
      "           3       0.44      0.43      0.44      1778\n",
      "           4       0.45      0.26      0.33      1329\n",
      "           5       0.68      0.69      0.69      1001\n",
      "\n",
      "    accuracy                           0.51     10896\n",
      "   macro avg       0.52      0.45      0.46     10896\n",
      "weighted avg       0.51      0.51      0.49     10896\n",
      "\n",
      "val loss:\t1.3847 | val acc:\t0.5712\n",
      "\n",
      "Epoch 15/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t1.2502 | train acc:\t0.5128\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.73      0.60      3943\n",
      "           1       0.53      0.44      0.48      1671\n",
      "           2       0.49      0.15      0.22      1174\n",
      "           3       0.45      0.42      0.43      1778\n",
      "           4       0.46      0.25      0.33      1329\n",
      "           5       0.69      0.71      0.70      1001\n",
      "\n",
      "    accuracy                           0.51     10896\n",
      "   macro avg       0.52      0.45      0.46     10896\n",
      "weighted avg       0.51      0.51      0.49     10896\n",
      "\n",
      "val loss:\t1.3835 | val acc:\t0.5697\n",
      "\n",
      "Epoch 16/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2381 | train acc:\t0.5173\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.74      0.61      3943\n",
      "           1       0.54      0.42      0.47      1671\n",
      "           2       0.49      0.15      0.23      1174\n",
      "           3       0.45      0.44      0.44      1778\n",
      "           4       0.46      0.26      0.33      1329\n",
      "           5       0.68      0.71      0.69      1001\n",
      "\n",
      "    accuracy                           0.52     10896\n",
      "   macro avg       0.52      0.45      0.46     10896\n",
      "weighted avg       0.51      0.52      0.49     10896\n",
      "\n",
      "val loss:\t1.4154 | val acc:\t0.5767\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      1518\n",
      "           1       0.46      0.37      0.41       468\n",
      "           2       0.47      0.36      0.41       321\n",
      "           3       0.39      0.35      0.37       397\n",
      "           4       0.41      0.37      0.39       309\n",
      "           5       0.77      0.75      0.76       252\n",
      "\n",
      "    accuracy                           0.58      3265\n",
      "   macro avg       0.53      0.49      0.51      3265\n",
      "weighted avg       0.56      0.58      0.57      3265\n",
      "\n",
      "Epoch 17/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2405 | train acc:\t0.5193\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.74      0.60      3943\n",
      "           1       0.55      0.43      0.48      1671\n",
      "           2       0.50      0.16      0.24      1174\n",
      "           3       0.44      0.42      0.43      1778\n",
      "           4       0.48      0.29      0.36      1329\n",
      "           5       0.70      0.72      0.71      1001\n",
      "\n",
      "    accuracy                           0.52     10896\n",
      "   macro avg       0.53      0.46      0.47     10896\n",
      "weighted avg       0.52      0.52      0.50     10896\n",
      "\n",
      "val loss:\t1.3722 | val acc:\t0.5746\n",
      "\n",
      "Epoch 18/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2278 | train acc:\t0.5160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.73      0.60      3943\n",
      "           1       0.53      0.42      0.47      1671\n",
      "           2       0.52      0.16      0.25      1174\n",
      "           3       0.45      0.43      0.44      1778\n",
      "           4       0.46      0.29      0.35      1329\n",
      "           5       0.69      0.71      0.70      1001\n",
      "\n",
      "    accuracy                           0.52     10896\n",
      "   macro avg       0.53      0.46      0.47     10896\n",
      "weighted avg       0.51      0.52      0.49     10896\n",
      "\n",
      "val loss:\t1.3840 | val acc:\t0.5841\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.82      0.71      1518\n",
      "           1       0.56      0.29      0.38       468\n",
      "           2       0.53      0.27      0.36       321\n",
      "           3       0.41      0.48      0.44       397\n",
      "           4       0.47      0.18      0.26       309\n",
      "           5       0.75      0.75      0.75       252\n",
      "\n",
      "    accuracy                           0.58      3265\n",
      "   macro avg       0.56      0.46      0.48      3265\n",
      "weighted avg       0.57      0.58      0.55      3265\n",
      "\n",
      "Epoch 19/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2228 | train acc:\t0.5245\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.74      0.61      3943\n",
      "           1       0.57      0.44      0.50      1671\n",
      "           2       0.52      0.18      0.27      1174\n",
      "           3       0.45      0.44      0.44      1778\n",
      "           4       0.47      0.28      0.35      1329\n",
      "           5       0.70      0.71      0.70      1001\n",
      "\n",
      "    accuracy                           0.52     10896\n",
      "   macro avg       0.54      0.46      0.48     10896\n",
      "weighted avg       0.52      0.52      0.50     10896\n",
      "\n",
      "val loss:\t1.3453 | val acc:\t0.5764\n",
      "\n",
      "Epoch 20/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2148 | train acc:\t0.5268\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.73      0.61      3943\n",
      "           1       0.57      0.43      0.49      1671\n",
      "           2       0.51      0.16      0.25      1174\n",
      "           3       0.46      0.46      0.46      1778\n",
      "           4       0.47      0.30      0.36      1329\n",
      "           5       0.70      0.73      0.72      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.54      0.47      0.48     10896\n",
      "weighted avg       0.53      0.53      0.51     10896\n",
      "\n",
      "val loss:\t1.4261 | val acc:\t0.5498\n",
      "\n",
      "Epoch 21/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2148 | train acc:\t0.5259\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.73      0.61      3943\n",
      "           1       0.56      0.43      0.49      1671\n",
      "           2       0.55      0.20      0.30      1174\n",
      "           3       0.45      0.44      0.45      1778\n",
      "           4       0.48      0.31      0.38      1329\n",
      "           5       0.69      0.69      0.69      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.54      0.47      0.48     10896\n",
      "weighted avg       0.53      0.53      0.51     10896\n",
      "\n",
      "val loss:\t1.3442 | val acc:\t0.5804\n",
      "\n",
      "Epoch 22/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2015 | train acc:\t0.5266\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.72      0.60      3943\n",
      "           1       0.57      0.43      0.49      1671\n",
      "           2       0.54      0.21      0.31      1174\n",
      "           3       0.45      0.45      0.45      1778\n",
      "           4       0.46      0.31      0.37      1329\n",
      "           5       0.69      0.72      0.71      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.54      0.47      0.49     10896\n",
      "weighted avg       0.53      0.53      0.51     10896\n",
      "\n",
      "val loss:\t1.4118 | val acc:\t0.5541\n",
      "\n",
      "Epoch 23/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1973 | train acc:\t0.5340\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.61      3943\n",
      "           1       0.58      0.45      0.51      1671\n",
      "           2       0.52      0.22      0.31      1174\n",
      "           3       0.46      0.45      0.46      1778\n",
      "           4       0.48      0.31      0.38      1329\n",
      "           5       0.70      0.72      0.71      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.54      0.48      0.49     10896\n",
      "weighted avg       0.53      0.53      0.52     10896\n",
      "\n",
      "val loss:\t1.3532 | val acc:\t0.5939\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.83      0.72      1518\n",
      "           1       0.51      0.36      0.42       468\n",
      "           2       0.62      0.24      0.34       321\n",
      "           3       0.42      0.39      0.40       397\n",
      "           4       0.41      0.32      0.36       309\n",
      "           5       0.88      0.69      0.78       252\n",
      "\n",
      "    accuracy                           0.59      3265\n",
      "   macro avg       0.58      0.47      0.50      3265\n",
      "weighted avg       0.59      0.59      0.57      3265\n",
      "\n",
      "Epoch 24/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1938 | train acc:\t0.5318\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.73      0.61      3943\n",
      "           1       0.58      0.43      0.49      1671\n",
      "           2       0.52      0.22      0.31      1174\n",
      "           3       0.46      0.45      0.45      1778\n",
      "           4       0.49      0.32      0.39      1329\n",
      "           5       0.70      0.72      0.71      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.54      0.48      0.49     10896\n",
      "weighted avg       0.53      0.53      0.52     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.3893 | val acc:\t0.5623\n",
      "\n",
      "Epoch 25/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1940 | train acc:\t0.5277\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.72      0.60      3943\n",
      "           1       0.57      0.44      0.50      1671\n",
      "           2       0.52      0.20      0.29      1174\n",
      "           3       0.46      0.45      0.46      1778\n",
      "           4       0.46      0.30      0.36      1329\n",
      "           5       0.69      0.73      0.71      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.54      0.47      0.49     10896\n",
      "weighted avg       0.53      0.53      0.51     10896\n",
      "\n",
      "val loss:\t1.3772 | val acc:\t0.5856\n",
      "\n",
      "Epoch 26/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1813 | train acc:\t0.5368\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.61      3943\n",
      "           1       0.58      0.45      0.51      1671\n",
      "           2       0.51      0.21      0.30      1174\n",
      "           3       0.48      0.45      0.46      1778\n",
      "           4       0.47      0.33      0.39      1329\n",
      "           5       0.70      0.73      0.71      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.54      0.48      0.50     10896\n",
      "weighted avg       0.53      0.54      0.52     10896\n",
      "\n",
      "val loss:\t1.3678 | val acc:\t0.5911\n",
      "\n",
      "Epoch 27/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1830 | train acc:\t0.5387\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.61      3943\n",
      "           1       0.58      0.43      0.49      1671\n",
      "           2       0.55      0.23      0.32      1174\n",
      "           3       0.47      0.46      0.47      1778\n",
      "           4       0.47      0.33      0.39      1329\n",
      "           5       0.71      0.75      0.73      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.55      0.49      0.50     10896\n",
      "weighted avg       0.54      0.54      0.52     10896\n",
      "\n",
      "val loss:\t1.3422 | val acc:\t0.5982\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.81      0.72      1518\n",
      "           1       0.54      0.33      0.41       468\n",
      "           2       0.51      0.38      0.43       321\n",
      "           3       0.44      0.44      0.44       397\n",
      "           4       0.42      0.32      0.37       309\n",
      "           5       0.88      0.68      0.77       252\n",
      "\n",
      "    accuracy                           0.60      3265\n",
      "   macro avg       0.57      0.49      0.52      3265\n",
      "weighted avg       0.59      0.60      0.58      3265\n",
      "\n",
      "Epoch 28/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1753 | train acc:\t0.5376\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.73      0.61      3943\n",
      "           1       0.60      0.45      0.51      1671\n",
      "           2       0.52      0.21      0.30      1174\n",
      "           3       0.48      0.46      0.47      1778\n",
      "           4       0.47      0.32      0.38      1329\n",
      "           5       0.70      0.73      0.71      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.55      0.48      0.50     10896\n",
      "weighted avg       0.54      0.54      0.52     10896\n",
      "\n",
      "val loss:\t1.3362 | val acc:\t0.5963\n",
      "\n",
      "Epoch 29/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1781 | train acc:\t0.5423\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.61      3943\n",
      "           1       0.60      0.46      0.52      1671\n",
      "           2       0.54      0.23      0.33      1174\n",
      "           3       0.48      0.47      0.47      1778\n",
      "           4       0.47      0.32      0.38      1329\n",
      "           5       0.70      0.74      0.72      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.55      0.49      0.51     10896\n",
      "weighted avg       0.54      0.54      0.53     10896\n",
      "\n",
      "val loss:\t1.3146 | val acc:\t0.5896\n",
      "\n",
      "Epoch 30/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1693 | train acc:\t0.5445\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.62      3943\n",
      "           1       0.59      0.44      0.51      1671\n",
      "           2       0.57      0.25      0.35      1174\n",
      "           3       0.47      0.47      0.47      1778\n",
      "           4       0.48      0.33      0.40      1329\n",
      "           5       0.71      0.74      0.72      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.56      0.49      0.51     10896\n",
      "weighted avg       0.55      0.54      0.53     10896\n",
      "\n",
      "val loss:\t1.3284 | val acc:\t0.5988\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.85      0.72      1518\n",
      "           1       0.50      0.38      0.43       468\n",
      "           2       0.61      0.30      0.40       321\n",
      "           3       0.46      0.32      0.38       397\n",
      "           4       0.44      0.26      0.33       309\n",
      "           5       0.81      0.74      0.77       252\n",
      "\n",
      "    accuracy                           0.60      3265\n",
      "   macro avg       0.57      0.47      0.51      3265\n",
      "weighted avg       0.58      0.60      0.57      3265\n",
      "\n",
      "Epoch 31/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1614 | train acc:\t0.5437\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.62      3943\n",
      "           1       0.60      0.43      0.50      1671\n",
      "           2       0.56      0.25      0.35      1174\n",
      "           3       0.49      0.48      0.48      1778\n",
      "           4       0.47      0.33      0.39      1329\n",
      "           5       0.69      0.73      0.71      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.56      0.49      0.51     10896\n",
      "weighted avg       0.55      0.54      0.53     10896\n",
      "\n",
      "val loss:\t1.3147 | val acc:\t0.5957\n",
      "\n",
      "Epoch 32/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1584 | train acc:\t0.5469\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.72      0.62      3943\n",
      "           1       0.61      0.46      0.52      1671\n",
      "           2       0.55      0.27      0.36      1174\n",
      "           3       0.47      0.46      0.46      1778\n",
      "           4       0.49      0.34      0.40      1329\n",
      "           5       0.71      0.73      0.72      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.56      0.50      0.52     10896\n",
      "weighted avg       0.55      0.55      0.53     10896\n",
      "\n",
      "val loss:\t1.2855 | val acc:\t0.5914\n",
      "\n",
      "Epoch 33/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1577 | train acc:\t0.5512\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.61      0.45      0.52      1671\n",
      "           2       0.55      0.27      0.36      1174\n",
      "           3       0.47      0.48      0.47      1778\n",
      "           4       0.50      0.36      0.42      1329\n",
      "           5       0.71      0.73      0.72      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.56      0.50      0.52     10896\n",
      "weighted avg       0.55      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.3324 | val acc:\t0.6003\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.81      0.72      1518\n",
      "           1       0.49      0.38      0.43       468\n",
      "           2       0.55      0.33      0.41       321\n",
      "           3       0.43      0.42      0.42       397\n",
      "           4       0.48      0.29      0.37       309\n",
      "           5       0.79      0.76      0.77       252\n",
      "\n",
      "    accuracy                           0.60      3265\n",
      "   macro avg       0.56      0.50      0.52      3265\n",
      "weighted avg       0.59      0.60      0.58      3265\n",
      "\n",
      "Epoch 34/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1514 | train acc:\t0.5495\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.60      0.44      0.50      1671\n",
      "           2       0.57      0.27      0.36      1174\n",
      "           3       0.48      0.48      0.48      1778\n",
      "           4       0.49      0.36      0.41      1329\n",
      "           5       0.71      0.73      0.72      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.56      0.50      0.52     10896\n",
      "weighted avg       0.55      0.55      0.54     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.3243 | val acc:\t0.5957\n",
      "\n",
      "Epoch 35/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1495 | train acc:\t0.5530\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.60      0.45      0.51      1671\n",
      "           2       0.55      0.27      0.37      1174\n",
      "           3       0.49      0.49      0.49      1778\n",
      "           4       0.49      0.35      0.41      1329\n",
      "           5       0.72      0.75      0.73      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.56      0.51      0.52     10896\n",
      "weighted avg       0.55      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.3264 | val acc:\t0.5991\n",
      "\n",
      "Epoch 36/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1446 | train acc:\t0.5555\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.62      0.46      0.53      1671\n",
      "           2       0.54      0.27      0.36      1174\n",
      "           3       0.50      0.49      0.49      1778\n",
      "           4       0.50      0.36      0.42      1329\n",
      "           5       0.71      0.74      0.72      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.57      0.51      0.52     10896\n",
      "weighted avg       0.56      0.56      0.54     10896\n",
      "\n",
      "val loss:\t1.3186 | val acc:\t0.6025\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.81      0.72      1518\n",
      "           1       0.57      0.32      0.41       468\n",
      "           2       0.58      0.35      0.43       321\n",
      "           3       0.47      0.45      0.46       397\n",
      "           4       0.40      0.40      0.40       309\n",
      "           5       0.91      0.67      0.77       252\n",
      "\n",
      "    accuracy                           0.60      3265\n",
      "   macro avg       0.59      0.50      0.53      3265\n",
      "weighted avg       0.60      0.60      0.59      3265\n",
      "\n",
      "Epoch 37/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1428 | train acc:\t0.5568\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.62      3943\n",
      "           1       0.62      0.46      0.53      1671\n",
      "           2       0.58      0.28      0.37      1174\n",
      "           3       0.50      0.48      0.49      1778\n",
      "           4       0.49      0.36      0.42      1329\n",
      "           5       0.72      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.57      0.51      0.53     10896\n",
      "weighted avg       0.56      0.56      0.54     10896\n",
      "\n",
      "val loss:\t1.3225 | val acc:\t0.6064\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.82      0.72      1518\n",
      "           1       0.52      0.37      0.43       468\n",
      "           2       0.61      0.34      0.43       321\n",
      "           3       0.45      0.45      0.45       397\n",
      "           4       0.46      0.33      0.38       309\n",
      "           5       0.86      0.71      0.78       252\n",
      "\n",
      "    accuracy                           0.61      3265\n",
      "   macro avg       0.59      0.50      0.53      3265\n",
      "weighted avg       0.60      0.61      0.59      3265\n",
      "\n",
      "Epoch 38/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1301 | train acc:\t0.5589\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63      3943\n",
      "           1       0.61      0.46      0.52      1671\n",
      "           2       0.55      0.28      0.37      1174\n",
      "           3       0.50      0.50      0.50      1778\n",
      "           4       0.51      0.36      0.42      1329\n",
      "           5       0.71      0.74      0.72      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.57      0.51      0.53     10896\n",
      "weighted avg       0.56      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.3004 | val acc:\t0.6028\n",
      "\n",
      "Epoch 39/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1282 | train acc:\t0.5602\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.62      0.44      0.52      1671\n",
      "           2       0.59      0.29      0.39      1174\n",
      "           3       0.49      0.49      0.49      1778\n",
      "           4       0.51      0.37      0.43      1329\n",
      "           5       0.71      0.74      0.72      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.51      0.53     10896\n",
      "weighted avg       0.56      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.3041 | val acc:\t0.6040\n",
      "\n",
      "Epoch 40/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1340 | train acc:\t0.5585\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.72      0.62      3943\n",
      "           1       0.61      0.45      0.52      1671\n",
      "           2       0.59      0.30      0.40      1174\n",
      "           3       0.49      0.49      0.49      1778\n",
      "           4       0.50      0.38      0.43      1329\n",
      "           5       0.72      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.52      0.53     10896\n",
      "weighted avg       0.56      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.2999 | val acc:\t0.6025\n",
      "\n",
      "Epoch 41/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1213 | train acc:\t0.5591\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.63      0.46      0.53      1671\n",
      "           2       0.58      0.29      0.38      1174\n",
      "           3       0.50      0.49      0.49      1778\n",
      "           4       0.50      0.37      0.42      1329\n",
      "           5       0.72      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.51      0.53     10896\n",
      "weighted avg       0.56      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.2945 | val acc:\t0.5982\n",
      "\n",
      "Epoch 42/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1156 | train acc:\t0.5669\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.63      0.45      0.53      1671\n",
      "           2       0.61      0.31      0.41      1174\n",
      "           3       0.50      0.48      0.49      1778\n",
      "           4       0.51      0.38      0.44      1329\n",
      "           5       0.72      0.76      0.74      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.52      0.54     10896\n",
      "weighted avg       0.57      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.3032 | val acc:\t0.6113\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.81      0.72      1518\n",
      "           1       0.53      0.42      0.47       468\n",
      "           2       0.54      0.43      0.47       321\n",
      "           3       0.49      0.39      0.43       397\n",
      "           4       0.46      0.31      0.37       309\n",
      "           5       0.82      0.75      0.78       252\n",
      "\n",
      "    accuracy                           0.61      3265\n",
      "   macro avg       0.58      0.52      0.54      3265\n",
      "weighted avg       0.60      0.61      0.60      3265\n",
      "\n",
      "Epoch 43/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1201 | train acc:\t0.5661\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63      3943\n",
      "           1       0.61      0.47      0.53      1671\n",
      "           2       0.57      0.30      0.39      1174\n",
      "           3       0.50      0.48      0.49      1778\n",
      "           4       0.53      0.41      0.46      1329\n",
      "           5       0.72      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.58      0.52      0.54     10896\n",
      "weighted avg       0.57      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.3110 | val acc:\t0.5954\n",
      "\n",
      "Epoch 44/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1086 | train acc:\t0.5693\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.63      0.47      0.54      1671\n",
      "           2       0.59      0.31      0.40      1174\n",
      "           3       0.50      0.49      0.50      1778\n",
      "           4       0.52      0.40      0.46      1329\n",
      "           5       0.73      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.53      0.54     10896\n",
      "weighted avg       0.57      0.57      0.56     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.2757 | val acc:\t0.6012\n",
      "\n",
      "Epoch 45/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1066 | train acc:\t0.5729\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.63      3943\n",
      "           1       0.63      0.47      0.54      1671\n",
      "           2       0.58      0.31      0.41      1174\n",
      "           3       0.51      0.51      0.51      1778\n",
      "           4       0.52      0.41      0.46      1329\n",
      "           5       0.73      0.76      0.74      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.53      0.55     10896\n",
      "weighted avg       0.58      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.2883 | val acc:\t0.6095\n",
      "\n",
      "Epoch 46/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1100 | train acc:\t0.5739\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.63      0.46      0.53      1671\n",
      "           2       0.58      0.31      0.41      1174\n",
      "           3       0.51      0.50      0.51      1778\n",
      "           4       0.54      0.40      0.46      1329\n",
      "           5       0.75      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.53      0.55     10896\n",
      "weighted avg       0.58      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.2717 | val acc:\t0.6052\n",
      "\n",
      "Epoch 47/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1003 | train acc:\t0.5769\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.64      3943\n",
      "           1       0.65      0.47      0.54      1671\n",
      "           2       0.59      0.33      0.43      1174\n",
      "           3       0.52      0.50      0.51      1778\n",
      "           4       0.54      0.40      0.46      1329\n",
      "           5       0.74      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.53      0.55     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2799 | val acc:\t0.6101\n",
      "\n",
      "Epoch 48/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1017 | train acc:\t0.5779\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.64      0.46      0.53      1671\n",
      "           2       0.60      0.32      0.42      1174\n",
      "           3       0.52      0.51      0.52      1778\n",
      "           4       0.53      0.41      0.46      1329\n",
      "           5       0.73      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.55     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2733 | val acc:\t0.6138\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.83      0.72      1518\n",
      "           1       0.51      0.43      0.47       468\n",
      "           2       0.58      0.37      0.45       321\n",
      "           3       0.49      0.38      0.43       397\n",
      "           4       0.51      0.27      0.35       309\n",
      "           5       0.80      0.78      0.79       252\n",
      "\n",
      "    accuracy                           0.61      3265\n",
      "   macro avg       0.59      0.51      0.54      3265\n",
      "weighted avg       0.60      0.61      0.59      3265\n",
      "\n",
      "Epoch 49/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0912 | train acc:\t0.5763\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.64      0.46      0.54      1671\n",
      "           2       0.60      0.34      0.43      1174\n",
      "           3       0.52      0.51      0.51      1778\n",
      "           4       0.54      0.40      0.46      1329\n",
      "           5       0.74      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.53      0.55     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2748 | val acc:\t0.6058\n",
      "\n",
      "Epoch 50/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0901 | train acc:\t0.5815\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      3943\n",
      "           1       0.65      0.48      0.55      1671\n",
      "           2       0.60      0.33      0.43      1174\n",
      "           3       0.53      0.51      0.52      1778\n",
      "           4       0.53      0.40      0.46      1329\n",
      "           5       0.74      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.59      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2821 | val acc:\t0.6156\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.81      0.72      1518\n",
      "           1       0.49      0.50      0.50       468\n",
      "           2       0.70      0.29      0.41       321\n",
      "           3       0.48      0.44      0.46       397\n",
      "           4       0.49      0.31      0.38       309\n",
      "           5       0.88      0.73      0.80       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.62      0.51      0.54      3265\n",
      "weighted avg       0.61      0.62      0.60      3265\n",
      "\n",
      "Epoch 51/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0881 | train acc:\t0.5817\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.64      0.47      0.54      1671\n",
      "           2       0.60      0.34      0.43      1174\n",
      "           3       0.52      0.51      0.52      1778\n",
      "           4       0.55      0.41      0.47      1329\n",
      "           5       0.73      0.77      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.59      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2805 | val acc:\t0.6156\n",
      "\n",
      "Epoch 52/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0947 | train acc:\t0.5766\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.63      3943\n",
      "           1       0.65      0.48      0.55      1671\n",
      "           2       0.62      0.35      0.44      1174\n",
      "           3       0.51      0.51      0.51      1778\n",
      "           4       0.52      0.40      0.45      1329\n",
      "           5       0.73      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2560 | val acc:\t0.6141\n",
      "\n",
      "Epoch 53/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0812 | train acc:\t0.5818\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.63      0.48      0.54      1671\n",
      "           2       0.61      0.35      0.44      1174\n",
      "           3       0.52      0.51      0.52      1778\n",
      "           4       0.55      0.40      0.46      1329\n",
      "           5       0.74      0.77      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.59      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2890 | val acc:\t0.6119\n",
      "\n",
      "Epoch 54/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0782 | train acc:\t0.5820\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.66      0.48      0.55      1671\n",
      "           2       0.60      0.35      0.44      1174\n",
      "           3       0.52      0.50      0.51      1778\n",
      "           4       0.54      0.42      0.47      1329\n",
      "           5       0.74      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.59      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.3018 | val acc:\t0.5911\n",
      "\n",
      "Epoch 55/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0835 | train acc:\t0.5814\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.64      0.47      0.54      1671\n",
      "           2       0.61      0.35      0.45      1174\n",
      "           3       0.52      0.51      0.52      1778\n",
      "           4       0.52      0.40      0.45      1329\n",
      "           5       0.75      0.76      0.76      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.59      0.58      0.57     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.3509 | val acc:\t0.5795\n",
      "\n",
      "Epoch 56/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0725 | train acc:\t0.5851\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      3943\n",
      "           1       0.64      0.48      0.55      1671\n",
      "           2       0.61      0.35      0.44      1174\n",
      "           3       0.51      0.52      0.52      1778\n",
      "           4       0.53      0.41      0.46      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2878 | val acc:\t0.5911\n",
      "\n",
      "Epoch 57/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0691 | train acc:\t0.5872\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.73      0.64      3943\n",
      "           1       0.65      0.49      0.56      1671\n",
      "           2       0.60      0.36      0.45      1174\n",
      "           3       0.52      0.51      0.52      1778\n",
      "           4       0.54      0.43      0.48      1329\n",
      "           5       0.74      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2771 | val acc:\t0.6064\n",
      "\n",
      "Epoch 58/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0644 | train acc:\t0.5865\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.64      3943\n",
      "           1       0.64      0.49      0.55      1671\n",
      "           2       0.63      0.36      0.46      1174\n",
      "           3       0.53      0.53      0.53      1778\n",
      "           4       0.54      0.43      0.48      1329\n",
      "           5       0.74      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2525 | val acc:\t0.6092\n",
      "\n",
      "Epoch 59/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0718 | train acc:\t0.5844\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.65      0.48      0.55      1671\n",
      "           2       0.61      0.37      0.46      1174\n",
      "           3       0.53      0.51      0.52      1778\n",
      "           4       0.55      0.42      0.48      1329\n",
      "           5       0.74      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.58      0.58     10896\n",
      "\n",
      "val loss:\t1.2949 | val acc:\t0.6116\n",
      "\n",
      "Epoch 60/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0601 | train acc:\t0.5905\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      3943\n",
      "           1       0.64      0.48      0.55      1671\n",
      "           2       0.62      0.36      0.45      1174\n",
      "           3       0.54      0.54      0.54      1778\n",
      "           4       0.57      0.44      0.50      1329\n",
      "           5       0.72      0.76      0.74      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.60      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2385 | val acc:\t0.6104\n",
      "\n",
      "Epoch 61/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0646 | train acc:\t0.5876\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65      3943\n",
      "           1       0.64      0.49      0.55      1671\n",
      "           2       0.61      0.38      0.47      1174\n",
      "           3       0.52      0.51      0.51      1778\n",
      "           4       0.52      0.42      0.46      1329\n",
      "           5       0.75      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.60      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2971 | val acc:\t0.6172\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.71      1518\n",
      "           1       0.51      0.55      0.53       468\n",
      "           2       0.57      0.39      0.47       321\n",
      "           3       0.48      0.47      0.48       397\n",
      "           4       0.45      0.46      0.45       309\n",
      "           5       0.81      0.76      0.78       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.59      0.56      0.57      3265\n",
      "weighted avg       0.62      0.62      0.61      3265\n",
      "\n",
      "Epoch 62/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0552 | train acc:\t0.5958\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65      3943\n",
      "           1       0.67      0.50      0.57      1671\n",
      "           2       0.62      0.38      0.47      1174\n",
      "           3       0.54      0.52      0.53      1778\n",
      "           4       0.54      0.46      0.50      1329\n",
      "           5       0.75      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2812 | val acc:\t0.6129\n",
      "\n",
      "Epoch 63/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0563 | train acc:\t0.5909\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      3943\n",
      "           1       0.66      0.49      0.56      1671\n",
      "           2       0.63      0.37      0.47      1174\n",
      "           3       0.54      0.52      0.53      1778\n",
      "           4       0.54      0.43      0.48      1329\n",
      "           5       0.74      0.75      0.75      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.60      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2591 | val acc:\t0.6116\n",
      "\n",
      "Epoch 64/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0549 | train acc:\t0.5934\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65      3943\n",
      "           1       0.64      0.49      0.55      1671\n",
      "           2       0.61      0.37      0.46      1174\n",
      "           3       0.53      0.53      0.53      1778\n",
      "           4       0.56      0.44      0.49      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.56      0.57     10896\n",
      "weighted avg       0.60      0.59      0.59     10896\n",
      "\n",
      "val loss:\t1.2515 | val acc:\t0.6156\n",
      "\n",
      "Epoch 65/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0465 | train acc:\t0.5945\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65      3943\n",
      "           1       0.66      0.49      0.56      1671\n",
      "           2       0.64      0.38      0.48      1174\n",
      "           3       0.54      0.53      0.53      1778\n",
      "           4       0.54      0.43      0.48      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.59      0.59     10896\n",
      "\n",
      "val loss:\t1.2532 | val acc:\t0.6205\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.82      0.73      1518\n",
      "           1       0.52      0.48      0.50       468\n",
      "           2       0.70      0.29      0.41       321\n",
      "           3       0.47      0.46      0.46       397\n",
      "           4       0.54      0.29      0.38       309\n",
      "           5       0.81      0.75      0.78       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.61      0.51      0.54      3265\n",
      "weighted avg       0.62      0.62      0.60      3265\n",
      "\n",
      "Epoch 66/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0463 | train acc:\t0.5940\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65      3943\n",
      "           1       0.66      0.49      0.56      1671\n",
      "           2       0.60      0.37      0.45      1174\n",
      "           3       0.54      0.53      0.53      1778\n",
      "           4       0.56      0.44      0.50      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.56      0.57     10896\n",
      "weighted avg       0.60      0.59      0.59     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.2772 | val acc:\t0.6055\n",
      "\n",
      "Epoch 67/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0453 | train acc:\t0.5985\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.75      0.65      3943\n",
      "           1       0.65      0.50      0.56      1671\n",
      "           2       0.64      0.38      0.48      1174\n",
      "           3       0.55      0.52      0.54      1778\n",
      "           4       0.56      0.46      0.51      1329\n",
      "           5       0.74      0.75      0.75      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2480 | val acc:\t0.6190\n",
      "\n",
      "Epoch 68/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0480 | train acc:\t0.5908\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      3943\n",
      "           1       0.66      0.49      0.56      1671\n",
      "           2       0.60      0.36      0.45      1174\n",
      "           3       0.54      0.52      0.53      1778\n",
      "           4       0.56      0.43      0.49      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.60      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2695 | val acc:\t0.6129\n",
      "\n",
      "Epoch 69/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0409 | train acc:\t0.5997\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.64      0.38      0.47      1174\n",
      "           3       0.54      0.52      0.53      1778\n",
      "           4       0.55      0.45      0.49      1329\n",
      "           5       0.76      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2530 | val acc:\t0.6208\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.83      0.73      1518\n",
      "           1       0.57      0.40      0.47       468\n",
      "           2       0.59      0.36      0.45       321\n",
      "           3       0.48      0.46      0.47       397\n",
      "           4       0.50      0.30      0.38       309\n",
      "           5       0.84      0.75      0.79       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.61      0.52      0.55      3265\n",
      "weighted avg       0.61      0.62      0.60      3265\n",
      "\n",
      "Epoch 70/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0314 | train acc:\t0.6007\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.67      0.50      0.58      1671\n",
      "           2       0.62      0.41      0.49      1174\n",
      "           3       0.55      0.53      0.54      1778\n",
      "           4       0.56      0.44      0.50      1329\n",
      "           5       0.74      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2290 | val acc:\t0.6242\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.83      0.72      1518\n",
      "           1       0.56      0.45      0.50       468\n",
      "           2       0.60      0.37      0.46       321\n",
      "           3       0.51      0.39      0.44       397\n",
      "           4       0.52      0.34      0.41       309\n",
      "           5       0.84      0.75      0.79       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.61      0.52      0.55      3265\n",
      "weighted avg       0.61      0.62      0.61      3265\n",
      "\n",
      "Epoch 71/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0313 | train acc:\t0.6045\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.64      0.38      0.48      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.57      0.47      0.52      1329\n",
      "           5       0.75      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.2794 | val acc:\t0.6110\n",
      "\n",
      "Epoch 72/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0344 | train acc:\t0.6056\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.65      0.50      0.57      1671\n",
      "           2       0.63      0.41      0.50      1174\n",
      "           3       0.55      0.52      0.53      1778\n",
      "           4       0.57      0.47      0.52      1329\n",
      "           5       0.74      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2503 | val acc:\t0.6254\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.85      0.73      1518\n",
      "           1       0.60      0.38      0.46       468\n",
      "           2       0.72      0.29      0.41       321\n",
      "           3       0.48      0.51      0.50       397\n",
      "           4       0.53      0.30      0.38       309\n",
      "           5       0.85      0.74      0.79       252\n",
      "\n",
      "    accuracy                           0.63      3265\n",
      "   macro avg       0.64      0.51      0.55      3265\n",
      "weighted avg       0.63      0.63      0.60      3265\n",
      "\n",
      "Epoch 73/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0347 | train acc:\t0.6028\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.66      0.51      0.57      1671\n",
      "           2       0.61      0.39      0.47      1174\n",
      "           3       0.55      0.53      0.54      1778\n",
      "           4       0.55      0.44      0.49      1329\n",
      "           5       0.76      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2724 | val acc:\t0.6181\n",
      "\n",
      "Epoch 74/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0255 | train acc:\t0.6081\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.66      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.63      0.39      0.48      1174\n",
      "           3       0.55      0.53      0.54      1778\n",
      "           4       0.57      0.47      0.51      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2510 | val acc:\t0.6165\n",
      "\n",
      "Epoch 75/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0231 | train acc:\t0.6034\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.63      0.38      0.48      1174\n",
      "           3       0.54      0.52      0.53      1778\n",
      "           4       0.57      0.46      0.51      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.2448 | val acc:\t0.6175\n",
      "\n",
      "Epoch 76/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0175 | train acc:\t0.6066\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.65      0.40      0.50      1174\n",
      "           3       0.56      0.53      0.55      1778\n",
      "           4       0.57      0.46      0.51      1329\n",
      "           5       0.75      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.2282 | val acc:\t0.6135\n",
      "\n",
      "Epoch 77/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0135 | train acc:\t0.6047\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      3943\n",
      "           1       0.65      0.49      0.56      1671\n",
      "           2       0.65      0.42      0.51      1174\n",
      "           3       0.55      0.53      0.54      1778\n",
      "           4       0.56      0.45      0.50      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.2237 | val acc:\t0.6221\n",
      "\n",
      "Epoch 78/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0088 | train acc:\t0.6099\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.66      3943\n",
      "           1       0.67      0.50      0.57      1671\n",
      "           2       0.63      0.40      0.49      1174\n",
      "           3       0.57      0.54      0.56      1778\n",
      "           4       0.56      0.46      0.51      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2270 | val acc:\t0.6168\n",
      "\n",
      "Epoch 79/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0248 | train acc:\t0.6023\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.66      0.50      0.57      1671\n",
      "           2       0.61      0.40      0.48      1174\n",
      "           3       0.55      0.54      0.54      1778\n",
      "           4       0.55      0.45      0.50      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.60      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.2122 | val acc:\t0.6119\n",
      "\n",
      "Epoch 80/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0138 | train acc:\t0.6073\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.64      0.40      0.49      1174\n",
      "           3       0.57      0.54      0.55      1778\n",
      "           4       0.57      0.47      0.52      1329\n",
      "           5       0.76      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2218 | val acc:\t0.6046\n",
      "\n",
      "Epoch 81/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0131 | train acc:\t0.6081\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.73      0.65      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.62      0.41      0.50      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.57      0.46      0.51      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2321 | val acc:\t0.6227\n",
      "\n",
      "Epoch 82/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9979 | train acc:\t0.6200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.65      0.44      0.53      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.59      0.47      0.52      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2378 | val acc:\t0.6104\n",
      "\n",
      "Epoch 83/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0005 | train acc:\t0.6134\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.64      0.41      0.50      1174\n",
      "           3       0.57      0.54      0.55      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.64      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.2516 | val acc:\t0.6098\n",
      "\n",
      "Epoch 84/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0004 | train acc:\t0.6151\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.63      0.41      0.50      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.58      0.48      0.53      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2122 | val acc:\t0.6221\n",
      "\n",
      "Epoch 85/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0064 | train acc:\t0.6068\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.65      3943\n",
      "           1       0.64      0.51      0.57      1671\n",
      "           2       0.64      0.42      0.50      1174\n",
      "           3       0.56      0.53      0.55      1778\n",
      "           4       0.56      0.47      0.51      1329\n",
      "           5       0.75      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.62      0.58      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2126 | val acc:\t0.6282\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.84      0.73      1518\n",
      "           1       0.57      0.42      0.48       468\n",
      "           2       0.61      0.38      0.47       321\n",
      "           3       0.53      0.41      0.46       397\n",
      "           4       0.51      0.37      0.43       309\n",
      "           5       0.87      0.74      0.80       252\n",
      "\n",
      "    accuracy                           0.63      3265\n",
      "   macro avg       0.62      0.53      0.56      3265\n",
      "weighted avg       0.62      0.63      0.61      3265\n",
      "\n",
      "Epoch 86/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0017 | train acc:\t0.6106\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.65      3943\n",
      "           1       0.67      0.52      0.58      1671\n",
      "           2       0.61      0.43      0.50      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.56      0.46      0.50      1329\n",
      "           5       0.76      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2450 | val acc:\t0.6165\n",
      "\n",
      "Epoch 87/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9972 | train acc:\t0.6169\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.63      0.44      0.52      1174\n",
      "           3       0.58      0.55      0.57      1778\n",
      "           4       0.57      0.47      0.52      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.63      0.59      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2383 | val acc:\t0.6230\n",
      "\n",
      "Epoch 88/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.9955 | train acc:\t0.6134\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.65      0.42      0.51      1174\n",
      "           3       0.57      0.53      0.55      1778\n",
      "           4       0.57      0.49      0.53      1329\n",
      "           5       0.76      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.2213 | val acc:\t0.6150\n",
      "\n",
      "Epoch 89/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9922 | train acc:\t0.6178\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.67      0.53      0.60      1671\n",
      "           2       0.61      0.41      0.49      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.58      0.49      0.53      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.63      0.59      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2387 | val acc:\t0.6141\n",
      "\n",
      "Epoch 90/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9907 | train acc:\t0.6189\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.63      0.43      0.51      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.58      0.48      0.53      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2300 | val acc:\t0.6221\n",
      "\n",
      "Epoch 91/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9909 | train acc:\t0.6224\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.69      0.53      0.60      1671\n",
      "           2       0.63      0.44      0.52      1174\n",
      "           3       0.57      0.56      0.56      1778\n",
      "           4       0.58      0.46      0.51      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1930 | val acc:\t0.6150\n",
      "\n",
      "Epoch 92/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9838 | train acc:\t0.6239\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.64      0.44      0.52      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.58      0.49      0.53      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2356 | val acc:\t0.6138\n",
      "\n",
      "Epoch 93/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9740 | train acc:\t0.6279\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.66      0.45      0.54      1174\n",
      "           3       0.58      0.55      0.57      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.2497 | val acc:\t0.6113\n",
      "\n",
      "Epoch 94/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9774 | train acc:\t0.6242\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.63      0.42      0.50      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2311 | val acc:\t0.6175\n",
      "\n",
      "Epoch 95/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9766 | train acc:\t0.6243\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.66      3943\n",
      "           1       0.67      0.53      0.60      1671\n",
      "           2       0.65      0.45      0.53      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.59      0.49      0.54      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2238 | val acc:\t0.6178\n",
      "\n",
      "Epoch 96/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9801 | train acc:\t0.6226\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.66      0.53      0.59      1671\n",
      "           2       0.63      0.44      0.52      1174\n",
      "           3       0.58      0.55      0.56      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2092 | val acc:\t0.6224\n",
      "\n",
      "Epoch 97/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9781 | train acc:\t0.6278\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.53      0.60      1671\n",
      "           2       0.65      0.46      0.54      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.2403 | val acc:\t0.6208\n",
      "\n",
      "Epoch 98/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9703 | train acc:\t0.6258\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.65      0.46      0.54      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.59      0.50      0.54      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.64      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.2478 | val acc:\t0.6101\n",
      "\n",
      "Epoch 99/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9659 | train acc:\t0.6280\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.69      0.54      0.60      1671\n",
      "           2       0.63      0.44      0.52      1174\n",
      "           3       0.59      0.58      0.58      1778\n",
      "           4       0.58      0.49      0.53      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.2352 | val acc:\t0.6159\n",
      "\n",
      "Epoch 100/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.9766 | train acc:\t0.6228\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.66      0.45      0.54      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.58      0.48      0.52      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1931 | val acc:\t0.6248\n",
      "\n",
      "Epoch 101/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9637 | train acc:\t0.6304\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.70      0.55      0.61      1671\n",
      "           2       0.65      0.43      0.52      1174\n",
      "           3       0.58      0.57      0.58      1778\n",
      "           4       0.58      0.51      0.54      1329\n",
      "           5       0.78      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.2126 | val acc:\t0.6202\n",
      "\n",
      "Epoch 102/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9600 | train acc:\t0.6278\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.53      0.59      1671\n",
      "           2       0.65      0.46      0.54      1174\n",
      "           3       0.57      0.56      0.57      1778\n",
      "           4       0.61      0.50      0.55      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.2027 | val acc:\t0.6233\n",
      "\n",
      "Epoch 103/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9639 | train acc:\t0.6262\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.64      0.45      0.53      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.64      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.2084 | val acc:\t0.6037\n",
      "\n",
      "Epoch 104/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9655 | train acc:\t0.6319\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.65      0.47      0.54      1174\n",
      "           3       0.57      0.56      0.57      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.2468 | val acc:\t0.6123\n",
      "\n",
      "Epoch 105/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9642 | train acc:\t0.6256\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.69      0.55      0.61      1671\n",
      "           2       0.66      0.45      0.54      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.58      0.48      0.52      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.2126 | val acc:\t0.6172\n",
      "\n",
      "Epoch 106/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9565 | train acc:\t0.6302\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      3943\n",
      "           1       0.67      0.54      0.59      1671\n",
      "           2       0.65      0.46      0.54      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.61      0.50      0.55      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.2076 | val acc:\t0.6214\n",
      "\n",
      "Epoch 107/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9585 | train acc:\t0.6263\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.69      0.53      0.60      1671\n",
      "           2       0.64      0.46      0.53      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.58      0.49      0.53      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.64      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.2177 | val acc:\t0.6248\n",
      "\n",
      "Epoch 108/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9499 | train acc:\t0.6326\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.67      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.64      0.44      0.52      1174\n",
      "           3       0.60      0.56      0.58      1778\n",
      "           4       0.60      0.50      0.54      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.2287 | val acc:\t0.6181\n",
      "\n",
      "Epoch 109/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9494 | train acc:\t0.6382\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.66      0.48      0.55      1174\n",
      "           3       0.60      0.57      0.59      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.2167 | val acc:\t0.6150\n",
      "\n",
      "Epoch 110/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9512 | train acc:\t0.6321\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.67      0.46      0.55      1174\n",
      "           3       0.60      0.57      0.59      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.2380 | val acc:\t0.6018\n",
      "\n",
      "Epoch 111/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9486 | train acc:\t0.6313\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.55      0.60      1671\n",
      "           2       0.66      0.46      0.54      1174\n",
      "           3       0.58      0.57      0.58      1778\n",
      "           4       0.58      0.49      0.53      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.2090 | val acc:\t0.6162\n",
      "\n",
      "Epoch 112/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.9513 | train acc:\t0.6331\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.65      0.45      0.53      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.78      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.2089 | val acc:\t0.6178\n",
      "\n",
      "Epoch 113/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9542 | train acc:\t0.6355\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.68      3943\n",
      "           1       0.68      0.56      0.62      1671\n",
      "           2       0.64      0.46      0.54      1174\n",
      "           3       0.60      0.57      0.58      1778\n",
      "           4       0.60      0.50      0.54      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.2433 | val acc:\t0.6150\n",
      "\n",
      "Epoch 114/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9426 | train acc:\t0.6381\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.66      0.47      0.55      1174\n",
      "           3       0.60      0.59      0.59      1778\n",
      "           4       0.59      0.49      0.54      1329\n",
      "           5       0.77      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.2229 | val acc:\t0.6162\n",
      "\n",
      "Epoch 115/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9473 | train acc:\t0.6381\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.67      0.55      0.61      1671\n",
      "           2       0.67      0.48      0.56      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.2277 | val acc:\t0.6260\n",
      "\n",
      "Epoch 116/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9355 | train acc:\t0.6383\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.67      0.46      0.54      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.61      0.49      0.54      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.1949 | val acc:\t0.6257\n",
      "\n",
      "Epoch 117/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9372 | train acc:\t0.6468\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.57      0.62      1671\n",
      "           2       0.67      0.48      0.56      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.60      0.52      0.56      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.1942 | val acc:\t0.6175\n",
      "\n",
      "Epoch 118/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9333 | train acc:\t0.6389\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.68      3943\n",
      "           1       0.68      0.56      0.62      1671\n",
      "           2       0.68      0.48      0.56      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.59      0.49      0.53      1329\n",
      "           5       0.79      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.1829 | val acc:\t0.6248\n",
      "\n",
      "Epoch 119/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9295 | train acc:\t0.6456\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.57      0.63      1671\n",
      "           2       0.66      0.46      0.54      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.1768 | val acc:\t0.6199\n",
      "\n",
      "Epoch 120/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9326 | train acc:\t0.6445\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.57      0.62      1671\n",
      "           2       0.65      0.49      0.56      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.62      0.52      0.57      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.2055 | val acc:\t0.6181\n",
      "\n",
      "Epoch 121/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9296 | train acc:\t0.6380\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.65      0.48      0.55      1174\n",
      "           3       0.59      0.58      0.59      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.78      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.2072 | val acc:\t0.6211\n",
      "\n",
      "Epoch 122/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9204 | train acc:\t0.6518\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.58      0.64      1671\n",
      "           2       0.67      0.50      0.57      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1683 | val acc:\t0.6312\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.84      0.73      1518\n",
      "           1       0.61      0.42      0.50       468\n",
      "           2       0.59      0.42      0.49       321\n",
      "           3       0.51      0.42      0.46       397\n",
      "           4       0.56      0.34      0.42       309\n",
      "           5       0.87      0.74      0.80       252\n",
      "\n",
      "    accuracy                           0.63      3265\n",
      "   macro avg       0.63      0.53      0.57      3265\n",
      "weighted avg       0.63      0.63      0.62      3265\n",
      "\n",
      "Epoch 123/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9281 | train acc:\t0.6474\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.67      0.49      0.56      1174\n",
      "           3       0.60      0.59      0.59      1778\n",
      "           4       0.62      0.52      0.57      1329\n",
      "           5       0.78      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1843 | val acc:\t0.6184\n",
      "\n",
      "Epoch 124/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9211 | train acc:\t0.6477\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      3943\n",
      "           1       0.69      0.57      0.62      1671\n",
      "           2       0.67      0.49      0.56      1174\n",
      "           3       0.60      0.60      0.60      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.2222 | val acc:\t0.6061\n",
      "\n",
      "Epoch 125/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9200 | train acc:\t0.6461\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.67      0.49      0.57      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.79      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.1747 | val acc:\t0.6175\n",
      "\n",
      "Epoch 126/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9084 | train acc:\t0.6510\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.69      0.51      0.58      1174\n",
      "           3       0.62      0.59      0.61      1778\n",
      "           4       0.62      0.52      0.56      1329\n",
      "           5       0.78      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.2189 | val acc:\t0.6040\n",
      "\n",
      "Epoch 127/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9100 | train acc:\t0.6523\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1698 | val acc:\t0.6227\n",
      "\n",
      "Epoch 128/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9180 | train acc:\t0.6456\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      3943\n",
      "           1       0.70      0.57      0.62      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.59      0.52      0.55      1329\n",
      "           5       0.79      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.1802 | val acc:\t0.6221\n",
      "\n",
      "Epoch 129/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9077 | train acc:\t0.6583\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.68      0.50      0.58      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.64      0.54      0.59      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1667 | val acc:\t0.6254\n",
      "\n",
      "Epoch 130/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9085 | train acc:\t0.6508\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.69      0.57      0.62      1671\n",
      "           2       0.67      0.50      0.57      1174\n",
      "           3       0.62      0.59      0.60      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1851 | val acc:\t0.6211\n",
      "\n",
      "Epoch 131/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9100 | train acc:\t0.6508\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.65      0.47      0.55      1174\n",
      "           3       0.63      0.60      0.61      1778\n",
      "           4       0.61      0.53      0.57      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.2058 | val acc:\t0.6159\n",
      "\n",
      "Epoch 132/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9112 | train acc:\t0.6517\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.63      0.59      0.61      1778\n",
      "           4       0.62      0.52      0.56      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1896 | val acc:\t0.6211\n",
      "\n",
      "Epoch 133/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9041 | train acc:\t0.6524\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.71      0.60      0.65      1671\n",
      "           2       0.67      0.49      0.57      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.62      0.52      0.57      1329\n",
      "           5       0.80      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1709 | val acc:\t0.6193\n",
      "\n",
      "Epoch 134/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9111 | train acc:\t0.6497\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.69      0.50      0.58      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1486 | val acc:\t0.6205\n",
      "\n",
      "Epoch 135/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8983 | train acc:\t0.6527\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.68      0.50      0.58      1174\n",
      "           3       0.63      0.59      0.61      1778\n",
      "           4       0.59      0.53      0.56      1329\n",
      "           5       0.79      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1535 | val acc:\t0.6175\n",
      "\n",
      "Epoch 136/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9037 | train acc:\t0.6581\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.69      0.51      0.59      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.61      0.53      0.57      1329\n",
      "           5       0.80      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1801 | val acc:\t0.6187\n",
      "\n",
      "Epoch 137/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9013 | train acc:\t0.6559\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.68      0.51      0.58      1174\n",
      "           3       0.61      0.61      0.61      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.80      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1841 | val acc:\t0.6294\n",
      "\n",
      "Epoch 138/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8950 | train acc:\t0.6551\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.69      0.53      0.60      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.61      0.53      0.57      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1711 | val acc:\t0.6270\n",
      "\n",
      "Epoch 139/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9005 | train acc:\t0.6541\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.58      0.63      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.80      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1995 | val acc:\t0.6116\n",
      "\n",
      "Epoch 140/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8889 | train acc:\t0.6629\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.71      0.59      0.65      1671\n",
      "           2       0.70      0.51      0.59      1174\n",
      "           3       0.61      0.60      0.61      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.2294 | val acc:\t0.6083\n",
      "\n",
      "Epoch 141/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8891 | train acc:\t0.6625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.70      0.53      0.61      1174\n",
      "           3       0.63      0.62      0.62      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1931 | val acc:\t0.6211\n",
      "\n",
      "Epoch 142/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8904 | train acc:\t0.6619\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.71      0.58      0.64      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.64      0.64      0.64      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1738 | val acc:\t0.6147\n",
      "\n",
      "Epoch 143/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8964 | train acc:\t0.6584\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.71      0.59      0.64      1671\n",
      "           2       0.70      0.52      0.60      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.64      0.54      0.59      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1744 | val acc:\t0.6110\n",
      "\n",
      "Epoch 144/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8891 | train acc:\t0.6598\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.68      0.51      0.58      1174\n",
      "           3       0.62      0.62      0.62      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.2538 | val acc:\t0.6052\n",
      "\n",
      "Epoch 145/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8914 | train acc:\t0.6563\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69      3943\n",
      "           1       0.69      0.59      0.64      1671\n",
      "           2       0.66      0.52      0.58      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1949 | val acc:\t0.6172\n",
      "\n",
      "Epoch 146/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8822 | train acc:\t0.6679\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.72      0.60      0.65      1671\n",
      "           2       0.70      0.54      0.61      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.65      0.55      0.59      1329\n",
      "           5       0.79      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.2061 | val acc:\t0.6205\n",
      "\n",
      "Epoch 147/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8775 | train acc:\t0.6641\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.61      0.65      1671\n",
      "           2       0.69      0.53      0.60      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.66      0.66     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.2099 | val acc:\t0.6175\n",
      "\n",
      "Epoch 148/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8704 | train acc:\t0.6685\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.71      0.59      0.64      1671\n",
      "           2       0.71      0.54      0.62      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.80      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1726 | val acc:\t0.6202\n",
      "\n",
      "Epoch 149/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8803 | train acc:\t0.6663\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69      3943\n",
      "           1       0.72      0.60      0.65      1671\n",
      "           2       0.69      0.55      0.61      1174\n",
      "           3       0.62      0.62      0.62      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.1952 | val acc:\t0.6217\n",
      "\n",
      "Epoch 150/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8685 | train acc:\t0.6680\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.70      0.54      0.61      1174\n",
      "           3       0.63      0.62      0.63      1778\n",
      "           4       0.63      0.55      0.59      1329\n",
      "           5       0.82      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1885 | val acc:\t0.6239\n",
      "\n",
      "Epoch 151/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8796 | train acc:\t0.6696\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.70      0.54      0.61      1174\n",
      "           3       0.63      0.62      0.62      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1741 | val acc:\t0.6126\n",
      "\n",
      "Epoch 152/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8813 | train acc:\t0.6645\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.69      0.52      0.59      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.80      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1750 | val acc:\t0.6196\n",
      "\n",
      "Epoch 153/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8762 | train acc:\t0.6670\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.72      0.60      0.66      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.63      0.60      0.62      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.1841 | val acc:\t0.6208\n",
      "\n",
      "Epoch 154/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8706 | train acc:\t0.6681\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.67      0.53      0.60      1174\n",
      "           3       0.63      0.62      0.63      1778\n",
      "           4       0.63      0.57      0.60      1329\n",
      "           5       0.80      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1867 | val acc:\t0.6086\n",
      "\n",
      "Epoch 155/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8672 | train acc:\t0.6661\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.71      0.55      0.62      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.63      0.53      0.57      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.1703 | val acc:\t0.6263\n",
      "\n",
      "Epoch 156/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8683 | train acc:\t0.6743\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.72      0.59      0.65      1671\n",
      "           2       0.71      0.54      0.61      1174\n",
      "           3       0.62      0.62      0.62      1778\n",
      "           4       0.63      0.57      0.60      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1861 | val acc:\t0.6168\n",
      "\n",
      "Epoch 157/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8640 | train acc:\t0.6704\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.72      0.60      0.65      1671\n",
      "           2       0.69      0.54      0.60      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.2082 | val acc:\t0.6147\n",
      "\n",
      "Epoch 158/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8706 | train acc:\t0.6730\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.72      0.60      0.65      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.64      0.63      0.64      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.80      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1518 | val acc:\t0.6168\n",
      "\n",
      "Epoch 159/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8695 | train acc:\t0.6698\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.68      0.53      0.59      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.65      0.57      0.60      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1845 | val acc:\t0.6110\n",
      "\n",
      "Epoch 160/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8641 | train acc:\t0.6720\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.72      0.60      0.65      1671\n",
      "           2       0.69      0.55      0.61      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.80      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1533 | val acc:\t0.6165\n",
      "\n",
      "Epoch 161/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8571 | train acc:\t0.6758\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.71      0.61      0.66      1671\n",
      "           2       0.70      0.55      0.61      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.64      0.55      0.59      1329\n",
      "           5       0.81      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.1754 | val acc:\t0.6331\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.73      1518\n",
      "           1       0.59      0.44      0.51       468\n",
      "           2       0.56      0.44      0.49       321\n",
      "           3       0.47      0.54      0.51       397\n",
      "           4       0.49      0.47      0.48       309\n",
      "           5       0.87      0.77      0.82       252\n",
      "\n",
      "    accuracy                           0.63      3265\n",
      "   macro avg       0.61      0.57      0.59      3265\n",
      "weighted avg       0.63      0.63      0.63      3265\n",
      "\n",
      "Epoch 162/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8584 | train acc:\t0.6709\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.71      0.55      0.62      1174\n",
      "           3       0.62      0.61      0.62      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.80      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1490 | val acc:\t0.6245\n",
      "\n",
      "Epoch 163/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8622 | train acc:\t0.6684\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.71      0.61      0.65      1671\n",
      "           2       0.66      0.53      0.59      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.63      0.55      0.59      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1909 | val acc:\t0.6211\n",
      "\n",
      "Epoch 164/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8488 | train acc:\t0.6769\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.69      0.56      0.62      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.65      0.55      0.60      1329\n",
      "           5       0.80      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.1888 | val acc:\t0.6083\n",
      "\n",
      "Epoch 165/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8619 | train acc:\t0.6735\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.71      0.56      0.62      1174\n",
      "           3       0.64      0.63      0.63      1778\n",
      "           4       0.61      0.54      0.57      1329\n",
      "           5       0.80      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.68      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.2037 | val acc:\t0.6123\n",
      "\n",
      "Epoch 166/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8481 | train acc:\t0.6743\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.71      0.61      0.65      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.66      0.57      0.61      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1738 | val acc:\t0.6147\n",
      "\n",
      "Epoch 167/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8491 | train acc:\t0.6779\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.71      0.61      0.66      1671\n",
      "           2       0.71      0.55      0.62      1174\n",
      "           3       0.64      0.63      0.63      1778\n",
      "           4       0.65      0.56      0.60      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1601 | val acc:\t0.6193\n",
      "\n",
      "Epoch 168/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8439 | train acc:\t0.6801\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.71      0.57      0.63      1174\n",
      "           3       0.64      0.63      0.64      1778\n",
      "           4       0.66      0.57      0.61      1329\n",
      "           5       0.81      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1754 | val acc:\t0.6190\n",
      "\n",
      "Epoch 169/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8538 | train acc:\t0.6789\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.73      0.61      0.66      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.63      0.56      0.59      1329\n",
      "           5       0.81      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1627 | val acc:\t0.6168\n",
      "\n",
      "Epoch 170/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8459 | train acc:\t0.6779\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.71      0.56      0.63      1174\n",
      "           3       0.64      0.63      0.63      1778\n",
      "           4       0.65      0.58      0.61      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1672 | val acc:\t0.6153\n",
      "\n",
      "Epoch 171/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.8511 | train acc:\t0.6753\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.71      0.61      0.66      1671\n",
      "           2       0.69      0.55      0.61      1174\n",
      "           3       0.63      0.62      0.63      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.1740 | val acc:\t0.6021\n",
      "\n",
      "Epoch 172/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8454 | train acc:\t0.6794\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.73      0.62      0.67      1671\n",
      "           2       0.68      0.54      0.60      1174\n",
      "           3       0.66      0.63      0.64      1778\n",
      "           4       0.65      0.58      0.61      1329\n",
      "           5       0.80      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1766 | val acc:\t0.6135\n",
      "\n",
      "Epoch 173/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8462 | train acc:\t0.6806\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.73      0.55      0.63      1174\n",
      "           3       0.65      0.64      0.64      1778\n",
      "           4       0.63      0.57      0.60      1329\n",
      "           5       0.81      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1442 | val acc:\t0.6208\n",
      "\n",
      "Epoch 174/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8456 | train acc:\t0.6738\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.72      0.60      0.65      1671\n",
      "           2       0.70      0.54      0.61      1174\n",
      "           3       0.64      0.63      0.63      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.80      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.68      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1650 | val acc:\t0.6159\n",
      "\n",
      "Epoch 175/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8430 | train acc:\t0.6808\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.69      0.56      0.62      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.65      0.58      0.62      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.2032 | val acc:\t0.6061\n",
      "\n",
      "Epoch 176/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8440 | train acc:\t0.6735\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.69      0.54      0.61      1174\n",
      "           3       0.64      0.63      0.63      1778\n",
      "           4       0.63      0.55      0.59      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.68      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1619 | val acc:\t0.6196\n",
      "\n",
      "Epoch 177/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8431 | train acc:\t0.6778\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.72      0.60      0.66      1671\n",
      "           2       0.72      0.58      0.64      1174\n",
      "           3       0.64      0.64      0.64      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1653 | val acc:\t0.6119\n",
      "\n",
      "Epoch 178/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8419 | train acc:\t0.6836\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.67      0.63      0.65      1778\n",
      "           4       0.63      0.56      0.59      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.69      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1395 | val acc:\t0.6205\n",
      "\n",
      "Epoch 179/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8491 | train acc:\t0.6830\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.71      0.62      0.67      1671\n",
      "           2       0.71      0.57      0.63      1174\n",
      "           3       0.66      0.63      0.64      1778\n",
      "           4       0.65      0.57      0.61      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1456 | val acc:\t0.6187\n",
      "\n",
      "Epoch 180/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8412 | train acc:\t0.6756\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.70      0.61      0.65      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.65      0.62      0.63      1778\n",
      "           4       0.65      0.57      0.61      1329\n",
      "           5       0.81      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.1723 | val acc:\t0.6178\n",
      "\n",
      "Epoch 181/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8368 | train acc:\t0.6850\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.74      0.62      0.67      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.66      0.65      0.66      1778\n",
      "           4       0.64      0.57      0.60      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.1785 | val acc:\t0.6055\n",
      "\n",
      "Epoch 182/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8340 | train acc:\t0.6792\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.66      0.63      0.64      1778\n",
      "           4       0.64      0.57      0.60      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1731 | val acc:\t0.6052\n",
      "\n",
      "Epoch 183/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.8379 | train acc:\t0.6814\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.71      3943\n",
      "           1       0.73      0.62      0.67      1671\n",
      "           2       0.71      0.56      0.62      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.65      0.56      0.60      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1536 | val acc:\t0.6104\n",
      "\n",
      "Epoch 184/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8368 | train acc:\t0.6832\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.70      0.55      0.62      1174\n",
      "           3       0.65      0.64      0.64      1778\n",
      "           4       0.66      0.58      0.62      1329\n",
      "           5       0.80      0.85      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1629 | val acc:\t0.6129\n",
      "\n",
      "Epoch 185/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8324 | train acc:\t0.6860\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.66      0.58      0.62      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.1562 | val acc:\t0.6242\n",
      "\n",
      "Epoch 186/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8152 | train acc:\t0.6944\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.74      0.64      0.69      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.67      0.66      0.66      1778\n",
      "           4       0.66      0.58      0.62      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1453 | val acc:\t0.6208\n",
      "\n",
      "Epoch 187/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8256 | train acc:\t0.6811\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.70      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.64      0.58      0.61      1329\n",
      "           5       0.81      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1563 | val acc:\t0.6181\n",
      "\n",
      "Epoch 188/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8241 | train acc:\t0.6840\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.72      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.70      0.55      0.62      1174\n",
      "           3       0.66      0.63      0.65      1778\n",
      "           4       0.66      0.57      0.61      1329\n",
      "           5       0.79      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.69      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1855 | val acc:\t0.6172\n",
      "\n",
      "Epoch 189/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8281 | train acc:\t0.6825\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.69      0.56      0.62      1174\n",
      "           3       0.64      0.63      0.64      1778\n",
      "           4       0.65      0.59      0.62      1329\n",
      "           5       0.82      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1616 | val acc:\t0.6196\n",
      "\n",
      "Epoch 190/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8389 | train acc:\t0.6819\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.65      0.64      0.64      1778\n",
      "           4       0.62      0.57      0.60      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1833 | val acc:\t0.6168\n",
      "\n",
      "Epoch 191/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8317 | train acc:\t0.6802\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.73      0.63      0.67      1671\n",
      "           2       0.70      0.58      0.63      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.65      0.56      0.60      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1631 | val acc:\t0.6138\n",
      "\n",
      "Epoch 192/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8258 | train acc:\t0.6876\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.73      0.63      0.68      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.66      0.56      0.60      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1947 | val acc:\t0.6135\n",
      "\n",
      "Epoch 193/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8181 | train acc:\t0.6909\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.74      0.63      0.68      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.66      0.65      0.65      1778\n",
      "           4       0.66      0.57      0.61      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1436 | val acc:\t0.6239\n",
      "\n",
      "Epoch 194/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8238 | train acc:\t0.6815\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.71      0.61      0.66      1671\n",
      "           2       0.71      0.57      0.63      1174\n",
      "           3       0.64      0.63      0.64      1778\n",
      "           4       0.64      0.57      0.60      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1841 | val acc:\t0.6116\n",
      "\n",
      "Epoch 195/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.8226 | train acc:\t0.6872\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.71      3943\n",
      "           1       0.71      0.63      0.67      1671\n",
      "           2       0.71      0.57      0.64      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.67      0.59      0.63      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1538 | val acc:\t0.6245\n",
      "\n",
      "Epoch 196/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8062 | train acc:\t0.6918\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.71      3943\n",
      "           1       0.74      0.64      0.68      1671\n",
      "           2       0.71      0.55      0.62      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.66      0.60      0.62      1329\n",
      "           5       0.81      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1711 | val acc:\t0.6009\n",
      "\n",
      "Epoch 197/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8188 | train acc:\t0.6869\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.69      0.58      0.63      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.66      0.58      0.62      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.1699 | val acc:\t0.6141\n",
      "\n",
      "Epoch 198/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8137 | train acc:\t0.6924\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.71      3943\n",
      "           1       0.73      0.63      0.67      1671\n",
      "           2       0.70      0.58      0.63      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1563 | val acc:\t0.6297\n",
      "\n",
      "Epoch 199/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8162 | train acc:\t0.6932\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.73      0.63      0.68      1671\n",
      "           2       0.72      0.56      0.63      1174\n",
      "           3       0.65      0.66      0.65      1778\n",
      "           4       0.66      0.58      0.62      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.70      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1508 | val acc:\t0.6230\n",
      "\n",
      "Epoch 200/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8142 | train acc:\t0.6931\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.72      0.63      0.68      1671\n",
      "           2       0.70      0.58      0.63      1174\n",
      "           3       0.66      0.65      0.66      1778\n",
      "           4       0.66      0.58      0.62      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1595 | val acc:\t0.6132\n",
      "\n",
      "Training complete in 30m 8s\n",
      "Best val acc: 0.633078\n",
      "\t conv1.weight\n",
      "\t conv1.bias\n",
      "\t conv2.weight\n",
      "\t conv2.bias\n",
      "\t conv3.weight\n",
      "\t conv3.bias\n",
      "\t linear1.weight\n",
      "\t linear1.bias\n",
      "\t linear2.weight\n",
      "\t linear2.bias\n",
      "Epoch 1/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.5975 | train acc:\t0.3738\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.92      0.53      3943\n",
      "           1       0.27      0.05      0.08      1671\n",
      "           2       0.22      0.01      0.01      1174\n",
      "           3       0.31      0.09      0.14      1778\n",
      "           4       0.14      0.00      0.00      1329\n",
      "           5       0.56      0.20      0.29      1001\n",
      "\n",
      "    accuracy                           0.37     10896\n",
      "   macro avg       0.31      0.21      0.18     10896\n",
      "weighted avg       0.32      0.37      0.26     10896\n",
      "\n",
      "val loss:\t1.4850 | val acc:\t0.5084\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.95      0.66      1518\n",
      "           1       0.50      0.12      0.20       468\n",
      "           2       0.00      0.00      0.00       321\n",
      "           3       0.41      0.16      0.23       397\n",
      "           4       0.00      0.00      0.00       309\n",
      "           5       0.82      0.37      0.51       252\n",
      "\n",
      "    accuracy                           0.51      3265\n",
      "   macro avg       0.37      0.27      0.27      3265\n",
      "weighted avg       0.42      0.51      0.40      3265\n",
      "\n",
      "Epoch 2/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.4279 | train acc:\t0.4284\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.77      0.55      3943\n",
      "           1       0.36      0.22      0.28      1671\n",
      "           2       0.67      0.01      0.01      1174\n",
      "           3       0.35      0.32      0.33      1778\n",
      "           4       0.41      0.04      0.08      1329\n",
      "           5       0.60      0.62      0.61      1001\n",
      "\n",
      "    accuracy                           0.43     10896\n",
      "   macro avg       0.47      0.33      0.31     10896\n",
      "weighted avg       0.45      0.43      0.36     10896\n",
      "\n",
      "val loss:\t1.4135 | val acc:\t0.5369\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.84      0.68      1518\n",
      "           1       0.46      0.23      0.31       468\n",
      "           2       0.00      0.00      0.00       321\n",
      "           3       0.36      0.40      0.38       397\n",
      "           4       0.46      0.07      0.12       309\n",
      "           5       0.65      0.75      0.69       252\n",
      "\n",
      "    accuracy                           0.54      3265\n",
      "   macro avg       0.42      0.38      0.36      3265\n",
      "weighted avg       0.47      0.54      0.47      3265\n",
      "\n",
      "Epoch 3/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3565 | train acc:\t0.4648\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.75      0.57      3943\n",
      "           1       0.47      0.32      0.38      1671\n",
      "           2       0.45      0.03      0.06      1174\n",
      "           3       0.40      0.40      0.40      1778\n",
      "           4       0.40      0.13      0.19      1329\n",
      "           5       0.62      0.64      0.63      1001\n",
      "\n",
      "    accuracy                           0.46     10896\n",
      "   macro avg       0.47      0.38      0.37     10896\n",
      "weighted avg       0.46      0.46      0.42     10896\n",
      "\n",
      "val loss:\t1.3964 | val acc:\t0.5136\n",
      "\n",
      "Epoch 4/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3242 | train acc:\t0.4814\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.73      0.58      3943\n",
      "           1       0.48      0.38      0.42      1671\n",
      "           2       0.50      0.06      0.11      1174\n",
      "           3       0.41      0.39      0.40      1778\n",
      "           4       0.41      0.19      0.26      1329\n",
      "           5       0.64      0.69      0.67      1001\n",
      "\n",
      "    accuracy                           0.48     10896\n",
      "   macro avg       0.49      0.41      0.41     10896\n",
      "weighted avg       0.48      0.48      0.45     10896\n",
      "\n",
      "val loss:\t1.3423 | val acc:\t0.5482\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.78      0.68      1518\n",
      "           1       0.44      0.40      0.42       468\n",
      "           2       0.50      0.21      0.29       321\n",
      "           3       0.36      0.44      0.39       397\n",
      "           4       0.43      0.20      0.27       309\n",
      "           5       0.90      0.51      0.65       252\n",
      "\n",
      "    accuracy                           0.55      3265\n",
      "   macro avg       0.54      0.42      0.45      3265\n",
      "weighted avg       0.55      0.55      0.53      3265\n",
      "\n",
      "Epoch 5/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t1.2935 | train acc:\t0.4907\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.73      0.59      3943\n",
      "           1       0.51      0.40      0.45      1671\n",
      "           2       0.49      0.10      0.17      1174\n",
      "           3       0.41      0.39      0.40      1778\n",
      "           4       0.42      0.22      0.29      1329\n",
      "           5       0.67      0.69      0.68      1001\n",
      "\n",
      "    accuracy                           0.49     10896\n",
      "   macro avg       0.50      0.42      0.43     10896\n",
      "weighted avg       0.49      0.49      0.46     10896\n",
      "\n",
      "val loss:\t1.3262 | val acc:\t0.5525\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.86      0.69      1518\n",
      "           1       0.56      0.18      0.28       468\n",
      "           2       0.61      0.15      0.24       321\n",
      "           3       0.43      0.35      0.38       397\n",
      "           4       0.37      0.33      0.35       309\n",
      "           5       0.88      0.48      0.62       252\n",
      "\n",
      "    accuracy                           0.55      3265\n",
      "   macro avg       0.57      0.39      0.43      3265\n",
      "weighted avg       0.56      0.55      0.51      3265\n",
      "\n",
      "Epoch 6/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2681 | train acc:\t0.5020\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.72      0.59      3943\n",
      "           1       0.52      0.40      0.45      1671\n",
      "           2       0.53      0.16      0.25      1174\n",
      "           3       0.42      0.41      0.42      1778\n",
      "           4       0.44      0.26      0.33      1329\n",
      "           5       0.68      0.69      0.69      1001\n",
      "\n",
      "    accuracy                           0.50     10896\n",
      "   macro avg       0.52      0.44      0.45     10896\n",
      "weighted avg       0.50      0.50      0.48     10896\n",
      "\n",
      "val loss:\t1.3299 | val acc:\t0.5436\n",
      "\n",
      "Epoch 7/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2496 | train acc:\t0.5134\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.72      0.59      3943\n",
      "           1       0.56      0.41      0.47      1671\n",
      "           2       0.53      0.18      0.27      1174\n",
      "           3       0.44      0.44      0.44      1778\n",
      "           4       0.45      0.29      0.35      1329\n",
      "           5       0.69      0.69      0.69      1001\n",
      "\n",
      "    accuracy                           0.51     10896\n",
      "   macro avg       0.53      0.45      0.47     10896\n",
      "weighted avg       0.51      0.51      0.49     10896\n",
      "\n",
      "val loss:\t1.2895 | val acc:\t0.5691\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.81      0.69      1518\n",
      "           1       0.47      0.33      0.39       468\n",
      "           2       0.47      0.28      0.35       321\n",
      "           3       0.43      0.36      0.39       397\n",
      "           4       0.43      0.25      0.32       309\n",
      "           5       0.85      0.67      0.75       252\n",
      "\n",
      "    accuracy                           0.57      3265\n",
      "   macro avg       0.54      0.45      0.48      3265\n",
      "weighted avg       0.55      0.57      0.55      3265\n",
      "\n",
      "Epoch 8/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2422 | train acc:\t0.5134\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.72      0.59      3943\n",
      "           1       0.55      0.41      0.47      1671\n",
      "           2       0.51      0.16      0.25      1174\n",
      "           3       0.45      0.45      0.45      1778\n",
      "           4       0.47      0.28      0.35      1329\n",
      "           5       0.69      0.70      0.69      1001\n",
      "\n",
      "    accuracy                           0.51     10896\n",
      "   macro avg       0.53      0.45      0.47     10896\n",
      "weighted avg       0.51      0.51      0.49     10896\n",
      "\n",
      "val loss:\t1.2706 | val acc:\t0.5706\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.77      0.69      1518\n",
      "           1       0.44      0.39      0.42       468\n",
      "           2       0.50      0.26      0.35       321\n",
      "           3       0.44      0.36      0.40       397\n",
      "           4       0.37      0.33      0.35       309\n",
      "           5       0.86      0.68      0.76       252\n",
      "\n",
      "    accuracy                           0.57      3265\n",
      "   macro avg       0.54      0.47      0.49      3265\n",
      "weighted avg       0.56      0.57      0.56      3265\n",
      "\n",
      "Epoch 9/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2379 | train acc:\t0.5125\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.73      0.60      3943\n",
      "           1       0.55      0.40      0.47      1671\n",
      "           2       0.53      0.18      0.27      1174\n",
      "           3       0.43      0.43      0.43      1778\n",
      "           4       0.44      0.28      0.35      1329\n",
      "           5       0.69      0.69      0.69      1001\n",
      "\n",
      "    accuracy                           0.51     10896\n",
      "   macro avg       0.53      0.45      0.47     10896\n",
      "weighted avg       0.51      0.51      0.49     10896\n",
      "\n",
      "val loss:\t1.2527 | val acc:\t0.5743\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.83      0.70      1518\n",
      "           1       0.42      0.49      0.45       468\n",
      "           2       0.61      0.19      0.29       321\n",
      "           3       0.41      0.28      0.33       397\n",
      "           4       0.57      0.11      0.18       309\n",
      "           5       0.82      0.71      0.76       252\n",
      "\n",
      "    accuracy                           0.57      3265\n",
      "   macro avg       0.57      0.44      0.45      3265\n",
      "weighted avg       0.57      0.57      0.54      3265\n",
      "\n",
      "Epoch 10/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2206 | train acc:\t0.5229\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.72      0.60      3943\n",
      "           1       0.57      0.41      0.48      1671\n",
      "           2       0.52      0.21      0.30      1174\n",
      "           3       0.45      0.45      0.45      1778\n",
      "           4       0.47      0.30      0.37      1329\n",
      "           5       0.70      0.71      0.70      1001\n",
      "\n",
      "    accuracy                           0.52     10896\n",
      "   macro avg       0.54      0.47      0.48     10896\n",
      "weighted avg       0.52      0.52      0.51     10896\n",
      "\n",
      "val loss:\t1.2902 | val acc:\t0.5672\n",
      "\n",
      "Epoch 11/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2004 | train acc:\t0.5277\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.72      0.60      3943\n",
      "           1       0.57      0.42      0.48      1671\n",
      "           2       0.55      0.22      0.31      1174\n",
      "           3       0.45      0.45      0.45      1778\n",
      "           4       0.48      0.32      0.38      1329\n",
      "           5       0.70      0.74      0.72      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.54      0.48      0.49     10896\n",
      "weighted avg       0.53      0.53      0.51     10896\n",
      "\n",
      "val loss:\t1.3008 | val acc:\t0.5467\n",
      "\n",
      "Epoch 12/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2002 | train acc:\t0.5313\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.72      0.60      3943\n",
      "           1       0.58      0.43      0.49      1671\n",
      "           2       0.54      0.23      0.32      1174\n",
      "           3       0.46      0.45      0.45      1778\n",
      "           4       0.48      0.34      0.40      1329\n",
      "           5       0.70      0.72      0.71      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.55      0.48      0.50     10896\n",
      "weighted avg       0.53      0.53      0.52     10896\n",
      "\n",
      "val loss:\t1.2404 | val acc:\t0.5688\n",
      "\n",
      "Epoch 13/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1863 | train acc:\t0.5341\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.71      0.60      3943\n",
      "           1       0.59      0.43      0.50      1671\n",
      "           2       0.56      0.23      0.33      1174\n",
      "           3       0.47      0.47      0.47      1778\n",
      "           4       0.49      0.35      0.41      1329\n",
      "           5       0.68      0.72      0.70      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.55      0.48      0.50     10896\n",
      "weighted avg       0.54      0.53      0.52     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.2435 | val acc:\t0.5715\n",
      "\n",
      "Epoch 14/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1728 | train acc:\t0.5484\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.60      0.43      0.50      1671\n",
      "           2       0.56      0.26      0.36      1174\n",
      "           3       0.48      0.48      0.48      1778\n",
      "           4       0.48      0.36      0.42      1329\n",
      "           5       0.71      0.74      0.72      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.56      0.50      0.52     10896\n",
      "weighted avg       0.55      0.55      0.53     10896\n",
      "\n",
      "val loss:\t1.2468 | val acc:\t0.5923\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.85      0.71      1518\n",
      "           1       0.66      0.23      0.34       468\n",
      "           2       0.49      0.27      0.35       321\n",
      "           3       0.44      0.51      0.47       397\n",
      "           4       0.51      0.22      0.31       309\n",
      "           5       0.85      0.69      0.76       252\n",
      "\n",
      "    accuracy                           0.59      3265\n",
      "   macro avg       0.59      0.46      0.49      3265\n",
      "weighted avg       0.59      0.59      0.56      3265\n",
      "\n",
      "Epoch 15/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1637 | train acc:\t0.5453\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.62      3943\n",
      "           1       0.60      0.41      0.49      1671\n",
      "           2       0.55      0.25      0.34      1174\n",
      "           3       0.48      0.49      0.48      1778\n",
      "           4       0.49      0.36      0.42      1329\n",
      "           5       0.72      0.73      0.73      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.56      0.50      0.51     10896\n",
      "weighted avg       0.55      0.55      0.53     10896\n",
      "\n",
      "val loss:\t1.1997 | val acc:\t0.5954\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.80      0.71      1518\n",
      "           1       0.55      0.36      0.43       468\n",
      "           2       0.46      0.35      0.40       321\n",
      "           3       0.44      0.48      0.46       397\n",
      "           4       0.49      0.27      0.35       309\n",
      "           5       0.86      0.71      0.78       252\n",
      "\n",
      "    accuracy                           0.60      3265\n",
      "   macro avg       0.57      0.49      0.52      3265\n",
      "weighted avg       0.59      0.60      0.58      3265\n",
      "\n",
      "Epoch 16/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1514 | train acc:\t0.5497\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.72      0.62      3943\n",
      "           1       0.61      0.43      0.50      1671\n",
      "           2       0.57      0.27      0.36      1174\n",
      "           3       0.48      0.49      0.49      1778\n",
      "           4       0.49      0.37      0.42      1329\n",
      "           5       0.72      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.57      0.50      0.52     10896\n",
      "weighted avg       0.55      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.1879 | val acc:\t0.5933\n",
      "\n",
      "Epoch 17/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1305 | train acc:\t0.5589\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.72      0.62      3943\n",
      "           1       0.62      0.46      0.52      1671\n",
      "           2       0.57      0.29      0.38      1174\n",
      "           3       0.51      0.48      0.49      1778\n",
      "           4       0.51      0.40      0.45      1329\n",
      "           5       0.71      0.75      0.73      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.52      0.53     10896\n",
      "weighted avg       0.56      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.1752 | val acc:\t0.5948\n",
      "\n",
      "Epoch 18/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1322 | train acc:\t0.5550\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.71      0.62      3943\n",
      "           1       0.62      0.46      0.52      1671\n",
      "           2       0.56      0.30      0.39      1174\n",
      "           3       0.49      0.50      0.49      1778\n",
      "           4       0.49      0.38      0.43      1329\n",
      "           5       0.72      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.57      0.51      0.53     10896\n",
      "weighted avg       0.56      0.55      0.55     10896\n",
      "\n",
      "val loss:\t1.2128 | val acc:\t0.6074\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.80      0.72      1518\n",
      "           1       0.50      0.45      0.47       468\n",
      "           2       0.52      0.37      0.43       321\n",
      "           3       0.48      0.36      0.41       397\n",
      "           4       0.45      0.40      0.43       309\n",
      "           5       0.91      0.70      0.79       252\n",
      "\n",
      "    accuracy                           0.61      3265\n",
      "   macro avg       0.59      0.51      0.54      3265\n",
      "weighted avg       0.60      0.61      0.60      3265\n",
      "\n",
      "Epoch 19/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1196 | train acc:\t0.5622\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.62      3943\n",
      "           1       0.62      0.45      0.53      1671\n",
      "           2       0.56      0.28      0.37      1174\n",
      "           3       0.50      0.50      0.50      1778\n",
      "           4       0.52      0.40      0.45      1329\n",
      "           5       0.73      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.52      0.54     10896\n",
      "weighted avg       0.56      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.1939 | val acc:\t0.5991\n",
      "\n",
      "Epoch 20/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1201 | train acc:\t0.5640\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.72      0.62      3943\n",
      "           1       0.62      0.46      0.53      1671\n",
      "           2       0.60      0.32      0.42      1174\n",
      "           3       0.50      0.48      0.49      1778\n",
      "           4       0.51      0.41      0.45      1329\n",
      "           5       0.73      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.52      0.54     10896\n",
      "weighted avg       0.57      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.1995 | val acc:\t0.5856\n",
      "\n",
      "Epoch 21/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1027 | train acc:\t0.5708\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.72      0.62      3943\n",
      "           1       0.63      0.47      0.53      1671\n",
      "           2       0.61      0.33      0.43      1174\n",
      "           3       0.51      0.51      0.51      1778\n",
      "           4       0.52      0.40      0.45      1329\n",
      "           5       0.73      0.77      0.75      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.53      0.55     10896\n",
      "weighted avg       0.58      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.2100 | val acc:\t0.5899\n",
      "\n",
      "Epoch 22/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0968 | train acc:\t0.5733\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63      3943\n",
      "           1       0.63      0.47      0.54      1671\n",
      "           2       0.58      0.34      0.43      1174\n",
      "           3       0.51      0.50      0.51      1778\n",
      "           4       0.54      0.42      0.47      1329\n",
      "           5       0.73      0.77      0.75      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.54      0.55     10896\n",
      "weighted avg       0.58      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.2294 | val acc:\t0.5685\n",
      "\n",
      "Epoch 23/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0924 | train acc:\t0.5766\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.63      3943\n",
      "           1       0.64      0.46      0.54      1671\n",
      "           2       0.58      0.33      0.42      1174\n",
      "           3       0.53      0.51      0.52      1778\n",
      "           4       0.51      0.40      0.45      1329\n",
      "           5       0.75      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.59      0.54      0.55     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.2191 | val acc:\t0.5902\n",
      "\n",
      "Epoch 24/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0798 | train acc:\t0.5809\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.64      0.48      0.54      1671\n",
      "           2       0.60      0.35      0.44      1174\n",
      "           3       0.52      0.49      0.51      1778\n",
      "           4       0.53      0.42      0.47      1329\n",
      "           5       0.74      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.1622 | val acc:\t0.6162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.81      0.72      1518\n",
      "           1       0.61      0.36      0.45       468\n",
      "           2       0.66      0.31      0.42       321\n",
      "           3       0.45      0.54      0.49       397\n",
      "           4       0.46      0.42      0.44       309\n",
      "           5       0.86      0.71      0.78       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.62      0.52      0.55      3265\n",
      "weighted avg       0.62      0.62      0.60      3265\n",
      "\n",
      "Epoch 25/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0698 | train acc:\t0.5864\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.64      3943\n",
      "           1       0.64      0.48      0.55      1671\n",
      "           2       0.63      0.37      0.46      1174\n",
      "           3       0.53      0.52      0.52      1778\n",
      "           4       0.54      0.44      0.48      1329\n",
      "           5       0.75      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.1441 | val acc:\t0.5982\n",
      "\n",
      "Epoch 26/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0686 | train acc:\t0.5883\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.63      0.48      0.54      1671\n",
      "           2       0.60      0.35      0.44      1174\n",
      "           3       0.54      0.52      0.53      1778\n",
      "           4       0.56      0.46      0.51      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.1566 | val acc:\t0.6083\n",
      "\n",
      "Epoch 27/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0574 | train acc:\t0.5910\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      3943\n",
      "           1       0.64      0.48      0.55      1671\n",
      "           2       0.62      0.38      0.47      1174\n",
      "           3       0.52      0.51      0.52      1778\n",
      "           4       0.56      0.44      0.49      1329\n",
      "           5       0.75      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.56      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.1304 | val acc:\t0.6086\n",
      "\n",
      "Epoch 28/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0576 | train acc:\t0.5868\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.73      0.64      3943\n",
      "           1       0.63      0.48      0.54      1671\n",
      "           2       0.63      0.38      0.47      1174\n",
      "           3       0.53      0.51      0.52      1778\n",
      "           4       0.53      0.44      0.48      1329\n",
      "           5       0.76      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.1451 | val acc:\t0.6156\n",
      "\n",
      "Epoch 29/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0579 | train acc:\t0.5884\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.73      0.64      3943\n",
      "           1       0.64      0.47      0.54      1671\n",
      "           2       0.61      0.37      0.46      1174\n",
      "           3       0.53      0.54      0.53      1778\n",
      "           4       0.54      0.42      0.48      1329\n",
      "           5       0.75      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.1661 | val acc:\t0.6083\n",
      "\n",
      "Epoch 30/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0371 | train acc:\t0.5954\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.73      0.64      3943\n",
      "           1       0.65      0.49      0.56      1671\n",
      "           2       0.64      0.40      0.49      1174\n",
      "           3       0.54      0.53      0.54      1778\n",
      "           4       0.55      0.45      0.49      1329\n",
      "           5       0.74      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.1283 | val acc:\t0.6110\n",
      "\n",
      "Epoch 31/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0567 | train acc:\t0.5878\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.64      3943\n",
      "           1       0.65      0.48      0.55      1671\n",
      "           2       0.62      0.41      0.49      1174\n",
      "           3       0.54      0.52      0.53      1778\n",
      "           4       0.54      0.42      0.47      1329\n",
      "           5       0.74      0.77      0.75      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.1717 | val acc:\t0.6104\n",
      "\n",
      "Epoch 32/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0278 | train acc:\t0.6049\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.67      0.50      0.58      1671\n",
      "           2       0.63      0.41      0.50      1174\n",
      "           3       0.55      0.54      0.55      1778\n",
      "           4       0.57      0.46      0.51      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.1581 | val acc:\t0.5954\n",
      "\n",
      "Epoch 33/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0228 | train acc:\t0.6039\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.73      0.65      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.62      0.39      0.48      1174\n",
      "           3       0.55      0.54      0.54      1778\n",
      "           4       0.57      0.46      0.51      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.1603 | val acc:\t0.6046\n",
      "\n",
      "Epoch 34/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0216 | train acc:\t0.6062\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.62      0.42      0.50      1174\n",
      "           3       0.55      0.55      0.55      1778\n",
      "           4       0.58      0.45      0.50      1329\n",
      "           5       0.76      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1955 | val acc:\t0.5881\n",
      "\n",
      "Epoch 35/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t1.0093 | train acc:\t0.6064\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.73      0.65      3943\n",
      "           1       0.65      0.51      0.57      1671\n",
      "           2       0.66      0.43      0.52      1174\n",
      "           3       0.54      0.54      0.54      1778\n",
      "           4       0.56      0.47      0.51      1329\n",
      "           5       0.76      0.81      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1702 | val acc:\t0.5911\n",
      "\n",
      "Epoch 36/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0066 | train acc:\t0.6114\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.65      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.62      0.43      0.51      1174\n",
      "           3       0.56      0.55      0.56      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.1472 | val acc:\t0.5942\n",
      "\n",
      "Epoch 37/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0164 | train acc:\t0.5997\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.73      0.64      3943\n",
      "           1       0.65      0.51      0.57      1671\n",
      "           2       0.59      0.40      0.48      1174\n",
      "           3       0.56      0.53      0.54      1778\n",
      "           4       0.56      0.44      0.49      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.1248 | val acc:\t0.6049\n",
      "\n",
      "Epoch 38/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9917 | train acc:\t0.6205\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.67      0.52      0.59      1671\n",
      "           2       0.66      0.43      0.52      1174\n",
      "           3       0.57      0.54      0.55      1778\n",
      "           4       0.61      0.48      0.54      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.65      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1554 | val acc:\t0.6074\n",
      "\n",
      "Epoch 39/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9992 | train acc:\t0.6129\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.64      0.49      0.56      1671\n",
      "           2       0.65      0.44      0.52      1174\n",
      "           3       0.56      0.55      0.55      1778\n",
      "           4       0.58      0.48      0.52      1329\n",
      "           5       0.77      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.1514 | val acc:\t0.5982\n",
      "\n",
      "Epoch 40/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9933 | train acc:\t0.6152\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.73      0.65      3943\n",
      "           1       0.67      0.52      0.59      1671\n",
      "           2       0.64      0.46      0.53      1174\n",
      "           3       0.56      0.55      0.56      1778\n",
      "           4       0.57      0.48      0.52      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1758 | val acc:\t0.5844\n",
      "\n",
      "Epoch 41/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9766 | train acc:\t0.6227\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.67      0.54      0.60      1671\n",
      "           2       0.68      0.47      0.55      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.59      0.47      0.53      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.65      0.60      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1282 | val acc:\t0.6156\n",
      "\n",
      "Epoch 42/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9733 | train acc:\t0.6287\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.65      0.47      0.55      1174\n",
      "           3       0.59      0.58      0.58      1778\n",
      "           4       0.60      0.48      0.53      1329\n",
      "           5       0.77      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1342 | val acc:\t0.6015\n",
      "\n",
      "Epoch 43/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9737 | train acc:\t0.6212\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.66      0.53      0.59      1671\n",
      "           2       0.65      0.47      0.54      1174\n",
      "           3       0.57      0.54      0.56      1778\n",
      "           4       0.58      0.48      0.52      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1259 | val acc:\t0.6046\n",
      "\n",
      "Epoch 44/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9608 | train acc:\t0.6288\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.65      0.46      0.54      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1456 | val acc:\t0.5972\n",
      "\n",
      "Epoch 45/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9645 | train acc:\t0.6275\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.69      0.55      0.61      1671\n",
      "           2       0.65      0.47      0.55      1174\n",
      "           3       0.57      0.56      0.57      1778\n",
      "           4       0.59      0.49      0.53      1329\n",
      "           5       0.80      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1092 | val acc:\t0.6034\n",
      "\n",
      "Epoch 46/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9586 | train acc:\t0.6309\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.67      0.50      0.57      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.58      0.49      0.53      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1466 | val acc:\t0.6043\n",
      "\n",
      "Epoch 47/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.9568 | train acc:\t0.6357\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.68      0.55      0.60      1671\n",
      "           2       0.66      0.48      0.56      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.79      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.1267 | val acc:\t0.6049\n",
      "\n",
      "Epoch 48/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9540 | train acc:\t0.6334\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.69      0.55      0.61      1671\n",
      "           2       0.68      0.48      0.57      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.79      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1012 | val acc:\t0.6098\n",
      "\n",
      "Epoch 49/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9497 | train acc:\t0.6399\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.68      0.56      0.61      1671\n",
      "           2       0.67      0.49      0.57      1174\n",
      "           3       0.59      0.58      0.59      1778\n",
      "           4       0.62      0.51      0.56      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.1137 | val acc:\t0.6083\n",
      "\n",
      "Epoch 50/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9485 | train acc:\t0.6354\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.67      0.47      0.55      1174\n",
      "           3       0.59      0.58      0.58      1778\n",
      "           4       0.60      0.52      0.56      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.1203 | val acc:\t0.6009\n",
      "\n",
      "Epoch 51/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9531 | train acc:\t0.6316\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      3943\n",
      "           1       0.68      0.56      0.61      1671\n",
      "           2       0.65      0.47      0.55      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.60      0.50      0.54      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.0970 | val acc:\t0.5991\n",
      "\n",
      "Epoch 52/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9451 | train acc:\t0.6349\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.73      0.66      3943\n",
      "           1       0.68      0.56      0.62      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1171 | val acc:\t0.6138\n",
      "\n",
      "Epoch 53/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9388 | train acc:\t0.6371\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      3943\n",
      "           1       0.68      0.56      0.61      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.60      0.53      0.56      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.1207 | val acc:\t0.6025\n",
      "\n",
      "Epoch 54/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9447 | train acc:\t0.6400\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.68      0.56      0.61      1671\n",
      "           2       0.67      0.50      0.57      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0917 | val acc:\t0.6095\n",
      "\n",
      "Epoch 55/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9358 | train acc:\t0.6382\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      3943\n",
      "           1       0.68      0.56      0.61      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.59      0.58      0.59      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.0950 | val acc:\t0.6178\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.86      0.72      1518\n",
      "           1       0.69      0.33      0.45       468\n",
      "           2       0.54      0.35      0.42       321\n",
      "           3       0.49      0.41      0.45       397\n",
      "           4       0.57      0.35      0.43       309\n",
      "           5       0.90      0.71      0.79       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.64      0.50      0.54      3265\n",
      "weighted avg       0.62      0.62      0.59      3265\n",
      "\n",
      "Epoch 56/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9326 | train acc:\t0.6406\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.74      0.67      3943\n",
      "           1       0.67      0.55      0.60      1671\n",
      "           2       0.68      0.50      0.57      1174\n",
      "           3       0.59      0.58      0.59      1778\n",
      "           4       0.61      0.52      0.57      1329\n",
      "           5       0.79      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.1411 | val acc:\t0.6043\n",
      "\n",
      "Epoch 57/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9172 | train acc:\t0.6491\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.74      0.68      3943\n",
      "           1       0.68      0.58      0.63      1671\n",
      "           2       0.66      0.52      0.58      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0680 | val acc:\t0.6141\n",
      "\n",
      "Epoch 58/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9164 | train acc:\t0.6532\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.69      3943\n",
      "           1       0.71      0.59      0.64      1671\n",
      "           2       0.69      0.49      0.57      1174\n",
      "           3       0.60      0.59      0.59      1778\n",
      "           4       0.63      0.53      0.58      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.68      0.63      0.65     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1412 | val acc:\t0.6040\n",
      "\n",
      "Epoch 59/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9223 | train acc:\t0.6476\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.57      0.62      1671\n",
      "           2       0.66      0.49      0.57      1174\n",
      "           3       0.59      0.58      0.59      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.80      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.1108 | val acc:\t0.6009\n",
      "\n",
      "Epoch 60/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9195 | train acc:\t0.6453\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.70      0.56      0.62      1671\n",
      "           2       0.66      0.51      0.58      1174\n",
      "           3       0.60      0.59      0.60      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.80      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0731 | val acc:\t0.6003\n",
      "\n",
      "Epoch 61/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9231 | train acc:\t0.6499\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.68      0.57      0.62      1671\n",
      "           2       0.68      0.51      0.59      1174\n",
      "           3       0.61      0.58      0.60      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.78      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1094 | val acc:\t0.6181\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.82      0.71      1518\n",
      "           1       0.55      0.46      0.50       468\n",
      "           2       0.57      0.40      0.47       321\n",
      "           3       0.48      0.39      0.43       397\n",
      "           4       0.61      0.31      0.41       309\n",
      "           5       0.88      0.75      0.81       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.62      0.52      0.55      3265\n",
      "weighted avg       0.61      0.62      0.60      3265\n",
      "\n",
      "Epoch 62/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9079 | train acc:\t0.6537\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.62      0.59      0.61      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1027 | val acc:\t0.6132\n",
      "\n",
      "Epoch 63/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9003 | train acc:\t0.6566\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.69      0.55      0.61      1174\n",
      "           3       0.62      0.59      0.60      1778\n",
      "           4       0.62      0.52      0.57      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1432 | val acc:\t0.5770\n",
      "\n",
      "Epoch 64/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9125 | train acc:\t0.6508\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.66      0.51      0.57      1174\n",
      "           3       0.61      0.58      0.60      1778\n",
      "           4       0.62      0.55      0.58      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1035 | val acc:\t0.6156\n",
      "\n",
      "Epoch 65/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8957 | train acc:\t0.6584\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.58      0.63      1671\n",
      "           2       0.69      0.53      0.60      1174\n",
      "           3       0.61      0.60      0.61      1778\n",
      "           4       0.64      0.54      0.59      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1085 | val acc:\t0.6015\n",
      "\n",
      "Epoch 66/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9000 | train acc:\t0.6554\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.69      3943\n",
      "           1       0.69      0.59      0.63      1671\n",
      "           2       0.67      0.50      0.57      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.65      0.54      0.59      1329\n",
      "           5       0.79      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1112 | val acc:\t0.6025\n",
      "\n",
      "Epoch 67/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9148 | train acc:\t0.6532\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.68      0.57      0.62      1671\n",
      "           2       0.66      0.51      0.58      1174\n",
      "           3       0.62      0.59      0.60      1778\n",
      "           4       0.65      0.53      0.58      1329\n",
      "           5       0.80      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1091 | val acc:\t0.6254\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.83      0.72      1518\n",
      "           1       0.66      0.35      0.45       468\n",
      "           2       0.62      0.38      0.47       321\n",
      "           3       0.46      0.47      0.47       397\n",
      "           4       0.52      0.41      0.46       309\n",
      "           5       0.88      0.75      0.81       252\n",
      "\n",
      "    accuracy                           0.63      3265\n",
      "   macro avg       0.63      0.53      0.56      3265\n",
      "weighted avg       0.63      0.63      0.61      3265\n",
      "\n",
      "Epoch 68/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8912 | train acc:\t0.6589\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.70      0.53      0.61      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.62      0.55      0.58      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1335 | val acc:\t0.5776\n",
      "\n",
      "Epoch 69/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9066 | train acc:\t0.6524\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.74      0.68      3943\n",
      "           1       0.68      0.58      0.63      1671\n",
      "           2       0.68      0.51      0.58      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.61      0.53      0.57      1329\n",
      "           5       0.80      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0939 | val acc:\t0.6092\n",
      "\n",
      "Epoch 70/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8906 | train acc:\t0.6613\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.58      0.63      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.64      0.61      0.62      1778\n",
      "           4       0.63      0.56      0.59      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1399 | val acc:\t0.5881\n",
      "\n",
      "Epoch 71/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8898 | train acc:\t0.6593\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.68      0.54      0.60      1174\n",
      "           3       0.61      0.61      0.61      1778\n",
      "           4       0.63      0.53      0.58      1329\n",
      "           5       0.80      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.0833 | val acc:\t0.6113\n",
      "\n",
      "Epoch 72/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8830 | train acc:\t0.6624\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69      3943\n",
      "           1       0.69      0.59      0.64      1671\n",
      "           2       0.68      0.53      0.60      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.64      0.55      0.59      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1399 | val acc:\t0.6034\n",
      "\n",
      "Epoch 73/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8825 | train acc:\t0.6605\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.63      0.59      0.61      1778\n",
      "           4       0.62      0.56      0.59      1329\n",
      "           5       0.79      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.0717 | val acc:\t0.6098\n",
      "\n",
      "Epoch 74/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8928 | train acc:\t0.6608\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.67      0.53      0.59      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.63      0.53      0.58      1329\n",
      "           5       0.80      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.0995 | val acc:\t0.6092\n",
      "\n",
      "Epoch 75/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8879 | train acc:\t0.6620\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.60      0.65      1671\n",
      "           2       0.67      0.53      0.59      1174\n",
      "           3       0.62      0.59      0.60      1778\n",
      "           4       0.64      0.54      0.58      1329\n",
      "           5       0.80      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1530 | val acc:\t0.5697\n",
      "\n",
      "Epoch 76/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8761 | train acc:\t0.6616\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.67      0.52      0.58      1174\n",
      "           3       0.64      0.60      0.62      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.81      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.0964 | val acc:\t0.6070\n",
      "\n",
      "Epoch 77/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8763 | train acc:\t0.6659\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.71      0.60      0.65      1671\n",
      "           2       0.70      0.54      0.61      1174\n",
      "           3       0.61      0.61      0.61      1778\n",
      "           4       0.64      0.54      0.59      1329\n",
      "           5       0.83      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.1220 | val acc:\t0.5816\n",
      "\n",
      "Epoch 78/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8853 | train acc:\t0.6635\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.60      0.65      1671\n",
      "           2       0.69      0.53      0.60      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.63      0.56      0.59      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.0962 | val acc:\t0.6055\n",
      "\n",
      "Epoch 79/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8820 | train acc:\t0.6618\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.68      0.53      0.60      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.64      0.55      0.59      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1128 | val acc:\t0.5985\n",
      "\n",
      "Epoch 80/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8675 | train acc:\t0.6673\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.60      0.65      1671\n",
      "           2       0.69      0.54      0.61      1174\n",
      "           3       0.63      0.60      0.61      1778\n",
      "           4       0.66      0.57      0.61      1329\n",
      "           5       0.80      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.1024 | val acc:\t0.6061\n",
      "\n",
      "Epoch 81/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8786 | train acc:\t0.6652\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69      3943\n",
      "           1       0.70      0.62      0.65      1671\n",
      "           2       0.69      0.53      0.60      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.63      0.55      0.59      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1279 | val acc:\t0.5994\n",
      "\n",
      "Epoch 82/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8743 | train acc:\t0.6680\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.68      0.53      0.60      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.63      0.56      0.59      1329\n",
      "           5       0.80      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1289 | val acc:\t0.5804\n",
      "\n",
      "Epoch 83/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8697 | train acc:\t0.6605\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.74      0.69      3943\n",
      "           1       0.71      0.61      0.66      1671\n",
      "           2       0.66      0.55      0.60      1174\n",
      "           3       0.63      0.60      0.62      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1180 | val acc:\t0.5926\n",
      "\n",
      "Epoch 84/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8580 | train acc:\t0.6692\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.80      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1186 | val acc:\t0.5923\n",
      "\n",
      "Epoch 85/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8759 | train acc:\t0.6627\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69      3943\n",
      "           1       0.70      0.60      0.64      1671\n",
      "           2       0.67      0.52      0.59      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1135 | val acc:\t0.5966\n",
      "\n",
      "Epoch 86/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8558 | train acc:\t0.6762\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.69      0.54      0.61      1174\n",
      "           3       0.66      0.62      0.64      1778\n",
      "           4       0.65      0.59      0.62      1329\n",
      "           5       0.80      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.1122 | val acc:\t0.5994\n",
      "\n",
      "Epoch 87/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8636 | train acc:\t0.6662\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69      3943\n",
      "           1       0.71      0.60      0.65      1671\n",
      "           2       0.69      0.55      0.61      1174\n",
      "           3       0.63      0.62      0.62      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.80      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.1389 | val acc:\t0.5841\n",
      "\n",
      "Epoch 88/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8589 | train acc:\t0.6697\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69      3943\n",
      "           1       0.71      0.61      0.66      1671\n",
      "           2       0.68      0.55      0.61      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.66      0.55      0.60      1329\n",
      "           5       0.79      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1161 | val acc:\t0.5979\n",
      "\n",
      "Epoch 89/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8644 | train acc:\t0.6730\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.69      0.61      0.65      1671\n",
      "           2       0.70      0.54      0.61      1174\n",
      "           3       0.65      0.62      0.63      1778\n",
      "           4       0.65      0.58      0.61      1329\n",
      "           5       0.80      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.0735 | val acc:\t0.6150\n",
      "\n",
      "Epoch 90/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8731 | train acc:\t0.6636\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.74      0.69      3943\n",
      "           1       0.70      0.62      0.66      1671\n",
      "           2       0.69      0.53      0.60      1174\n",
      "           3       0.63      0.62      0.62      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.65      0.66     10896\n",
      "weighted avg       0.67      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.0916 | val acc:\t0.6159\n",
      "\n",
      "Epoch 91/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8481 | train acc:\t0.6701\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69      3943\n",
      "           1       0.70      0.61      0.65      1671\n",
      "           2       0.69      0.54      0.61      1174\n",
      "           3       0.65      0.62      0.63      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.0724 | val acc:\t0.6159\n",
      "\n",
      "Epoch 92/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8414 | train acc:\t0.6787\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.69      0.57      0.63      1174\n",
      "           3       0.66      0.63      0.64      1778\n",
      "           4       0.64      0.55      0.59      1329\n",
      "           5       0.81      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0800 | val acc:\t0.6116\n",
      "\n",
      "Epoch 93/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8542 | train acc:\t0.6762\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.69      0.60      0.65      1671\n",
      "           2       0.71      0.57      0.63      1174\n",
      "           3       0.65      0.62      0.63      1778\n",
      "           4       0.65      0.56      0.60      1329\n",
      "           5       0.80      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0656 | val acc:\t0.6119\n",
      "\n",
      "Epoch 94/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8575 | train acc:\t0.6682\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.70      0.55      0.62      1174\n",
      "           3       0.63      0.62      0.62      1778\n",
      "           4       0.65      0.55      0.60      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1116 | val acc:\t0.6040\n",
      "\n",
      "Epoch 95/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8463 | train acc:\t0.6777\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.65      0.62      0.64      1778\n",
      "           4       0.65      0.56      0.60      1329\n",
      "           5       0.82      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0792 | val acc:\t0.6101\n",
      "\n",
      "Epoch 96/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8380 | train acc:\t0.6841\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.71      0.64      0.68      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.68      0.59      0.63      1329\n",
      "           5       0.80      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0837 | val acc:\t0.6123\n",
      "\n",
      "Epoch 97/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8348 | train acc:\t0.6818\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.72      0.55      0.62      1174\n",
      "           3       0.63      0.63      0.63      1778\n",
      "           4       0.65      0.58      0.61      1329\n",
      "           5       0.81      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0817 | val acc:\t0.6077\n",
      "\n",
      "Epoch 98/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8405 | train acc:\t0.6832\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.71      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.63      0.63      0.63      1778\n",
      "           4       0.65      0.56      0.61      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1009 | val acc:\t0.6052\n",
      "\n",
      "Epoch 99/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8441 | train acc:\t0.6794\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.71      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.65      0.61      0.63      1778\n",
      "           4       0.65      0.57      0.61      1329\n",
      "           5       0.82      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0918 | val acc:\t0.6080\n",
      "\n",
      "Epoch 100/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8380 | train acc:\t0.6799\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.69      0.55      0.62      1174\n",
      "           3       0.64      0.64      0.64      1778\n",
      "           4       0.65      0.57      0.61      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0657 | val acc:\t0.6221\n",
      "\n",
      "Epoch 101/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8400 | train acc:\t0.6771\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.64      0.61      0.63      1778\n",
      "           4       0.63      0.55      0.59      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.0945 | val acc:\t0.5991\n",
      "\n",
      "Epoch 102/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8280 | train acc:\t0.6852\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.71      3943\n",
      "           1       0.70      0.62      0.66      1671\n",
      "           2       0.69      0.57      0.63      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.66      0.58      0.61      1329\n",
      "           5       0.81      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.0614 | val acc:\t0.6098\n",
      "\n",
      "Epoch 103/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8469 | train acc:\t0.6791\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.69      0.55      0.61      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.65      0.58      0.61      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1011 | val acc:\t0.5948\n",
      "\n",
      "Epoch 104/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8400 | train acc:\t0.6728\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.75      0.70      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.68      0.55      0.61      1174\n",
      "           3       0.63      0.62      0.62      1778\n",
      "           4       0.63      0.57      0.60      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.0989 | val acc:\t0.6040\n",
      "\n",
      "Epoch 105/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8334 | train acc:\t0.6814\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.70      3943\n",
      "           1       0.71      0.63      0.67      1671\n",
      "           2       0.70      0.58      0.63      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.67      0.57      0.62      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1098 | val acc:\t0.5982\n",
      "\n",
      "Epoch 106/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8295 | train acc:\t0.6836\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.70      3943\n",
      "           1       0.71      0.62      0.67      1671\n",
      "           2       0.71      0.59      0.64      1174\n",
      "           3       0.66      0.62      0.64      1778\n",
      "           4       0.65      0.57      0.61      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1144 | val acc:\t0.5850\n",
      "\n",
      "Epoch 107/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8293 | train acc:\t0.6804\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.75      0.70      3943\n",
      "           1       0.72      0.64      0.68      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.80      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0924 | val acc:\t0.6043\n",
      "\n",
      "Epoch 108/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8301 | train acc:\t0.6825\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.76      0.71      3943\n",
      "           1       0.69      0.63      0.66      1671\n",
      "           2       0.71      0.59      0.64      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.66      0.57      0.61      1329\n",
      "           5       0.81      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0699 | val acc:\t0.6138\n",
      "\n",
      "Epoch 109/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8201 | train acc:\t0.6903\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.71      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.70      0.58      0.63      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.68      0.59      0.63      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.0954 | val acc:\t0.6098\n",
      "\n",
      "Epoch 110/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8352 | train acc:\t0.6794\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.70      0.61      0.65      1671\n",
      "           2       0.70      0.58      0.63      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.81      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1115 | val acc:\t0.5795\n",
      "\n",
      "Epoch 111/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8298 | train acc:\t0.6846\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.66      0.57      0.61      1329\n",
      "           5       0.81      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1032 | val acc:\t0.5945\n",
      "\n",
      "Epoch 112/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8298 | train acc:\t0.6810\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.70      0.62      0.66      1671\n",
      "           2       0.71      0.57      0.63      1174\n",
      "           3       0.64      0.63      0.63      1778\n",
      "           4       0.64      0.57      0.60      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0812 | val acc:\t0.6172\n",
      "\n",
      "Epoch 113/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8268 | train acc:\t0.6836\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.70      0.63      0.66      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.65      0.59      0.62      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0846 | val acc:\t0.6046\n",
      "\n",
      "Epoch 114/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8325 | train acc:\t0.6787\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.75      0.70      3943\n",
      "           1       0.72      0.64      0.67      1671\n",
      "           2       0.68      0.58      0.63      1174\n",
      "           3       0.65      0.62      0.64      1778\n",
      "           4       0.64      0.56      0.59      1329\n",
      "           5       0.81      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1084 | val acc:\t0.5923\n",
      "\n",
      "Epoch 115/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8191 | train acc:\t0.6871\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.76      0.71      3943\n",
      "           1       0.70      0.62      0.66      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.65      0.58      0.61      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.1027 | val acc:\t0.6104\n",
      "\n",
      "Epoch 116/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8213 | train acc:\t0.6815\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.71      0.63      0.66      1671\n",
      "           2       0.70      0.58      0.63      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.63      0.57      0.60      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0908 | val acc:\t0.6031\n",
      "\n",
      "Epoch 117/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8211 | train acc:\t0.6900\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.71      3943\n",
      "           1       0.71      0.63      0.67      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.66      0.63      0.64      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0839 | val acc:\t0.6015\n",
      "\n",
      "Epoch 118/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8109 | train acc:\t0.6865\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.71      0.63      0.67      1671\n",
      "           2       0.71      0.57      0.64      1174\n",
      "           3       0.65      0.62      0.63      1778\n",
      "           4       0.67      0.60      0.64      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.0695 | val acc:\t0.6067\n",
      "\n",
      "Epoch 119/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8194 | train acc:\t0.6881\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.71      3943\n",
      "           1       0.71      0.63      0.67      1671\n",
      "           2       0.70      0.58      0.63      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.67      0.59      0.62      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1060 | val acc:\t0.5942\n",
      "\n",
      "Epoch 120/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8075 | train acc:\t0.6927\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.71      0.62      0.67      1671\n",
      "           2       0.70      0.58      0.64      1174\n",
      "           3       0.66      0.65      0.65      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.82      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.0924 | val acc:\t0.6034\n",
      "\n",
      "Epoch 121/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8131 | train acc:\t0.6881\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.69      0.58      0.63      1174\n",
      "           3       0.66      0.65      0.66      1778\n",
      "           4       0.67      0.59      0.63      1329\n",
      "           5       0.81      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.0715 | val acc:\t0.6055\n",
      "\n",
      "Epoch 122/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8170 | train acc:\t0.6925\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.76      0.72      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.69      0.59      0.64      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.66      0.58      0.62      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.68      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1146 | val acc:\t0.5884\n",
      "\n",
      "Epoch 123/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8095 | train acc:\t0.6953\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      3943\n",
      "           1       0.71      0.63      0.67      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.67      0.60      0.64      1329\n",
      "           5       0.82      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.69     10896\n",
      "\n",
      "val loss:\t1.0908 | val acc:\t0.5997\n",
      "\n",
      "Epoch 124/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8079 | train acc:\t0.6913\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.76      0.71      3943\n",
      "           1       0.73      0.65      0.69      1671\n",
      "           2       0.72      0.60      0.66      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.0962 | val acc:\t0.5957\n",
      "\n",
      "Epoch 125/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7996 | train acc:\t0.6934\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.76      0.72      3943\n",
      "           1       0.73      0.65      0.68      1671\n",
      "           2       0.70      0.59      0.64      1174\n",
      "           3       0.66      0.66      0.66      1778\n",
      "           4       0.65      0.59      0.62      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.0690 | val acc:\t0.6178\n",
      "\n",
      "Epoch 126/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8158 | train acc:\t0.6964\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.70      0.64      0.67      1671\n",
      "           2       0.73      0.59      0.66      1174\n",
      "           3       0.68      0.64      0.66      1778\n",
      "           4       0.67      0.59      0.63      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.69     10896\n",
      "\n",
      "val loss:\t1.1205 | val acc:\t0.5969\n",
      "\n",
      "Epoch 127/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8124 | train acc:\t0.6939\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.76      0.72      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.69      0.59      0.63      1174\n",
      "           3       0.66      0.65      0.66      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.0971 | val acc:\t0.5930\n",
      "\n",
      "Epoch 128/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8081 | train acc:\t0.6865\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.72      0.64      0.67      1671\n",
      "           2       0.70      0.60      0.65      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.66      0.58      0.62      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.0886 | val acc:\t0.6049\n",
      "\n",
      "Epoch 129/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8030 | train acc:\t0.6990\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.72      3943\n",
      "           1       0.73      0.65      0.69      1671\n",
      "           2       0.73      0.60      0.66      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.66      0.59      0.63      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0824 | val acc:\t0.6077\n",
      "\n",
      "Epoch 130/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8054 | train acc:\t0.6950\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.76      0.72      3943\n",
      "           1       0.71      0.65      0.68      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.67      0.64      0.66      1778\n",
      "           4       0.66      0.60      0.63      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.69     10896\n",
      "\n",
      "val loss:\t1.0753 | val acc:\t0.6031\n",
      "\n",
      "Epoch 131/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8042 | train acc:\t0.6966\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.74      0.64      0.69      1671\n",
      "           2       0.72      0.60      0.65      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.69     10896\n",
      "\n",
      "val loss:\t1.0986 | val acc:\t0.5923\n",
      "\n",
      "Epoch 132/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8061 | train acc:\t0.6970\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.76      0.72      3943\n",
      "           1       0.72      0.66      0.69      1671\n",
      "           2       0.71      0.59      0.64      1174\n",
      "           3       0.66      0.65      0.66      1778\n",
      "           4       0.67      0.61      0.64      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.0658 | val acc:\t0.6083\n",
      "\n",
      "Epoch 133/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7950 | train acc:\t0.6986\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.72      0.66      0.69      1671\n",
      "           2       0.72      0.60      0.66      1174\n",
      "           3       0.68      0.64      0.66      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.83      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1075 | val acc:\t0.5945\n",
      "\n",
      "Epoch 134/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7940 | train acc:\t0.7007\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.74      0.63      0.68      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.68      0.60      0.64      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1103 | val acc:\t0.6018\n",
      "\n",
      "Epoch 135/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8048 | train acc:\t0.6968\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      3943\n",
      "           1       0.71      0.64      0.67      1671\n",
      "           2       0.71      0.59      0.65      1174\n",
      "           3       0.68      0.66      0.67      1778\n",
      "           4       0.66      0.58      0.62      1329\n",
      "           5       0.82      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.69     10896\n",
      "\n",
      "val loss:\t1.0708 | val acc:\t0.6104\n",
      "\n",
      "Epoch 136/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7952 | train acc:\t0.7022\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.72      3943\n",
      "           1       0.74      0.66      0.70      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.68      0.66      0.67      1778\n",
      "           4       0.67      0.58      0.62      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.0773 | val acc:\t0.6141\n",
      "\n",
      "Epoch 137/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7965 | train acc:\t0.6979\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.76      0.72      3943\n",
      "           1       0.73      0.65      0.69      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.67      0.60      0.64      1329\n",
      "           5       0.81      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1069 | val acc:\t0.5975\n",
      "\n",
      "Epoch 138/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8018 | train acc:\t0.6956\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.73      0.63      0.68      1671\n",
      "           2       0.71      0.59      0.65      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.66      0.60      0.63      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.69     10896\n",
      "\n",
      "val loss:\t1.0864 | val acc:\t0.6028\n",
      "\n",
      "Epoch 139/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7938 | train acc:\t0.6932\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72      3943\n",
      "           1       0.72      0.64      0.68      1671\n",
      "           2       0.70      0.60      0.65      1174\n",
      "           3       0.67      0.64      0.66      1778\n",
      "           4       0.65      0.58      0.61      1329\n",
      "           5       0.82      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.0729 | val acc:\t0.6012\n",
      "\n",
      "Epoch 140/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7964 | train acc:\t0.6968\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.76      0.72      3943\n",
      "           1       0.72      0.65      0.68      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.66      0.65      0.65      1778\n",
      "           4       0.67      0.59      0.63      1329\n",
      "           5       0.82      0.86      0.84      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.69     10896\n",
      "\n",
      "val loss:\t1.0809 | val acc:\t0.5982\n",
      "\n",
      "Epoch 141/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7993 | train acc:\t0.6906\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.76      0.71      3943\n",
      "           1       0.73      0.65      0.69      1671\n",
      "           2       0.69      0.57      0.63      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.65      0.59      0.62      1329\n",
      "           5       0.82      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.68      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0680 | val acc:\t0.6006\n",
      "\n",
      "Epoch 142/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7931 | train acc:\t0.6970\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.71      0.59      0.65      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.66      0.61      0.63      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1087 | val acc:\t0.5822\n",
      "\n",
      "Epoch 143/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7908 | train acc:\t0.6990\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      3943\n",
      "           1       0.72      0.66      0.69      1671\n",
      "           2       0.71      0.61      0.66      1174\n",
      "           3       0.67      0.64      0.66      1778\n",
      "           4       0.67      0.59      0.63      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1024 | val acc:\t0.5948\n",
      "\n",
      "Epoch 144/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7905 | train acc:\t0.6980\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      3943\n",
      "           1       0.74      0.65      0.69      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.65      0.65      0.65      1778\n",
      "           4       0.67      0.59      0.63      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.0774 | val acc:\t0.6043\n",
      "\n",
      "Epoch 145/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7943 | train acc:\t0.7007\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      3943\n",
      "           1       0.73      0.65      0.69      1671\n",
      "           2       0.72      0.59      0.65      1174\n",
      "           3       0.68      0.66      0.67      1778\n",
      "           4       0.66      0.61      0.63      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.0672 | val acc:\t0.6129\n",
      "\n",
      "Epoch 146/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7794 | train acc:\t0.7017\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.73      3943\n",
      "           1       0.72      0.65      0.69      1671\n",
      "           2       0.71      0.63      0.67      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.68      0.60      0.64      1329\n",
      "           5       0.80      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.0769 | val acc:\t0.6126\n",
      "\n",
      "Epoch 147/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7886 | train acc:\t0.7017\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      3943\n",
      "           1       0.73      0.65      0.69      1671\n",
      "           2       0.72      0.59      0.65      1174\n",
      "           3       0.67      0.66      0.66      1778\n",
      "           4       0.68      0.60      0.64      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1056 | val acc:\t0.5881\n",
      "\n",
      "Epoch 148/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7829 | train acc:\t0.7025\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.72      3943\n",
      "           1       0.73      0.67      0.70      1671\n",
      "           2       0.71      0.61      0.66      1174\n",
      "           3       0.67      0.64      0.66      1778\n",
      "           4       0.65      0.60      0.62      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.0731 | val acc:\t0.6116\n",
      "\n",
      "Epoch 149/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7743 | train acc:\t0.7050\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      3943\n",
      "           1       0.73      0.66      0.69      1671\n",
      "           2       0.73      0.62      0.67      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.69      0.62      0.65      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.71      0.70     10896\n",
      "\n",
      "val loss:\t1.0699 | val acc:\t0.6070\n",
      "\n",
      "Epoch 150/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7894 | train acc:\t0.7042\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.74      0.66      0.69      1671\n",
      "           2       0.72      0.60      0.66      1174\n",
      "           3       0.67      0.66      0.67      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.82      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1481 | val acc:\t0.5648\n",
      "\n",
      "Epoch 151/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7723 | train acc:\t0.7031\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      3943\n",
      "           1       0.75      0.66      0.71      1671\n",
      "           2       0.72      0.60      0.66      1174\n",
      "           3       0.68      0.65      0.66      1778\n",
      "           4       0.66      0.60      0.63      1329\n",
      "           5       0.82      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1151 | val acc:\t0.5844\n",
      "\n",
      "Epoch 152/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7779 | train acc:\t0.7062\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.75      0.66      0.70      1671\n",
      "           2       0.71      0.61      0.65      1174\n",
      "           3       0.67      0.67      0.67      1778\n",
      "           4       0.69      0.61      0.64      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.71      0.70     10896\n",
      "\n",
      "val loss:\t1.0710 | val acc:\t0.6083\n",
      "\n",
      "Epoch 153/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7767 | train acc:\t0.7015\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      3943\n",
      "           1       0.73      0.66      0.69      1671\n",
      "           2       0.71      0.61      0.66      1174\n",
      "           3       0.67      0.66      0.67      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0901 | val acc:\t0.6141\n",
      "\n",
      "Epoch 154/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7819 | train acc:\t0.7030\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.72      0.66      0.69      1671\n",
      "           2       0.72      0.60      0.65      1174\n",
      "           3       0.68      0.65      0.66      1778\n",
      "           4       0.69      0.60      0.64      1329\n",
      "           5       0.81      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1169 | val acc:\t0.5828\n",
      "\n",
      "Epoch 155/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7848 | train acc:\t0.7009\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      3943\n",
      "           1       0.75      0.66      0.70      1671\n",
      "           2       0.71      0.61      0.66      1174\n",
      "           3       0.66      0.65      0.66      1778\n",
      "           4       0.66      0.60      0.63      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.0473 | val acc:\t0.6123\n",
      "\n",
      "Epoch 156/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7746 | train acc:\t0.7071\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.73      0.67      0.70      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.69      0.66      0.67      1778\n",
      "           4       0.69      0.60      0.64      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0519 | val acc:\t0.6168\n",
      "\n",
      "Epoch 157/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7809 | train acc:\t0.7093\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73      3943\n",
      "           1       0.75      0.66      0.70      1671\n",
      "           2       0.73      0.62      0.67      1174\n",
      "           3       0.68      0.67      0.68      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0951 | val acc:\t0.6064\n",
      "\n",
      "Epoch 158/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7777 | train acc:\t0.7030\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.73      3943\n",
      "           1       0.73      0.65      0.69      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.67      0.66      0.66      1778\n",
      "           4       0.67      0.59      0.63      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.0809 | val acc:\t0.6034\n",
      "\n",
      "Epoch 159/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7721 | train acc:\t0.7060\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.74      0.65      0.69      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.67      0.67      0.67      1778\n",
      "           4       0.68      0.61      0.64      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.71      0.70     10896\n",
      "\n",
      "val loss:\t1.0781 | val acc:\t0.5997\n",
      "\n",
      "Epoch 160/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7724 | train acc:\t0.7092\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73      3943\n",
      "           1       0.73      0.66      0.70      1671\n",
      "           2       0.71      0.59      0.65      1174\n",
      "           3       0.69      0.67      0.68      1778\n",
      "           4       0.67      0.61      0.64      1329\n",
      "           5       0.82      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1277 | val acc:\t0.5764\n",
      "\n",
      "Epoch 161/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7752 | train acc:\t0.7062\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      3943\n",
      "           1       0.74      0.67      0.70      1671\n",
      "           2       0.71      0.61      0.65      1174\n",
      "           3       0.68      0.66      0.67      1778\n",
      "           4       0.67      0.61      0.64      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.71     10896\n",
      "weighted avg       0.71      0.71      0.70     10896\n",
      "\n",
      "val loss:\t1.0876 | val acc:\t0.6034\n",
      "\n",
      "Epoch 162/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7817 | train acc:\t0.6977\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.76      0.72      3943\n",
      "           1       0.73      0.65      0.69      1671\n",
      "           2       0.70      0.60      0.65      1174\n",
      "           3       0.68      0.67      0.67      1778\n",
      "           4       0.66      0.60      0.63      1329\n",
      "           5       0.82      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.0767 | val acc:\t0.6089\n",
      "\n",
      "Epoch 163/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7740 | train acc:\t0.7036\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.73      3943\n",
      "           1       0.71      0.66      0.68      1671\n",
      "           2       0.73      0.62      0.67      1174\n",
      "           3       0.68      0.65      0.66      1778\n",
      "           4       0.68      0.60      0.64      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1504 | val acc:\t0.5825\n",
      "\n",
      "Epoch 164/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7571 | train acc:\t0.7115\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.77      0.73      3943\n",
      "           1       0.74      0.66      0.70      1671\n",
      "           2       0.71      0.62      0.66      1174\n",
      "           3       0.68      0.68      0.68      1778\n",
      "           4       0.70      0.62      0.66      1329\n",
      "           5       0.82      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0870 | val acc:\t0.6070\n",
      "\n",
      "Epoch 165/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7694 | train acc:\t0.7062\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      3943\n",
      "           1       0.75      0.67      0.71      1671\n",
      "           2       0.72      0.62      0.66      1174\n",
      "           3       0.68      0.66      0.67      1778\n",
      "           4       0.65      0.59      0.62      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.71     10896\n",
      "weighted avg       0.71      0.71      0.70     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0796 | val acc:\t0.6009\n",
      "\n",
      "Epoch 166/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7706 | train acc:\t0.7086\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73      3943\n",
      "           1       0.72      0.65      0.69      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.68      0.66      0.67      1778\n",
      "           4       0.69      0.62      0.65      1329\n",
      "           5       0.82      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0983 | val acc:\t0.6006\n",
      "\n",
      "Epoch 167/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7721 | train acc:\t0.7055\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.76      0.72      3943\n",
      "           1       0.73      0.66      0.69      1671\n",
      "           2       0.72      0.62      0.66      1174\n",
      "           3       0.68      0.67      0.68      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.81      0.86      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.71      0.70     10896\n",
      "\n",
      "val loss:\t1.0982 | val acc:\t0.5982\n",
      "\n",
      "Epoch 168/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7774 | train acc:\t0.7026\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      3943\n",
      "           1       0.74      0.66      0.70      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.68      0.66      0.67      1778\n",
      "           4       0.66      0.61      0.63      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.0589 | val acc:\t0.6141\n",
      "\n",
      "Epoch 169/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7627 | train acc:\t0.7112\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.73      3943\n",
      "           1       0.74      0.67      0.70      1671\n",
      "           2       0.72      0.63      0.67      1174\n",
      "           3       0.68      0.67      0.68      1778\n",
      "           4       0.68      0.61      0.65      1329\n",
      "           5       0.82      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0935 | val acc:\t0.5832\n",
      "\n",
      "Epoch 170/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7707 | train acc:\t0.7059\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.73      3943\n",
      "           1       0.73      0.66      0.70      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.68      0.65      0.67      1778\n",
      "           4       0.67      0.62      0.65      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.71      0.70     10896\n",
      "\n",
      "val loss:\t1.0957 | val acc:\t0.5936\n",
      "\n",
      "Epoch 171/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7746 | train acc:\t0.7080\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.73      3943\n",
      "           1       0.75      0.66      0.70      1671\n",
      "           2       0.70      0.62      0.66      1174\n",
      "           3       0.68      0.68      0.68      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0739 | val acc:\t0.6052\n",
      "\n",
      "Epoch 172/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7763 | train acc:\t0.7048\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.73      3943\n",
      "           1       0.73      0.65      0.69      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.68      0.66      0.67      1778\n",
      "           4       0.67      0.61      0.64      1329\n",
      "           5       0.81      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1326 | val acc:\t0.5804\n",
      "\n",
      "Epoch 173/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7605 | train acc:\t0.7116\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.77      0.73      3943\n",
      "           1       0.75      0.66      0.71      1671\n",
      "           2       0.72      0.62      0.67      1174\n",
      "           3       0.68      0.67      0.68      1778\n",
      "           4       0.67      0.62      0.65      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0771 | val acc:\t0.6080\n",
      "\n",
      "Epoch 174/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7633 | train acc:\t0.7104\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73      3943\n",
      "           1       0.75      0.67      0.71      1671\n",
      "           2       0.71      0.62      0.66      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.69      0.61      0.65      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0780 | val acc:\t0.6064\n",
      "\n",
      "Epoch 175/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7690 | train acc:\t0.7127\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3943\n",
      "           1       0.76      0.66      0.70      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.68      0.67      0.68      1778\n",
      "           4       0.67      0.61      0.64      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0688 | val acc:\t0.6181\n",
      "\n",
      "Epoch 176/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7551 | train acc:\t0.7113\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73      3943\n",
      "           1       0.74      0.66      0.70      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.69      0.67      0.68      1778\n",
      "           4       0.68      0.62      0.65      1329\n",
      "           5       0.82      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0852 | val acc:\t0.5991\n",
      "\n",
      "Epoch 177/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7646 | train acc:\t0.7112\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.73      3943\n",
      "           1       0.74      0.66      0.70      1671\n",
      "           2       0.72      0.59      0.65      1174\n",
      "           3       0.68      0.67      0.67      1778\n",
      "           4       0.68      0.63      0.65      1329\n",
      "           5       0.83      0.86      0.85      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0844 | val acc:\t0.6015\n",
      "\n",
      "Epoch 178/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7722 | train acc:\t0.7065\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.73      3943\n",
      "           1       0.74      0.67      0.70      1671\n",
      "           2       0.71      0.62      0.66      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.68      0.63      0.65      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0844 | val acc:\t0.6049\n",
      "\n",
      "Epoch 179/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7593 | train acc:\t0.7153\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.78      0.74      3943\n",
      "           1       0.74      0.67      0.70      1671\n",
      "           2       0.73      0.62      0.67      1174\n",
      "           3       0.68      0.67      0.68      1778\n",
      "           4       0.69      0.62      0.65      1329\n",
      "           5       0.82      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.71     10896\n",
      "\n",
      "val loss:\t1.0964 | val acc:\t0.5994\n",
      "\n",
      "Epoch 180/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7644 | train acc:\t0.7099\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.73      3943\n",
      "           1       0.74      0.67      0.70      1671\n",
      "           2       0.72      0.62      0.67      1174\n",
      "           3       0.69      0.66      0.67      1778\n",
      "           4       0.69      0.62      0.65      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0628 | val acc:\t0.6067\n",
      "\n",
      "Epoch 181/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7598 | train acc:\t0.7126\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.78      0.74      3943\n",
      "           1       0.72      0.66      0.69      1671\n",
      "           2       0.71      0.62      0.66      1174\n",
      "           3       0.67      0.66      0.67      1778\n",
      "           4       0.69      0.65      0.67      1329\n",
      "           5       0.84      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0715 | val acc:\t0.6009\n",
      "\n",
      "Epoch 182/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7596 | train acc:\t0.7155\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73      3943\n",
      "           1       0.74      0.67      0.71      1671\n",
      "           2       0.72      0.62      0.67      1174\n",
      "           3       0.69      0.67      0.68      1778\n",
      "           4       0.68      0.62      0.65      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.71     10896\n",
      "\n",
      "val loss:\t1.0978 | val acc:\t0.5905\n",
      "\n",
      "Epoch 183/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7612 | train acc:\t0.7122\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73      3943\n",
      "           1       0.74      0.66      0.70      1671\n",
      "           2       0.74      0.61      0.67      1174\n",
      "           3       0.67      0.67      0.67      1778\n",
      "           4       0.68      0.62      0.65      1329\n",
      "           5       0.84      0.85      0.85      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0982 | val acc:\t0.5841\n",
      "\n",
      "Epoch 184/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7579 | train acc:\t0.7136\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73      3943\n",
      "           1       0.75      0.67      0.71      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.68      0.67      0.68      1778\n",
      "           4       0.70      0.63      0.67      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0819 | val acc:\t0.5997\n",
      "\n",
      "Epoch 185/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7604 | train acc:\t0.7116\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.73      3943\n",
      "           1       0.75      0.67      0.70      1671\n",
      "           2       0.72      0.63      0.67      1174\n",
      "           3       0.70      0.68      0.69      1778\n",
      "           4       0.67      0.61      0.64      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0765 | val acc:\t0.5963\n",
      "\n",
      "Epoch 186/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7604 | train acc:\t0.7143\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.78      0.74      3943\n",
      "           1       0.74      0.65      0.69      1671\n",
      "           2       0.72      0.63      0.67      1174\n",
      "           3       0.68      0.68      0.68      1778\n",
      "           4       0.70      0.62      0.66      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0865 | val acc:\t0.5954\n",
      "\n",
      "Epoch 187/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7556 | train acc:\t0.7104\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.72      3943\n",
      "           1       0.74      0.66      0.70      1671\n",
      "           2       0.73      0.64      0.68      1174\n",
      "           3       0.68      0.67      0.67      1778\n",
      "           4       0.69      0.62      0.65      1329\n",
      "           5       0.82      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0896 | val acc:\t0.6101\n",
      "\n",
      "Epoch 188/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7559 | train acc:\t0.7145\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73      3943\n",
      "           1       0.75      0.66      0.70      1671\n",
      "           2       0.73      0.61      0.67      1174\n",
      "           3       0.70      0.68      0.69      1778\n",
      "           4       0.69      0.63      0.66      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1099 | val acc:\t0.5982\n",
      "\n",
      "Epoch 189/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7628 | train acc:\t0.7092\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.73      3943\n",
      "           1       0.75      0.66      0.70      1671\n",
      "           2       0.71      0.62      0.66      1174\n",
      "           3       0.68      0.67      0.68      1778\n",
      "           4       0.68      0.62      0.65      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0675 | val acc:\t0.6070\n",
      "\n",
      "Epoch 190/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7593 | train acc:\t0.7099\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.77      0.73      3943\n",
      "           1       0.74      0.67      0.70      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.69      0.64      0.66      1329\n",
      "           5       0.82      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0745 | val acc:\t0.6172\n",
      "\n",
      "Epoch 191/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7620 | train acc:\t0.7131\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.77      0.73      3943\n",
      "           1       0.74      0.68      0.71      1671\n",
      "           2       0.69      0.60      0.64      1174\n",
      "           3       0.70      0.68      0.69      1778\n",
      "           4       0.68      0.62      0.65      1329\n",
      "           5       0.82      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1238 | val acc:\t0.5792\n",
      "\n",
      "Epoch 192/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7628 | train acc:\t0.7100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73      3943\n",
      "           1       0.72      0.65      0.68      1671\n",
      "           2       0.73      0.63      0.68      1174\n",
      "           3       0.68      0.65      0.67      1778\n",
      "           4       0.69      0.63      0.66      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0873 | val acc:\t0.6107\n",
      "\n",
      "Epoch 193/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7568 | train acc:\t0.7110\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.73      3943\n",
      "           1       0.74      0.67      0.70      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.69      0.68      0.68      1778\n",
      "           4       0.67      0.62      0.64      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0578 | val acc:\t0.6000\n",
      "\n",
      "Epoch 194/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7516 | train acc:\t0.7092\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.76      0.72      3943\n",
      "           1       0.72      0.67      0.69      1671\n",
      "           2       0.72      0.63      0.67      1174\n",
      "           3       0.69      0.66      0.68      1778\n",
      "           4       0.68      0.63      0.66      1329\n",
      "           5       0.82      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0916 | val acc:\t0.5767\n",
      "\n",
      "Epoch 195/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7588 | train acc:\t0.7082\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.77      0.73      3943\n",
      "           1       0.74      0.67      0.70      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.68      0.66      0.67      1778\n",
      "           4       0.67      0.62      0.64      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0663 | val acc:\t0.6046\n",
      "\n",
      "Epoch 196/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7665 | train acc:\t0.7094\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73      3943\n",
      "           1       0.73      0.67      0.70      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.69      0.66      0.67      1778\n",
      "           4       0.68      0.61      0.64      1329\n",
      "           5       0.82      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0765 | val acc:\t0.6028\n",
      "\n",
      "Epoch 197/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7564 | train acc:\t0.7078\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73      3943\n",
      "           1       0.74      0.66      0.70      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.67      0.67      0.67      1778\n",
      "           4       0.67      0.59      0.63      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0661 | val acc:\t0.6061\n",
      "\n",
      "Epoch 198/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7602 | train acc:\t0.7180\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.78      0.74      3943\n",
      "           1       0.75      0.67      0.71      1671\n",
      "           2       0.72      0.62      0.67      1174\n",
      "           3       0.70      0.68      0.69      1778\n",
      "           4       0.70      0.63      0.66      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.72     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.0993 | val acc:\t0.5969\n",
      "\n",
      "Epoch 199/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7661 | train acc:\t0.7073\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.73      3943\n",
      "           1       0.73      0.66      0.69      1671\n",
      "           2       0.71      0.61      0.66      1174\n",
      "           3       0.68      0.67      0.68      1778\n",
      "           4       0.67      0.62      0.64      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1305 | val acc:\t0.5825\n",
      "\n",
      "Epoch 200/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7374 | train acc:\t0.7223\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.78      0.74      3943\n",
      "           1       0.76      0.67      0.71      1671\n",
      "           2       0.72      0.63      0.67      1174\n",
      "           3       0.69      0.68      0.69      1778\n",
      "           4       0.70      0.64      0.67      1329\n",
      "           5       0.83      0.86      0.85      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.71      0.72     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.0596 | val acc:\t0.6028\n",
      "\n",
      "Training complete in 30m 8s\n",
      "Best val acc: 0.625421\n",
      "\t conv1.weight\n",
      "\t conv1.bias\n",
      "\t conv2.weight\n",
      "\t conv2.bias\n",
      "\t conv3.weight\n",
      "\t conv3.bias\n",
      "\t linear1.weight\n",
      "\t linear1.bias\n",
      "\t linear2.weight\n",
      "\t linear2.bias\n",
      "Epoch 1/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.5838 | train acc:\t0.3781\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.91      0.54      3943\n",
      "           1       0.32      0.11      0.16      1671\n",
      "           2       0.00      0.00      0.00      1174\n",
      "           3       0.32      0.11      0.16      1778\n",
      "           4       0.50      0.00      0.00      1329\n",
      "           5       0.54      0.17      0.26      1001\n",
      "\n",
      "    accuracy                           0.38     10896\n",
      "   macro avg       0.34      0.22      0.19     10896\n",
      "weighted avg       0.35      0.38      0.27     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.4478 | val acc:\t0.5145\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.88      0.67      1518\n",
      "           1       0.40      0.25      0.31       468\n",
      "           2       0.00      0.00      0.00       321\n",
      "           3       0.33      0.28      0.31       397\n",
      "           4       0.57      0.01      0.03       309\n",
      "           5       0.76      0.42      0.54       252\n",
      "\n",
      "    accuracy                           0.51      3265\n",
      "   macro avg       0.43      0.31      0.31      3265\n",
      "weighted avg       0.46      0.51      0.44      3265\n",
      "\n",
      "Epoch 2/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.4090 | train acc:\t0.4441\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.77      0.57      3943\n",
      "           1       0.41      0.29      0.34      1671\n",
      "           2       0.42      0.01      0.02      1174\n",
      "           3       0.36      0.35      0.36      1778\n",
      "           4       0.36      0.05      0.09      1329\n",
      "           5       0.59      0.60      0.60      1001\n",
      "\n",
      "    accuracy                           0.44     10896\n",
      "   macro avg       0.43      0.35      0.33     10896\n",
      "weighted avg       0.43      0.44      0.38     10896\n",
      "\n",
      "val loss:\t1.3922 | val acc:\t0.5292\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.81      0.67      1518\n",
      "           1       0.39      0.51      0.44       468\n",
      "           2       1.00      0.02      0.04       321\n",
      "           3       0.37      0.15      0.21       397\n",
      "           4       0.41      0.18      0.25       309\n",
      "           5       0.77      0.56      0.65       252\n",
      "\n",
      "    accuracy                           0.53      3265\n",
      "   macro avg       0.58      0.37      0.38      3265\n",
      "weighted avg       0.56      0.53      0.48      3265\n",
      "\n",
      "Epoch 3/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3487 | train acc:\t0.4681\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.76      0.57      3943\n",
      "           1       0.48      0.34      0.40      1671\n",
      "           2       0.57      0.02      0.05      1174\n",
      "           3       0.40      0.39      0.39      1778\n",
      "           4       0.39      0.14      0.21      1329\n",
      "           5       0.64      0.63      0.64      1001\n",
      "\n",
      "    accuracy                           0.47     10896\n",
      "   macro avg       0.49      0.38      0.38     10896\n",
      "weighted avg       0.47      0.47      0.42     10896\n",
      "\n",
      "val loss:\t1.3482 | val acc:\t0.5289\n",
      "\n",
      "Epoch 4/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3215 | train acc:\t0.4841\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.76      0.59      3943\n",
      "           1       0.50      0.38      0.43      1671\n",
      "           2       0.48      0.05      0.09      1174\n",
      "           3       0.43      0.40      0.41      1778\n",
      "           4       0.43      0.17      0.24      1329\n",
      "           5       0.64      0.65      0.65      1001\n",
      "\n",
      "    accuracy                           0.48     10896\n",
      "   macro avg       0.49      0.40      0.40     10896\n",
      "weighted avg       0.48      0.48      0.44     10896\n",
      "\n",
      "val loss:\t1.3141 | val acc:\t0.5660\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.79      0.69      1518\n",
      "           1       0.48      0.40      0.44       468\n",
      "           2       0.55      0.17      0.26       321\n",
      "           3       0.39      0.43      0.41       397\n",
      "           4       0.40      0.21      0.27       309\n",
      "           5       0.81      0.69      0.74       252\n",
      "\n",
      "    accuracy                           0.57      3265\n",
      "   macro avg       0.54      0.45      0.47      3265\n",
      "weighted avg       0.55      0.57      0.54      3265\n",
      "\n",
      "Epoch 5/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3109 | train acc:\t0.4861\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.74      0.58      3943\n",
      "           1       0.51      0.39      0.44      1671\n",
      "           2       0.53      0.07      0.13      1174\n",
      "           3       0.43      0.41      0.42      1778\n",
      "           4       0.39      0.20      0.27      1329\n",
      "           5       0.65      0.65      0.65      1001\n",
      "\n",
      "    accuracy                           0.49     10896\n",
      "   macro avg       0.50      0.41      0.42     10896\n",
      "weighted avg       0.49      0.49      0.45     10896\n",
      "\n",
      "val loss:\t1.4280 | val acc:\t0.5590\n",
      "\n",
      "Epoch 6/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2696 | train acc:\t0.5019\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.73      0.59      3943\n",
      "           1       0.52      0.38      0.44      1671\n",
      "           2       0.51      0.14      0.22      1174\n",
      "           3       0.43      0.42      0.43      1778\n",
      "           4       0.45      0.26      0.33      1329\n",
      "           5       0.68      0.69      0.69      1001\n",
      "\n",
      "    accuracy                           0.50     10896\n",
      "   macro avg       0.51      0.44      0.45     10896\n",
      "weighted avg       0.50      0.50      0.48     10896\n",
      "\n",
      "val loss:\t1.3303 | val acc:\t0.5498\n",
      "\n",
      "Epoch 7/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2546 | train acc:\t0.5088\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.73      0.59      3943\n",
      "           1       0.55      0.42      0.48      1671\n",
      "           2       0.53      0.12      0.19      1174\n",
      "           3       0.44      0.44      0.44      1778\n",
      "           4       0.44      0.26      0.33      1329\n",
      "           5       0.68      0.70      0.69      1001\n",
      "\n",
      "    accuracy                           0.51     10896\n",
      "   macro avg       0.52      0.44      0.45     10896\n",
      "weighted avg       0.51      0.51      0.48     10896\n",
      "\n",
      "val loss:\t1.3439 | val acc:\t0.5283\n",
      "\n",
      "Epoch 8/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2377 | train acc:\t0.5162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.71      0.59      3943\n",
      "           1       0.56      0.40      0.47      1671\n",
      "           2       0.54      0.20      0.29      1174\n",
      "           3       0.45      0.47      0.46      1778\n",
      "           4       0.45      0.29      0.35      1329\n",
      "           5       0.68      0.71      0.69      1001\n",
      "\n",
      "    accuracy                           0.52     10896\n",
      "   macro avg       0.53      0.46      0.48     10896\n",
      "weighted avg       0.52      0.52      0.50     10896\n",
      "\n",
      "val loss:\t1.2889 | val acc:\t0.5629\n",
      "\n",
      "Epoch 9/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2203 | train acc:\t0.5218\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.71      0.59      3943\n",
      "           1       0.57      0.40      0.47      1671\n",
      "           2       0.55      0.22      0.31      1174\n",
      "           3       0.46      0.45      0.45      1778\n",
      "           4       0.46      0.33      0.38      1329\n",
      "           5       0.70      0.72      0.71      1001\n",
      "\n",
      "    accuracy                           0.52     10896\n",
      "   macro avg       0.54      0.47      0.49     10896\n",
      "weighted avg       0.53      0.52      0.51     10896\n",
      "\n",
      "val loss:\t1.2563 | val acc:\t0.5859\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.84      0.70      1518\n",
      "           1       0.47      0.48      0.48       468\n",
      "           2       0.60      0.25      0.36       321\n",
      "           3       0.47      0.31      0.37       397\n",
      "           4       0.58      0.12      0.20       309\n",
      "           5       0.86      0.69      0.76       252\n",
      "\n",
      "    accuracy                           0.59      3265\n",
      "   macro avg       0.60      0.45      0.48      3265\n",
      "weighted avg       0.58      0.59      0.55      3265\n",
      "\n",
      "Epoch 10/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2010 | train acc:\t0.5276\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.71      0.60      3943\n",
      "           1       0.57      0.41      0.48      1671\n",
      "           2       0.51      0.23      0.31      1174\n",
      "           3       0.47      0.47      0.47      1778\n",
      "           4       0.48      0.31      0.38      1329\n",
      "           5       0.71      0.73      0.72      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.54      0.48      0.49     10896\n",
      "weighted avg       0.53      0.53      0.51     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.2181 | val acc:\t0.5804\n",
      "\n",
      "Epoch 11/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1966 | train acc:\t0.5319\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.71      0.60      3943\n",
      "           1       0.59      0.43      0.50      1671\n",
      "           2       0.52      0.22      0.31      1174\n",
      "           3       0.47      0.46      0.47      1778\n",
      "           4       0.48      0.34      0.40      1329\n",
      "           5       0.72      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.55      0.48      0.50     10896\n",
      "weighted avg       0.53      0.53      0.52     10896\n",
      "\n",
      "val loss:\t1.2559 | val acc:\t0.5724\n",
      "\n",
      "Epoch 12/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1761 | train acc:\t0.5419\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.61      3943\n",
      "           1       0.59      0.43      0.50      1671\n",
      "           2       0.56      0.25      0.34      1174\n",
      "           3       0.48      0.47      0.47      1778\n",
      "           4       0.48      0.34      0.40      1329\n",
      "           5       0.72      0.73      0.72      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.56      0.49      0.51     10896\n",
      "weighted avg       0.54      0.54      0.53     10896\n",
      "\n",
      "val loss:\t1.2575 | val acc:\t0.5899\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.78      0.70      1518\n",
      "           1       0.49      0.44      0.47       468\n",
      "           2       0.42      0.43      0.43       321\n",
      "           3       0.52      0.31      0.39       397\n",
      "           4       0.51      0.30      0.38       309\n",
      "           5       0.86      0.70      0.77       252\n",
      "\n",
      "    accuracy                           0.59      3265\n",
      "   macro avg       0.57      0.50      0.52      3265\n",
      "weighted avg       0.58      0.59      0.58      3265\n",
      "\n",
      "Epoch 13/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1741 | train acc:\t0.5437\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.61      3943\n",
      "           1       0.60      0.43      0.50      1671\n",
      "           2       0.55      0.27      0.36      1174\n",
      "           3       0.48      0.46      0.47      1778\n",
      "           4       0.49      0.36      0.42      1329\n",
      "           5       0.71      0.73      0.72      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.56      0.50      0.51     10896\n",
      "weighted avg       0.55      0.54      0.53     10896\n",
      "\n",
      "val loss:\t1.2195 | val acc:\t0.5862\n",
      "\n",
      "Epoch 14/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1611 | train acc:\t0.5422\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.71      0.61      3943\n",
      "           1       0.59      0.44      0.50      1671\n",
      "           2       0.55      0.27      0.36      1174\n",
      "           3       0.48      0.46      0.47      1778\n",
      "           4       0.49      0.36      0.41      1329\n",
      "           5       0.72      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.56      0.50      0.51     10896\n",
      "weighted avg       0.54      0.54      0.53     10896\n",
      "\n",
      "val loss:\t1.2456 | val acc:\t0.5697\n",
      "\n",
      "Epoch 15/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1467 | train acc:\t0.5507\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.60      0.45      0.51      1671\n",
      "           2       0.56      0.29      0.38      1174\n",
      "           3       0.49      0.47      0.48      1778\n",
      "           4       0.50      0.36      0.42      1329\n",
      "           5       0.72      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.57      0.50      0.52     10896\n",
      "weighted avg       0.55      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.2328 | val acc:\t0.5969\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.80      0.71      1518\n",
      "           1       0.51      0.42      0.46       468\n",
      "           2       0.64      0.21      0.32       321\n",
      "           3       0.43      0.47      0.45       397\n",
      "           4       0.48      0.30      0.37       309\n",
      "           5       0.83      0.75      0.79       252\n",
      "\n",
      "    accuracy                           0.60      3265\n",
      "   macro avg       0.59      0.49      0.52      3265\n",
      "weighted avg       0.59      0.60      0.58      3265\n",
      "\n",
      "Epoch 16/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1353 | train acc:\t0.5567\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.61      0.44      0.51      1671\n",
      "           2       0.59      0.30      0.40      1174\n",
      "           3       0.50      0.49      0.49      1778\n",
      "           4       0.50      0.38      0.43      1329\n",
      "           5       0.72      0.75      0.73      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.51      0.53     10896\n",
      "weighted avg       0.56      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.1627 | val acc:\t0.5917\n",
      "\n",
      "Epoch 17/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1357 | train acc:\t0.5553\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.72      0.62      3943\n",
      "           1       0.61      0.43      0.51      1671\n",
      "           2       0.56      0.30      0.39      1174\n",
      "           3       0.50      0.48      0.49      1778\n",
      "           4       0.52      0.39      0.44      1329\n",
      "           5       0.72      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.57      0.51      0.53     10896\n",
      "weighted avg       0.56      0.56      0.54     10896\n",
      "\n",
      "val loss:\t1.1945 | val acc:\t0.6028\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.83      0.71      1518\n",
      "           1       0.52      0.42      0.46       468\n",
      "           2       0.63      0.26      0.37       321\n",
      "           3       0.49      0.36      0.41       397\n",
      "           4       0.49      0.42      0.45       309\n",
      "           5       0.91      0.65      0.76       252\n",
      "\n",
      "    accuracy                           0.60      3265\n",
      "   macro avg       0.61      0.49      0.53      3265\n",
      "weighted avg       0.60      0.60      0.58      3265\n",
      "\n",
      "Epoch 18/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1173 | train acc:\t0.5648\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63      3943\n",
      "           1       0.62      0.44      0.52      1671\n",
      "           2       0.57      0.31      0.40      1174\n",
      "           3       0.50      0.50      0.50      1778\n",
      "           4       0.52      0.40      0.45      1329\n",
      "           5       0.74      0.77      0.75      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.52      0.54     10896\n",
      "weighted avg       0.57      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.1378 | val acc:\t0.6006\n",
      "\n",
      "Epoch 19/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1202 | train acc:\t0.5635\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.72      0.62      3943\n",
      "           1       0.60      0.46      0.52      1671\n",
      "           2       0.57      0.32      0.41      1174\n",
      "           3       0.50      0.48      0.49      1778\n",
      "           4       0.52      0.40      0.45      1329\n",
      "           5       0.73      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.53      0.54     10896\n",
      "weighted avg       0.56      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.1859 | val acc:\t0.6000\n",
      "\n",
      "Epoch 20/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1094 | train acc:\t0.5700\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63      3943\n",
      "           1       0.61      0.45      0.52      1671\n",
      "           2       0.57      0.34      0.42      1174\n",
      "           3       0.51      0.50      0.50      1778\n",
      "           4       0.52      0.39      0.44      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.53      0.55     10896\n",
      "weighted avg       0.57      0.57      0.56     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1569 | val acc:\t0.6031\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.88      0.70      1518\n",
      "           1       0.59      0.35      0.44       468\n",
      "           2       0.64      0.22      0.33       321\n",
      "           3       0.58      0.30      0.39       397\n",
      "           4       0.53      0.30      0.39       309\n",
      "           5       0.89      0.72      0.80       252\n",
      "\n",
      "    accuracy                           0.60      3265\n",
      "   macro avg       0.63      0.46      0.51      3265\n",
      "weighted avg       0.61      0.60      0.57      3265\n",
      "\n",
      "Epoch 21/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1007 | train acc:\t0.5686\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.72      0.63      3943\n",
      "           1       0.62      0.45      0.52      1671\n",
      "           2       0.58      0.35      0.44      1174\n",
      "           3       0.50      0.49      0.49      1778\n",
      "           4       0.52      0.41      0.46      1329\n",
      "           5       0.74      0.77      0.75      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.53      0.55     10896\n",
      "weighted avg       0.57      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.1514 | val acc:\t0.5972\n",
      "\n",
      "Epoch 22/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1028 | train acc:\t0.5711\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63      3943\n",
      "           1       0.62      0.46      0.53      1671\n",
      "           2       0.62      0.36      0.46      1174\n",
      "           3       0.51      0.50      0.51      1778\n",
      "           4       0.53      0.38      0.45      1329\n",
      "           5       0.73      0.74      0.74      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.53      0.55     10896\n",
      "weighted avg       0.58      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.1907 | val acc:\t0.5899\n",
      "\n",
      "Epoch 23/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0823 | train acc:\t0.5818\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.63      3943\n",
      "           1       0.63      0.48      0.54      1671\n",
      "           2       0.59      0.35      0.44      1174\n",
      "           3       0.53      0.52      0.52      1778\n",
      "           4       0.55      0.42      0.47      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.1601 | val acc:\t0.5948\n",
      "\n",
      "Epoch 24/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0799 | train acc:\t0.5752\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63      3943\n",
      "           1       0.62      0.47      0.54      1671\n",
      "           2       0.61      0.36      0.45      1174\n",
      "           3       0.51      0.49      0.50      1778\n",
      "           4       0.54      0.42      0.48      1329\n",
      "           5       0.74      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.1388 | val acc:\t0.6064\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.88      0.71      1518\n",
      "           1       0.55      0.42      0.48       468\n",
      "           2       0.71      0.21      0.32       321\n",
      "           3       0.50      0.33      0.40       397\n",
      "           4       0.58      0.22      0.32       309\n",
      "           5       0.83      0.75      0.78       252\n",
      "\n",
      "    accuracy                           0.61      3265\n",
      "   macro avg       0.63      0.47      0.50      3265\n",
      "weighted avg       0.61      0.61      0.57      3265\n",
      "\n",
      "Epoch 25/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0796 | train acc:\t0.5810\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.63      0.48      0.54      1671\n",
      "           2       0.61      0.35      0.44      1174\n",
      "           3       0.52      0.50      0.51      1778\n",
      "           4       0.55      0.42      0.47      1329\n",
      "           5       0.75      0.76      0.76      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.59      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2061 | val acc:\t0.6021\n",
      "\n",
      "Epoch 26/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0767 | train acc:\t0.5832\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.63      0.47      0.54      1671\n",
      "           2       0.60      0.39      0.47      1174\n",
      "           3       0.52      0.49      0.51      1778\n",
      "           4       0.54      0.42      0.47      1329\n",
      "           5       0.75      0.76      0.76      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.55      0.56     10896\n",
      "weighted avg       0.59      0.58      0.58     10896\n",
      "\n",
      "val loss:\t1.1305 | val acc:\t0.6040\n",
      "\n",
      "Epoch 27/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0749 | train acc:\t0.5818\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.72      0.63      3943\n",
      "           1       0.63      0.47      0.54      1671\n",
      "           2       0.60      0.38      0.47      1174\n",
      "           3       0.52      0.51      0.52      1778\n",
      "           4       0.54      0.42      0.47      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.55      0.57     10896\n",
      "weighted avg       0.59      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.1872 | val acc:\t0.5979\n",
      "\n",
      "Epoch 28/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0677 | train acc:\t0.5848\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.63      3943\n",
      "           1       0.63      0.48      0.54      1671\n",
      "           2       0.60      0.38      0.47      1174\n",
      "           3       0.54      0.53      0.53      1778\n",
      "           4       0.55      0.41      0.47      1329\n",
      "           5       0.75      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.55      0.57     10896\n",
      "weighted avg       0.59      0.58      0.58     10896\n",
      "\n",
      "val loss:\t1.1694 | val acc:\t0.6098\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      1518\n",
      "           1       0.59      0.39      0.47       468\n",
      "           2       0.56      0.36      0.44       321\n",
      "           3       0.49      0.48      0.49       397\n",
      "           4       0.41      0.55      0.47       309\n",
      "           5       0.91      0.64      0.75       252\n",
      "\n",
      "    accuracy                           0.61      3265\n",
      "   macro avg       0.60      0.53      0.55      3265\n",
      "weighted avg       0.62      0.61      0.60      3265\n",
      "\n",
      "Epoch 29/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0636 | train acc:\t0.5835\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.64      3943\n",
      "           1       0.63      0.49      0.55      1671\n",
      "           2       0.59      0.37      0.46      1174\n",
      "           3       0.52      0.49      0.51      1778\n",
      "           4       0.55      0.44      0.49      1329\n",
      "           5       0.76      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.55      0.57     10896\n",
      "weighted avg       0.59      0.58      0.58     10896\n",
      "\n",
      "val loss:\t1.1650 | val acc:\t0.6049\n",
      "\n",
      "Epoch 30/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0578 | train acc:\t0.5868\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.64      3943\n",
      "           1       0.64      0.49      0.55      1671\n",
      "           2       0.62      0.38      0.47      1174\n",
      "           3       0.53      0.51      0.52      1778\n",
      "           4       0.54      0.43      0.48      1329\n",
      "           5       0.76      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1269 | val acc:\t0.6025\n",
      "\n",
      "Epoch 31/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0643 | train acc:\t0.5883\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.63      3943\n",
      "           1       0.64      0.48      0.55      1671\n",
      "           2       0.62      0.39      0.48      1174\n",
      "           3       0.54      0.51      0.53      1778\n",
      "           4       0.55      0.43      0.48      1329\n",
      "           5       0.76      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.1685 | val acc:\t0.5985\n",
      "\n",
      "Epoch 32/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0434 | train acc:\t0.5920\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.73      0.64      3943\n",
      "           1       0.65      0.48      0.55      1671\n",
      "           2       0.60      0.40      0.48      1174\n",
      "           3       0.55      0.54      0.54      1778\n",
      "           4       0.54      0.41      0.47      1329\n",
      "           5       0.75      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.56      0.58     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.1169 | val acc:\t0.6025\n",
      "\n",
      "Epoch 33/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0437 | train acc:\t0.5977\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.64      0.49      0.56      1671\n",
      "           2       0.63      0.41      0.50      1174\n",
      "           3       0.55      0.53      0.54      1778\n",
      "           4       0.54      0.43      0.48      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.1157 | val acc:\t0.6031\n",
      "\n",
      "Epoch 34/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0493 | train acc:\t0.5900\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.73      0.64      3943\n",
      "           1       0.63      0.49      0.55      1671\n",
      "           2       0.62      0.39      0.48      1174\n",
      "           3       0.53      0.51      0.52      1778\n",
      "           4       0.55      0.43      0.48      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.56      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.1335 | val acc:\t0.6141\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.87      0.72      1518\n",
      "           1       0.60      0.32      0.41       468\n",
      "           2       0.56      0.40      0.47       321\n",
      "           3       0.52      0.25      0.34       397\n",
      "           4       0.55      0.43      0.48       309\n",
      "           5       0.87      0.72      0.79       252\n",
      "\n",
      "    accuracy                           0.61      3265\n",
      "   macro avg       0.62      0.50      0.53      3265\n",
      "weighted avg       0.61      0.61      0.59      3265\n",
      "\n",
      "Epoch 35/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0404 | train acc:\t0.5988\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.73      0.65      3943\n",
      "           1       0.64      0.50      0.56      1671\n",
      "           2       0.62      0.42      0.50      1174\n",
      "           3       0.54      0.53      0.54      1778\n",
      "           4       0.56      0.44      0.49      1329\n",
      "           5       0.74      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.61      0.57      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.1284 | val acc:\t0.6028\n",
      "\n",
      "Epoch 36/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0340 | train acc:\t0.5987\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.65      3943\n",
      "           1       0.64      0.50      0.56      1671\n",
      "           2       0.61      0.41      0.49      1174\n",
      "           3       0.55      0.50      0.53      1778\n",
      "           4       0.57      0.45      0.50      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.1228 | val acc:\t0.5994\n",
      "\n",
      "Epoch 37/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0344 | train acc:\t0.6019\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.61      0.40      0.48      1174\n",
      "           3       0.55      0.53      0.54      1778\n",
      "           4       0.57      0.45      0.50      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.60      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.1161 | val acc:\t0.6119\n",
      "\n",
      "Epoch 38/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0223 | train acc:\t0.6064\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      3943\n",
      "           1       0.66      0.49      0.56      1671\n",
      "           2       0.62      0.41      0.50      1174\n",
      "           3       0.55      0.54      0.54      1778\n",
      "           4       0.59      0.47      0.52      1329\n",
      "           5       0.77      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1331 | val acc:\t0.6021\n",
      "\n",
      "Epoch 39/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0314 | train acc:\t0.5954\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.64      0.50      0.56      1671\n",
      "           2       0.60      0.42      0.49      1174\n",
      "           3       0.53      0.50      0.51      1778\n",
      "           4       0.57      0.45      0.50      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.61      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.1235 | val acc:\t0.6101\n",
      "\n",
      "Epoch 40/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0173 | train acc:\t0.6083\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.63      0.51      0.56      1671\n",
      "           2       0.61      0.42      0.50      1174\n",
      "           3       0.56      0.53      0.55      1778\n",
      "           4       0.58      0.46      0.52      1329\n",
      "           5       0.76      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.62      0.58      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1456 | val acc:\t0.5877\n",
      "\n",
      "Epoch 41/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0271 | train acc:\t0.6044\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.66      0.48      0.56      1671\n",
      "           2       0.65      0.44      0.53      1174\n",
      "           3       0.54      0.54      0.54      1778\n",
      "           4       0.56      0.44      0.50      1329\n",
      "           5       0.74      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.1053 | val acc:\t0.6168\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.85      0.72      1518\n",
      "           1       0.55      0.48      0.51       468\n",
      "           2       0.60      0.33      0.42       321\n",
      "           3       0.51      0.34      0.41       397\n",
      "           4       0.60      0.31      0.41       309\n",
      "           5       0.91      0.65      0.76       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.63      0.49      0.54      3265\n",
      "weighted avg       0.62      0.62      0.60      3265\n",
      "\n",
      "Epoch 42/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t1.0180 | train acc:\t0.6038\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.64      0.51      0.57      1671\n",
      "           2       0.61      0.42      0.50      1174\n",
      "           3       0.55      0.52      0.53      1778\n",
      "           4       0.58      0.46      0.51      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.1204 | val acc:\t0.6159\n",
      "\n",
      "Epoch 43/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0053 | train acc:\t0.6107\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.73      0.65      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.63      0.43      0.51      1174\n",
      "           3       0.56      0.55      0.56      1778\n",
      "           4       0.57      0.47      0.51      1329\n",
      "           5       0.76      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1269 | val acc:\t0.6147\n",
      "\n",
      "Epoch 44/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0216 | train acc:\t0.6066\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.65      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.61      0.44      0.51      1174\n",
      "           3       0.55      0.52      0.53      1778\n",
      "           4       0.56      0.45      0.50      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.62      0.58      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1048 | val acc:\t0.6187\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.82      0.72      1518\n",
      "           1       0.58      0.44      0.50       468\n",
      "           2       0.55      0.33      0.41       321\n",
      "           3       0.48      0.41      0.44       397\n",
      "           4       0.61      0.33      0.43       309\n",
      "           5       0.77      0.79      0.78       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.61      0.52      0.55      3265\n",
      "weighted avg       0.61      0.62      0.60      3265\n",
      "\n",
      "Epoch 45/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0081 | train acc:\t0.6134\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.65      0.53      0.58      1671\n",
      "           2       0.63      0.43      0.51      1174\n",
      "           3       0.58      0.54      0.56      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.1260 | val acc:\t0.6083\n",
      "\n",
      "Epoch 46/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0023 | train acc:\t0.6095\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.65      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.61      0.42      0.50      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.57      0.47      0.51      1329\n",
      "           5       0.76      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.0793 | val acc:\t0.6178\n",
      "\n",
      "Epoch 47/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0014 | train acc:\t0.6113\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.65      3943\n",
      "           1       0.67      0.50      0.57      1671\n",
      "           2       0.61      0.46      0.52      1174\n",
      "           3       0.57      0.56      0.57      1778\n",
      "           4       0.55      0.44      0.49      1329\n",
      "           5       0.78      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.1030 | val acc:\t0.6098\n",
      "\n",
      "Epoch 48/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9984 | train acc:\t0.6110\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.65      0.52      0.58      1671\n",
      "           2       0.63      0.44      0.52      1174\n",
      "           3       0.55      0.53      0.54      1778\n",
      "           4       0.58      0.46      0.51      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.1217 | val acc:\t0.6187\n",
      "\n",
      "Epoch 49/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9970 | train acc:\t0.6153\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.63      0.45      0.53      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.57      0.47      0.51      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.63      0.59      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1057 | val acc:\t0.6184\n",
      "\n",
      "Epoch 50/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9985 | train acc:\t0.6185\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.65      0.46      0.54      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.76      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.0840 | val acc:\t0.6162\n",
      "\n",
      "Epoch 51/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0051 | train acc:\t0.6110\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.67      0.51      0.57      1671\n",
      "           2       0.61      0.43      0.50      1174\n",
      "           3       0.57      0.56      0.56      1778\n",
      "           4       0.59      0.46      0.52      1329\n",
      "           5       0.76      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.0909 | val acc:\t0.6260\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.88      0.73      1518\n",
      "           1       0.67      0.35      0.46       468\n",
      "           2       0.70      0.33      0.45       321\n",
      "           3       0.51      0.38      0.44       397\n",
      "           4       0.54      0.34      0.42       309\n",
      "           5       0.84      0.72      0.78       252\n",
      "\n",
      "    accuracy                           0.63      3265\n",
      "   macro avg       0.65      0.50      0.54      3265\n",
      "weighted avg       0.63      0.63      0.60      3265\n",
      "\n",
      "Epoch 52/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9861 | train acc:\t0.6136\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.62      0.47      0.53      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.58      0.46      0.52      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1391 | val acc:\t0.6067\n",
      "\n",
      "Epoch 53/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9926 | train acc:\t0.6207\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.52      0.58      1671\n",
      "           2       0.64      0.47      0.54      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.58      0.48      0.53      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1678 | val acc:\t0.6000\n",
      "\n",
      "Epoch 54/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9949 | train acc:\t0.6235\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.52      0.59      1671\n",
      "           2       0.66      0.48      0.55      1174\n",
      "           3       0.56      0.53      0.55      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.65      0.60      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.0889 | val acc:\t0.6086\n",
      "\n",
      "Epoch 55/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9817 | train acc:\t0.6244\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.52      0.59      1671\n",
      "           2       0.65      0.46      0.54      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.77      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1158 | val acc:\t0.5933\n",
      "\n",
      "Epoch 56/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9809 | train acc:\t0.6289\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.54      0.60      1671\n",
      "           2       0.66      0.48      0.55      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1064 | val acc:\t0.6196\n",
      "\n",
      "Epoch 57/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9886 | train acc:\t0.6189\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.62      0.46      0.53      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.63      0.59      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1649 | val acc:\t0.6116\n",
      "\n",
      "Epoch 58/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9833 | train acc:\t0.6175\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.65      0.48      0.55      1174\n",
      "           3       0.58      0.53      0.55      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.0728 | val acc:\t0.6266\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.85      0.73      1518\n",
      "           1       0.71      0.32      0.45       468\n",
      "           2       0.61      0.40      0.48       321\n",
      "           3       0.46      0.50      0.48       397\n",
      "           4       0.54      0.38      0.45       309\n",
      "           5       0.91      0.66      0.77       252\n",
      "\n",
      "    accuracy                           0.63      3265\n",
      "   macro avg       0.64      0.52      0.56      3265\n",
      "weighted avg       0.64      0.63      0.61      3265\n",
      "\n",
      "Epoch 59/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9847 | train acc:\t0.6160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.64      0.46      0.54      1174\n",
      "           3       0.58      0.54      0.56      1778\n",
      "           4       0.57      0.48      0.52      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.63      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.0851 | val acc:\t0.6214\n",
      "\n",
      "Epoch 60/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9728 | train acc:\t0.6255\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.67      0.54      0.60      1671\n",
      "           2       0.65      0.47      0.55      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.59      0.47      0.52      1329\n",
      "           5       0.77      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.64      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.0909 | val acc:\t0.6288\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.83      0.73      1518\n",
      "           1       0.58      0.44      0.50       468\n",
      "           2       0.55      0.41      0.47       321\n",
      "           3       0.49      0.41      0.45       397\n",
      "           4       0.58      0.38      0.46       309\n",
      "           5       0.87      0.71      0.78       252\n",
      "\n",
      "    accuracy                           0.63      3265\n",
      "   macro avg       0.62      0.53      0.56      3265\n",
      "weighted avg       0.62      0.63      0.61      3265\n",
      "\n",
      "Epoch 61/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9745 | train acc:\t0.6269\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.52      0.59      1671\n",
      "           2       0.62      0.46      0.52      1174\n",
      "           3       0.60      0.57      0.58      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.0673 | val acc:\t0.6107\n",
      "\n",
      "Epoch 62/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9777 | train acc:\t0.6200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.73      0.66      3943\n",
      "           1       0.69      0.54      0.60      1671\n",
      "           2       0.64      0.48      0.55      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.58      0.49      0.53      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1121 | val acc:\t0.6095\n",
      "\n",
      "Epoch 63/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9688 | train acc:\t0.6293\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.65      0.47      0.54      1174\n",
      "           3       0.61      0.56      0.58      1778\n",
      "           4       0.57      0.49      0.53      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0272 | val acc:\t0.6202\n",
      "\n",
      "Epoch 64/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9715 | train acc:\t0.6242\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.53      0.59      1671\n",
      "           2       0.65      0.47      0.55      1174\n",
      "           3       0.57      0.56      0.56      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.0594 | val acc:\t0.6135\n",
      "\n",
      "Epoch 65/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9670 | train acc:\t0.6300\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      3943\n",
      "           1       0.66      0.53      0.58      1671\n",
      "           2       0.65      0.49      0.56      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.59      0.49      0.54      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.0620 | val acc:\t0.6239\n",
      "\n",
      "Epoch 66/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9696 | train acc:\t0.6333\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.68      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.67      0.48      0.56      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.57      0.48      0.52      1329\n",
      "           5       0.77      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.0638 | val acc:\t0.6165\n",
      "\n",
      "Epoch 67/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9638 | train acc:\t0.6289\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.65      0.48      0.55      1174\n",
      "           3       0.58      0.55      0.56      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1011 | val acc:\t0.6254\n",
      "\n",
      "Epoch 68/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9733 | train acc:\t0.6255\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.64      0.48      0.55      1174\n",
      "           3       0.57      0.54      0.55      1778\n",
      "           4       0.59      0.49      0.54      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.0651 | val acc:\t0.6196\n",
      "\n",
      "Epoch 69/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9677 | train acc:\t0.6291\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.52      0.59      1671\n",
      "           2       0.65      0.48      0.55      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.0494 | val acc:\t0.6153\n",
      "\n",
      "Epoch 70/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9617 | train acc:\t0.6296\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.52      0.59      1671\n",
      "           2       0.64      0.48      0.55      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.76      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.0958 | val acc:\t0.6233\n",
      "\n",
      "Epoch 71/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9673 | train acc:\t0.6312\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      3943\n",
      "           1       0.66      0.53      0.59      1671\n",
      "           2       0.65      0.48      0.55      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.77      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.0612 | val acc:\t0.6150\n",
      "\n",
      "Epoch 72/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9543 | train acc:\t0.6281\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.66      0.48      0.55      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.58      0.51      0.54      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1099 | val acc:\t0.6144\n",
      "\n",
      "Epoch 73/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9588 | train acc:\t0.6255\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.65      0.47      0.55      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.58      0.49      0.53      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.64      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.0695 | val acc:\t0.6282\n",
      "\n",
      "Epoch 74/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9646 | train acc:\t0.6248\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.67      0.55      0.60      1671\n",
      "           2       0.65      0.47      0.55      1174\n",
      "           3       0.58      0.54      0.56      1778\n",
      "           4       0.57      0.49      0.53      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.60      0.62     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.0925 | val acc:\t0.6276\n",
      "\n",
      "Epoch 75/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9646 | train acc:\t0.6308\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.68      0.49      0.57      1174\n",
      "           3       0.59      0.58      0.58      1778\n",
      "           4       0.59      0.49      0.54      1329\n",
      "           5       0.79      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1104 | val acc:\t0.6190\n",
      "\n",
      "Epoch 76/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9592 | train acc:\t0.6309\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      3943\n",
      "           1       0.67      0.55      0.60      1671\n",
      "           2       0.63      0.48      0.54      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.78      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.0780 | val acc:\t0.6184\n",
      "\n",
      "Epoch 77/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9578 | train acc:\t0.6315\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.65      0.50      0.56      1174\n",
      "           3       0.59      0.56      0.58      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.0840 | val acc:\t0.6257\n",
      "\n",
      "Epoch 78/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9484 | train acc:\t0.6347\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.65      0.48      0.55      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.79      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.0564 | val acc:\t0.6159\n",
      "\n",
      "Epoch 79/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9511 | train acc:\t0.6311\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      3943\n",
      "           1       0.67      0.54      0.60      1671\n",
      "           2       0.63      0.47      0.54      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.59      0.51      0.55      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.0895 | val acc:\t0.6242\n",
      "\n",
      "Epoch 80/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9534 | train acc:\t0.6339\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.69      0.54      0.60      1671\n",
      "           2       0.68      0.49      0.57      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.57      0.50      0.53      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.0999 | val acc:\t0.6132\n",
      "\n",
      "Epoch 81/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9505 | train acc:\t0.6344\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      3943\n",
      "           1       0.69      0.54      0.60      1671\n",
      "           2       0.66      0.51      0.58      1174\n",
      "           3       0.59      0.55      0.57      1778\n",
      "           4       0.59      0.49      0.54      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.0642 | val acc:\t0.6202\n",
      "\n",
      "Epoch 82/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9561 | train acc:\t0.6348\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.67      0.48      0.55      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.0638 | val acc:\t0.6322\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.83      0.73      1518\n",
      "           1       0.68      0.32      0.44       468\n",
      "           2       0.59      0.40      0.48       321\n",
      "           3       0.56      0.47      0.51       397\n",
      "           4       0.47      0.51      0.49       309\n",
      "           5       0.90      0.68      0.78       252\n",
      "\n",
      "    accuracy                           0.63      3265\n",
      "   macro avg       0.64      0.54      0.57      3265\n",
      "weighted avg       0.64      0.63      0.62      3265\n",
      "\n",
      "Epoch 83/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9409 | train acc:\t0.6363\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.54      0.60      1671\n",
      "           2       0.66      0.48      0.55      1174\n",
      "           3       0.59      0.58      0.59      1778\n",
      "           4       0.62      0.52      0.57      1329\n",
      "           5       0.78      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.0756 | val acc:\t0.6199\n",
      "\n",
      "Epoch 84/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9482 | train acc:\t0.6311\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.65      0.49      0.56      1174\n",
      "           3       0.59      0.56      0.58      1778\n",
      "           4       0.58      0.50      0.54      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.0570 | val acc:\t0.6239\n",
      "\n",
      "Epoch 85/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9481 | train acc:\t0.6367\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.65      0.49      0.56      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.0641 | val acc:\t0.6417\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.86      0.74      1518\n",
      "           1       0.70      0.33      0.45       468\n",
      "           2       0.66      0.36      0.46       321\n",
      "           3       0.49      0.48      0.49       397\n",
      "           4       0.56      0.44      0.49       309\n",
      "           5       0.88      0.77      0.82       252\n",
      "\n",
      "    accuracy                           0.64      3265\n",
      "   macro avg       0.66      0.54      0.57      3265\n",
      "weighted avg       0.65      0.64      0.62      3265\n",
      "\n",
      "Epoch 86/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9533 | train acc:\t0.6325\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.68      0.53      0.60      1671\n",
      "           2       0.65      0.51      0.57      1174\n",
      "           3       0.59      0.58      0.58      1778\n",
      "           4       0.58      0.49      0.53      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0790 | val acc:\t0.6233\n",
      "\n",
      "Epoch 87/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9427 | train acc:\t0.6394\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.74      0.67      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.67      0.48      0.56      1174\n",
      "           3       0.60      0.57      0.59      1778\n",
      "           4       0.59      0.52      0.56      1329\n",
      "           5       0.78      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.1044 | val acc:\t0.6086\n",
      "\n",
      "Epoch 88/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9561 | train acc:\t0.6306\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.68      0.53      0.60      1671\n",
      "           2       0.64      0.47      0.55      1174\n",
      "           3       0.59      0.58      0.58      1778\n",
      "           4       0.59      0.49      0.53      1329\n",
      "           5       0.79      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.0431 | val acc:\t0.6288\n",
      "\n",
      "Epoch 89/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9399 | train acc:\t0.6368\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.69      0.54      0.60      1671\n",
      "           2       0.69      0.49      0.57      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.59      0.50      0.55      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.0617 | val acc:\t0.6248\n",
      "\n",
      "Epoch 90/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9413 | train acc:\t0.6398\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.56      0.61      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.60      0.57      0.59      1778\n",
      "           4       0.61      0.51      0.56      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0400 | val acc:\t0.6328\n",
      "\n",
      "Epoch 91/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9577 | train acc:\t0.6273\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.69      0.53      0.60      1671\n",
      "           2       0.65      0.50      0.56      1174\n",
      "           3       0.57      0.57      0.57      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.0649 | val acc:\t0.6285\n",
      "\n",
      "Epoch 92/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9472 | train acc:\t0.6366\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.68      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.67      0.50      0.58      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.60      0.50      0.54      1329\n",
      "           5       0.77      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.0639 | val acc:\t0.6162\n",
      "\n",
      "Epoch 93/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9517 | train acc:\t0.6356\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.74      0.67      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.64      0.48      0.55      1174\n",
      "           3       0.59      0.58      0.59      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.78      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.1231 | val acc:\t0.6101\n",
      "\n",
      "Epoch 94/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9520 | train acc:\t0.6346\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.61      0.51      0.55      1329\n",
      "           5       0.77      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.0823 | val acc:\t0.6242\n",
      "\n",
      "Epoch 95/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9476 | train acc:\t0.6378\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.54      0.61      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.59      0.58      0.59      1778\n",
      "           4       0.59      0.50      0.54      1329\n",
      "           5       0.78      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.0398 | val acc:\t0.6227\n",
      "\n",
      "Epoch 96/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9366 | train acc:\t0.6381\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.67      0.55      0.60      1671\n",
      "           2       0.68      0.53      0.60      1174\n",
      "           3       0.58      0.55      0.56      1778\n",
      "           4       0.60      0.50      0.54      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.0391 | val acc:\t0.6193\n",
      "\n",
      "Epoch 97/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9453 | train acc:\t0.6373\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.66      0.51      0.58      1174\n",
      "           3       0.59      0.56      0.58      1778\n",
      "           4       0.59      0.50      0.54      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.0856 | val acc:\t0.6245\n",
      "\n",
      "Epoch 98/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9433 | train acc:\t0.6374\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.65      0.48      0.56      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0831 | val acc:\t0.6383\n",
      "\n",
      "Epoch 99/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9375 | train acc:\t0.6431\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.68      3943\n",
      "           1       0.70      0.56      0.62      1671\n",
      "           2       0.68      0.49      0.57      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.79      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0478 | val acc:\t0.6352\n",
      "\n",
      "Epoch 100/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9510 | train acc:\t0.6343\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.67      0.54      0.60      1671\n",
      "           2       0.65      0.50      0.56      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.77      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.0752 | val acc:\t0.6236\n",
      "\n",
      "Epoch 101/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9404 | train acc:\t0.6408\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.59      0.51      0.55      1329\n",
      "           5       0.77      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0397 | val acc:\t0.6251\n",
      "\n",
      "Epoch 102/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9428 | train acc:\t0.6374\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.65      0.50      0.56      1174\n",
      "           3       0.60      0.56      0.58      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.0323 | val acc:\t0.6119\n",
      "\n",
      "Epoch 103/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9377 | train acc:\t0.6433\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.68      0.56      0.61      1671\n",
      "           2       0.69      0.50      0.58      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.61      0.51      0.56      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0578 | val acc:\t0.6303\n",
      "\n",
      "Epoch 104/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9328 | train acc:\t0.6415\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.66      0.52      0.59      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.61      0.51      0.55      1329\n",
      "           5       0.79      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0415 | val acc:\t0.6178\n",
      "\n",
      "Epoch 105/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9424 | train acc:\t0.6357\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.65      0.54      0.59      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.60      0.57      0.58      1778\n",
      "           4       0.59      0.51      0.55      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.0421 | val acc:\t0.6202\n",
      "\n",
      "Epoch 106/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9400 | train acc:\t0.6364\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.68      0.57      0.62      1671\n",
      "           2       0.67      0.50      0.58      1174\n",
      "           3       0.59      0.56      0.58      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.0752 | val acc:\t0.6245\n",
      "\n",
      "Epoch 107/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9317 | train acc:\t0.6411\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.70      0.55      0.62      1671\n",
      "           2       0.68      0.51      0.59      1174\n",
      "           3       0.60      0.57      0.58      1778\n",
      "           4       0.58      0.50      0.54      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0789 | val acc:\t0.6086\n",
      "\n",
      "Epoch 108/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9263 | train acc:\t0.6423\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67      3943\n",
      "           1       0.71      0.58      0.63      1671\n",
      "           2       0.66      0.49      0.57      1174\n",
      "           3       0.60      0.57      0.58      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.77      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0790 | val acc:\t0.6300\n",
      "\n",
      "Epoch 109/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9337 | train acc:\t0.6422\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.67      0.56      0.61      1671\n",
      "           2       0.68      0.51      0.58      1174\n",
      "           3       0.60      0.57      0.59      1778\n",
      "           4       0.60      0.52      0.56      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0486 | val acc:\t0.6221\n",
      "\n",
      "Epoch 110/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9322 | train acc:\t0.6417\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.54      0.61      1671\n",
      "           2       0.65      0.49      0.56      1174\n",
      "           3       0.59      0.58      0.59      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0526 | val acc:\t0.6221\n",
      "\n",
      "Epoch 111/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9324 | train acc:\t0.6409\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.66      0.49      0.57      1174\n",
      "           3       0.60      0.57      0.58      1778\n",
      "           4       0.60      0.52      0.56      1329\n",
      "           5       0.77      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0736 | val acc:\t0.6227\n",
      "\n",
      "Epoch 112/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9274 | train acc:\t0.6480\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.56      0.62      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.78      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0847 | val acc:\t0.6129\n",
      "\n",
      "Epoch 113/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9362 | train acc:\t0.6381\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.57      0.62      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.57      0.50      0.53      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.0802 | val acc:\t0.6113\n",
      "\n",
      "Epoch 114/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9251 | train acc:\t0.6449\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.68      0.56      0.62      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.61      0.58      0.60      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.78      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0477 | val acc:\t0.6319\n",
      "\n",
      "Epoch 115/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9260 | train acc:\t0.6482\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.56      0.62      1671\n",
      "           2       0.69      0.52      0.59      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0450 | val acc:\t0.6251\n",
      "\n",
      "Epoch 116/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9307 | train acc:\t0.6454\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.67      0.52      0.59      1174\n",
      "           3       0.61      0.57      0.59      1778\n",
      "           4       0.60      0.51      0.56      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0667 | val acc:\t0.6331\n",
      "\n",
      "Epoch 117/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9243 | train acc:\t0.6451\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0280 | val acc:\t0.6352\n",
      "\n",
      "Epoch 118/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9209 | train acc:\t0.6456\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.57      0.63      1671\n",
      "           2       0.66      0.51      0.57      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0549 | val acc:\t0.6211\n",
      "\n",
      "Epoch 119/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9376 | train acc:\t0.6411\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.68      0.56      0.62      1671\n",
      "           2       0.67      0.50      0.57      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.1030 | val acc:\t0.6297\n",
      "\n",
      "Epoch 120/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9221 | train acc:\t0.6453\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.69      0.55      0.61      1671\n",
      "           2       0.67      0.53      0.59      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.62      0.50      0.56      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0458 | val acc:\t0.6236\n",
      "\n",
      "Epoch 121/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9226 | train acc:\t0.6461\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.68      0.57      0.62      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.61      0.58      0.60      1778\n",
      "           4       0.59      0.51      0.55      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0365 | val acc:\t0.6266\n",
      "\n",
      "Epoch 122/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9235 | train acc:\t0.6456\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      3943\n",
      "           1       0.68      0.56      0.62      1671\n",
      "           2       0.66      0.51      0.58      1174\n",
      "           3       0.59      0.59      0.59      1778\n",
      "           4       0.61      0.51      0.55      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0180 | val acc:\t0.6205\n",
      "\n",
      "Epoch 123/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9219 | train acc:\t0.6477\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.66      0.51      0.58      1174\n",
      "           3       0.61      0.58      0.59      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0470 | val acc:\t0.6297\n",
      "\n",
      "Epoch 124/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9197 | train acc:\t0.6490\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.59      0.51      0.55      1329\n",
      "           5       0.78      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0238 | val acc:\t0.6368\n",
      "\n",
      "Epoch 125/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9248 | train acc:\t0.6453\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.68      0.56      0.61      1671\n",
      "           2       0.68      0.49      0.57      1174\n",
      "           3       0.60      0.59      0.59      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0452 | val acc:\t0.6377\n",
      "\n",
      "Epoch 126/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9385 | train acc:\t0.6405\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.67      0.56      0.61      1671\n",
      "           2       0.65      0.50      0.56      1174\n",
      "           3       0.61      0.58      0.59      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0571 | val acc:\t0.6325\n",
      "\n",
      "Epoch 127/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9210 | train acc:\t0.6440\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.70      0.56      0.62      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.59      0.49      0.54      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0594 | val acc:\t0.6123\n",
      "\n",
      "Epoch 128/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9227 | train acc:\t0.6464\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.61      0.53      0.57      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0465 | val acc:\t0.6371\n",
      "\n",
      "Epoch 129/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9196 | train acc:\t0.6459\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.66      0.51      0.57      1174\n",
      "           3       0.60      0.59      0.59      1778\n",
      "           4       0.59      0.52      0.55      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.1147 | val acc:\t0.6089\n",
      "\n",
      "Epoch 130/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9287 | train acc:\t0.6438\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.68      0.56      0.61      1671\n",
      "           2       0.68      0.51      0.58      1174\n",
      "           3       0.59      0.58      0.59      1778\n",
      "           4       0.60      0.50      0.54      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0693 | val acc:\t0.6147\n",
      "\n",
      "Epoch 131/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9218 | train acc:\t0.6445\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.57      0.62      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.60      0.53      0.56      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0398 | val acc:\t0.6319\n",
      "\n",
      "Epoch 132/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9246 | train acc:\t0.6472\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.69      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.62      0.52      0.56      1329\n",
      "           5       0.79      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0638 | val acc:\t0.6227\n",
      "\n",
      "Epoch 133/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9263 | train acc:\t0.6432\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.68      0.57      0.62      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.61      0.50      0.55      1329\n",
      "           5       0.78      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0550 | val acc:\t0.6312\n",
      "\n",
      "Epoch 134/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9210 | train acc:\t0.6493\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.69      0.53      0.60      1174\n",
      "           3       0.61      0.58      0.59      1778\n",
      "           4       0.60      0.52      0.56      1329\n",
      "           5       0.77      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0498 | val acc:\t0.6328\n",
      "\n",
      "Epoch 135/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9154 | train acc:\t0.6463\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.74      0.68      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.64      0.51      0.57      1174\n",
      "           3       0.60      0.59      0.59      1778\n",
      "           4       0.61      0.53      0.57      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0505 | val acc:\t0.6319\n",
      "\n",
      "Epoch 136/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9149 | train acc:\t0.6485\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.69      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.65      0.51      0.57      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.59      0.51      0.55      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0385 | val acc:\t0.6178\n",
      "\n",
      "Epoch 137/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9310 | train acc:\t0.6425\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.58      0.57      0.58      1778\n",
      "           4       0.62      0.52      0.56      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0357 | val acc:\t0.6312\n",
      "\n",
      "Epoch 138/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9086 | train acc:\t0.6526\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.66      0.53      0.59      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.62      0.52      0.56      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0939 | val acc:\t0.6294\n",
      "\n",
      "Epoch 139/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9188 | train acc:\t0.6500\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.69      3943\n",
      "           1       0.69      0.57      0.62      1671\n",
      "           2       0.67      0.53      0.60      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.60      0.53      0.56      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0433 | val acc:\t0.6447\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.86      0.74      1518\n",
      "           1       0.67      0.42      0.52       468\n",
      "           2       0.68      0.34      0.45       321\n",
      "           3       0.51      0.47      0.49       397\n",
      "           4       0.58      0.39      0.47       309\n",
      "           5       0.88      0.71      0.79       252\n",
      "\n",
      "    accuracy                           0.64      3265\n",
      "   macro avg       0.66      0.53      0.58      3265\n",
      "weighted avg       0.65      0.64      0.63      3265\n",
      "\n",
      "Epoch 140/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9271 | train acc:\t0.6464\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.65      0.50      0.56      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0505 | val acc:\t0.6282\n",
      "\n",
      "Epoch 141/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9105 | train acc:\t0.6574\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.56      0.63      1671\n",
      "           2       0.69      0.53      0.60      1174\n",
      "           3       0.61      0.61      0.61      1778\n",
      "           4       0.63      0.53      0.57      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.0284 | val acc:\t0.6361\n",
      "\n",
      "Epoch 142/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9153 | train acc:\t0.6525\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.68      0.51      0.58      1174\n",
      "           3       0.60      0.59      0.60      1778\n",
      "           4       0.60      0.53      0.56      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0358 | val acc:\t0.6294\n",
      "\n",
      "Epoch 143/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9101 | train acc:\t0.6539\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.69      0.52      0.59      1174\n",
      "           3       0.59      0.58      0.58      1778\n",
      "           4       0.61      0.54      0.58      1329\n",
      "           5       0.80      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0590 | val acc:\t0.6257\n",
      "\n",
      "Epoch 144/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9139 | train acc:\t0.6441\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.74      0.68      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.59      0.58      0.59      1778\n",
      "           4       0.61      0.53      0.57      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0535 | val acc:\t0.6168\n",
      "\n",
      "Epoch 145/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9118 | train acc:\t0.6527\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.67      0.53      0.59      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.78      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0582 | val acc:\t0.6325\n",
      "\n",
      "Epoch 146/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.9091 | train acc:\t0.6540\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.69      0.53      0.60      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.61      0.53      0.57      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0375 | val acc:\t0.6371\n",
      "\n",
      "Epoch 147/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9168 | train acc:\t0.6481\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.65      0.49      0.56      1174\n",
      "           3       0.60      0.57      0.58      1778\n",
      "           4       0.61      0.51      0.56      1329\n",
      "           5       0.80      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0428 | val acc:\t0.6233\n",
      "\n",
      "Epoch 148/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9138 | train acc:\t0.6546\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.68      0.57      0.62      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0612 | val acc:\t0.6245\n",
      "\n",
      "Epoch 149/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9128 | train acc:\t0.6530\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      3943\n",
      "           1       0.71      0.58      0.64      1671\n",
      "           2       0.68      0.51      0.58      1174\n",
      "           3       0.62      0.59      0.60      1778\n",
      "           4       0.61      0.54      0.57      1329\n",
      "           5       0.78      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0530 | val acc:\t0.6270\n",
      "\n",
      "Epoch 150/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9208 | train acc:\t0.6485\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.69      3943\n",
      "           1       0.69      0.55      0.62      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.59      0.58      0.59      1778\n",
      "           4       0.59      0.51      0.54      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0632 | val acc:\t0.6294\n",
      "\n",
      "Epoch 151/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9139 | train acc:\t0.6496\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.71      0.56      0.63      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.61      0.53      0.56      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0349 | val acc:\t0.6358\n",
      "\n",
      "Epoch 152/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9172 | train acc:\t0.6527\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.69      0.52      0.59      1174\n",
      "           3       0.60      0.59      0.59      1778\n",
      "           4       0.63      0.53      0.57      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0492 | val acc:\t0.6398\n",
      "\n",
      "Epoch 153/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9193 | train acc:\t0.6523\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.56      0.62      1671\n",
      "           2       0.66      0.52      0.58      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0594 | val acc:\t0.6254\n",
      "\n",
      "Epoch 154/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9184 | train acc:\t0.6541\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.62      0.52      0.56      1329\n",
      "           5       0.78      0.83      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0142 | val acc:\t0.6245\n",
      "\n",
      "Epoch 155/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9071 | train acc:\t0.6525\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.69      0.52      0.59      1174\n",
      "           3       0.60      0.59      0.59      1778\n",
      "           4       0.62      0.53      0.58      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0197 | val acc:\t0.6309\n",
      "\n",
      "Epoch 156/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9109 | train acc:\t0.6525\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.56      0.62      1671\n",
      "           2       0.67      0.54      0.60      1174\n",
      "           3       0.61      0.58      0.60      1778\n",
      "           4       0.62      0.52      0.57      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0309 | val acc:\t0.6254\n",
      "\n",
      "Epoch 157/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9102 | train acc:\t0.6526\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.67      0.52      0.59      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.78      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0333 | val acc:\t0.6266\n",
      "\n",
      "Epoch 158/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.9118 | train acc:\t0.6510\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.57      0.62      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.61      0.54      0.57      1329\n",
      "           5       0.80      0.79      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0232 | val acc:\t0.6303\n",
      "\n",
      "Epoch 159/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9248 | train acc:\t0.6445\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.60      0.57      0.58      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.0595 | val acc:\t0.6340\n",
      "\n",
      "Epoch 160/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9122 | train acc:\t0.6505\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.55      0.62      1671\n",
      "           2       0.68      0.53      0.60      1174\n",
      "           3       0.60      0.59      0.59      1778\n",
      "           4       0.60      0.52      0.56      1329\n",
      "           5       0.80      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0268 | val acc:\t0.6303\n",
      "\n",
      "Epoch 161/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9292 | train acc:\t0.6471\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.68      0.56      0.62      1671\n",
      "           2       0.68      0.50      0.58      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.60      0.52      0.56      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0379 | val acc:\t0.6395\n",
      "\n",
      "Epoch 162/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9069 | train acc:\t0.6511\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.69      3943\n",
      "           1       0.68      0.58      0.63      1671\n",
      "           2       0.67      0.52      0.58      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0490 | val acc:\t0.6279\n",
      "\n",
      "Epoch 163/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9081 | train acc:\t0.6582\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.71      0.57      0.63      1671\n",
      "           2       0.70      0.53      0.60      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.62      0.52      0.57      1329\n",
      "           5       0.79      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.0140 | val acc:\t0.6410\n",
      "\n",
      "Epoch 164/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9073 | train acc:\t0.6496\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.61      0.60      0.61      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0319 | val acc:\t0.6266\n",
      "\n",
      "Epoch 165/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9187 | train acc:\t0.6495\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.70      0.56      0.62      1671\n",
      "           2       0.70      0.52      0.59      1174\n",
      "           3       0.60      0.59      0.59      1778\n",
      "           4       0.62      0.52      0.56      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0152 | val acc:\t0.6435\n",
      "\n",
      "Epoch 166/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9135 | train acc:\t0.6521\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.55      0.62      1671\n",
      "           2       0.70      0.55      0.61      1174\n",
      "           3       0.59      0.59      0.59      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.80      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0453 | val acc:\t0.6352\n",
      "\n",
      "Epoch 167/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9141 | train acc:\t0.6534\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0280 | val acc:\t0.6386\n",
      "\n",
      "Epoch 168/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9121 | train acc:\t0.6519\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.67      0.52      0.59      1174\n",
      "           3       0.60      0.61      0.61      1778\n",
      "           4       0.60      0.52      0.56      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0357 | val acc:\t0.6423\n",
      "\n",
      "Epoch 169/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9060 | train acc:\t0.6574\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.71      0.56      0.62      1671\n",
      "           2       0.67      0.52      0.59      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.64      0.54      0.59      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.0233 | val acc:\t0.6266\n",
      "\n",
      "Epoch 170/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.9087 | train acc:\t0.6495\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.57      0.62      1671\n",
      "           2       0.67      0.52      0.59      1174\n",
      "           3       0.60      0.57      0.58      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0693 | val acc:\t0.6196\n",
      "\n",
      "Epoch 171/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9147 | train acc:\t0.6512\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.77      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.68      0.51      0.58      1174\n",
      "           3       0.61      0.58      0.60      1778\n",
      "           4       0.61      0.51      0.56      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1017 | val acc:\t0.6285\n",
      "\n",
      "Epoch 172/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8985 | train acc:\t0.6564\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.72      0.58      0.64      1671\n",
      "           2       0.67      0.52      0.59      1174\n",
      "           3       0.61      0.60      0.61      1778\n",
      "           4       0.60      0.53      0.56      1329\n",
      "           5       0.78      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.0449 | val acc:\t0.6312\n",
      "\n",
      "Epoch 173/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9200 | train acc:\t0.6488\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      3943\n",
      "           1       0.69      0.57      0.63      1671\n",
      "           2       0.66      0.51      0.57      1174\n",
      "           3       0.60      0.60      0.60      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0408 | val acc:\t0.6346\n",
      "\n",
      "Epoch 174/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9135 | train acc:\t0.6520\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.69      0.51      0.59      1174\n",
      "           3       0.60      0.59      0.59      1778\n",
      "           4       0.63      0.53      0.58      1329\n",
      "           5       0.78      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0358 | val acc:\t0.6368\n",
      "\n",
      "Epoch 175/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9177 | train acc:\t0.6456\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.68      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.66      0.51      0.58      1174\n",
      "           3       0.61      0.58      0.60      1778\n",
      "           4       0.59      0.50      0.54      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0365 | val acc:\t0.6361\n",
      "\n",
      "Epoch 176/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9113 | train acc:\t0.6482\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.67      0.57      0.62      1671\n",
      "           2       0.66      0.52      0.58      1174\n",
      "           3       0.61      0.58      0.59      1778\n",
      "           4       0.61      0.51      0.56      1329\n",
      "           5       0.80      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0512 | val acc:\t0.6306\n",
      "\n",
      "Epoch 177/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9158 | train acc:\t0.6463\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.68      0.57      0.62      1671\n",
      "           2       0.67      0.50      0.57      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.63      0.52      0.57      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0193 | val acc:\t0.6227\n",
      "\n",
      "Epoch 178/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9017 | train acc:\t0.6561\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.72      0.60      0.65      1671\n",
      "           2       0.67      0.52      0.59      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.63      0.51      0.57      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.0106 | val acc:\t0.6340\n",
      "\n",
      "Epoch 179/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9039 | train acc:\t0.6544\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.67      0.52      0.58      1174\n",
      "           3       0.61      0.61      0.61      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0464 | val acc:\t0.6389\n",
      "\n",
      "Epoch 180/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9107 | train acc:\t0.6564\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.57      0.62      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.61      0.60      0.61      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.0422 | val acc:\t0.6309\n",
      "\n",
      "Epoch 181/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9150 | train acc:\t0.6468\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.57      0.63      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.60      0.60      0.60      1778\n",
      "           4       0.61      0.51      0.56      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0678 | val acc:\t0.6429\n",
      "\n",
      "Epoch 182/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.9063 | train acc:\t0.6574\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.60      0.61      0.60      1778\n",
      "           4       0.63      0.52      0.57      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.0681 | val acc:\t0.6199\n",
      "\n",
      "Epoch 183/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9089 | train acc:\t0.6523\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69      3943\n",
      "           1       0.68      0.57      0.62      1671\n",
      "           2       0.69      0.54      0.60      1174\n",
      "           3       0.59      0.60      0.59      1778\n",
      "           4       0.63      0.52      0.57      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0737 | val acc:\t0.6352\n",
      "\n",
      "Epoch 184/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9019 | train acc:\t0.6528\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69      3943\n",
      "           1       0.69      0.57      0.62      1671\n",
      "           2       0.66      0.52      0.58      1174\n",
      "           3       0.61      0.60      0.61      1778\n",
      "           4       0.61      0.54      0.57      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0675 | val acc:\t0.6331\n",
      "\n",
      "Epoch 185/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9089 | train acc:\t0.6556\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.77      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.70      0.52      0.60      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.78      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.0548 | val acc:\t0.6364\n",
      "\n",
      "Epoch 186/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9061 | train acc:\t0.6566\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.71      0.57      0.63      1671\n",
      "           2       0.66      0.51      0.58      1174\n",
      "           3       0.61      0.61      0.61      1778\n",
      "           4       0.63      0.52      0.57      1329\n",
      "           5       0.79      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.0272 | val acc:\t0.6126\n",
      "\n",
      "Epoch 187/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8982 | train acc:\t0.6596\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.70      0.58      0.63      1671\n",
      "           2       0.68      0.54      0.60      1174\n",
      "           3       0.61      0.60      0.61      1778\n",
      "           4       0.61      0.53      0.57      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.0423 | val acc:\t0.6257\n",
      "\n",
      "Epoch 188/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9103 | train acc:\t0.6556\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.58      0.64      1671\n",
      "           2       0.68      0.51      0.58      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.0837 | val acc:\t0.6190\n",
      "\n",
      "Epoch 189/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9060 | train acc:\t0.6560\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.62      0.61      0.61      1778\n",
      "           4       0.63      0.53      0.57      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.0494 | val acc:\t0.6358\n",
      "\n",
      "Epoch 190/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9018 | train acc:\t0.6569\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.69      0.57      0.63      1671\n",
      "           2       0.69      0.51      0.59      1174\n",
      "           3       0.61      0.60      0.61      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.79      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.0810 | val acc:\t0.6221\n",
      "\n",
      "Epoch 191/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9055 | train acc:\t0.6584\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.58      0.64      1671\n",
      "           2       0.67      0.52      0.59      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.0239 | val acc:\t0.6331\n",
      "\n",
      "Epoch 192/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9104 | train acc:\t0.6500\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.67      0.52      0.58      1174\n",
      "           3       0.60      0.60      0.60      1778\n",
      "           4       0.61      0.51      0.56      1329\n",
      "           5       0.79      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0834 | val acc:\t0.6257\n",
      "\n",
      "Epoch 193/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8938 | train acc:\t0.6632\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.67      0.53      0.59      1174\n",
      "           3       0.63      0.62      0.62      1778\n",
      "           4       0.63      0.53      0.58      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.0553 | val acc:\t0.6374\n",
      "\n",
      "Epoch 194/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.8939 | train acc:\t0.6590\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.68      0.57      0.62      1671\n",
      "           2       0.69      0.53      0.60      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.64      0.55      0.59      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.0202 | val acc:\t0.6417\n",
      "\n",
      "Epoch 195/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8955 | train acc:\t0.6646\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.71      0.59      0.64      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.64      0.54      0.59      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.0224 | val acc:\t0.6328\n",
      "\n",
      "Epoch 196/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9009 | train acc:\t0.6615\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69      3943\n",
      "           1       0.71      0.59      0.65      1671\n",
      "           2       0.68      0.55      0.61      1174\n",
      "           3       0.63      0.60      0.61      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.0520 | val acc:\t0.6230\n",
      "\n",
      "Epoch 197/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9066 | train acc:\t0.6519\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.67      0.52      0.59      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.62      0.52      0.57      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0219 | val acc:\t0.6291\n",
      "\n",
      "Epoch 198/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9048 | train acc:\t0.6535\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.66      0.51      0.58      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.61      0.54      0.57      1329\n",
      "           5       0.79      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0347 | val acc:\t0.6334\n",
      "\n",
      "Epoch 199/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9095 | train acc:\t0.6535\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.68      0.50      0.58      1174\n",
      "           3       0.61      0.60      0.61      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0599 | val acc:\t0.6239\n",
      "\n",
      "Epoch 200/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9041 | train acc:\t0.6521\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.68      0.57      0.62      1671\n",
      "           2       0.67      0.54      0.59      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.61      0.53      0.57      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0378 | val acc:\t0.6248\n",
      "\n",
      "Training complete in 29m 50s\n",
      "Best val acc: 0.644717\n",
      "\t conv1.weight\n",
      "\t conv1.bias\n",
      "\t conv2.weight\n",
      "\t conv2.bias\n",
      "\t conv3.weight\n",
      "\t conv3.bias\n",
      "\t linear1.weight\n",
      "\t linear1.bias\n",
      "\t linear2.weight\n",
      "\t linear2.bias\n",
      "Epoch 1/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.6686 | train acc:\t0.3582\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.98      0.53      3943\n",
      "           1       0.17      0.03      0.04      1671\n",
      "           2       0.12      0.00      0.01      1174\n",
      "           3       0.00      0.00      0.00      1778\n",
      "           4       0.50      0.00      0.00      1329\n",
      "           5       0.00      0.00      0.00      1001\n",
      "\n",
      "    accuracy                           0.36     10896\n",
      "   macro avg       0.19      0.17      0.10     10896\n",
      "weighted avg       0.23      0.36      0.20     10896\n",
      "\n",
      "val loss:\t1.6177 | val acc:\t0.4649\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      1.00      0.63      1518\n",
      "           1       0.00      0.00      0.00       468\n",
      "           2       0.00      0.00      0.00       321\n",
      "           3       0.00      0.00      0.00       397\n",
      "           4       0.00      0.00      0.00       309\n",
      "           5       0.00      0.00      0.00       252\n",
      "\n",
      "    accuracy                           0.46      3265\n",
      "   macro avg       0.08      0.17      0.11      3265\n",
      "weighted avg       0.22      0.46      0.30      3265\n",
      "\n",
      "Epoch 2/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.5853 | train acc:\t0.3663\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.94      0.53      3943\n",
      "           1       0.29      0.03      0.05      1671\n",
      "           2       0.00      0.00      0.00      1174\n",
      "           3       0.28      0.05      0.09      1778\n",
      "           4       0.33      0.00      0.00      1329\n",
      "           5       0.48      0.12      0.20      1001\n",
      "\n",
      "    accuracy                           0.37     10896\n",
      "   macro avg       0.29      0.19      0.15     10896\n",
      "weighted avg       0.31      0.37      0.23     10896\n",
      "\n",
      "val loss:\t1.5525 | val acc:\t0.4809\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.92      0.64      1518\n",
      "           1       0.38      0.03      0.05       468\n",
      "           2       0.00      0.00      0.00       321\n",
      "           3       0.25      0.13      0.17       397\n",
      "           4       0.00      0.00      0.00       309\n",
      "           5       0.52      0.44      0.48       252\n",
      "\n",
      "    accuracy                           0.48      3265\n",
      "   macro avg       0.27      0.25      0.22      3265\n",
      "weighted avg       0.35      0.48      0.36      3265\n",
      "\n",
      "Epoch 3/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.5023 | train acc:\t0.3955\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.82      0.54      3943\n",
      "           1       0.29      0.11      0.16      1671\n",
      "           2       0.00      0.00      0.00      1174\n",
      "           3       0.33      0.24      0.28      1778\n",
      "           4       0.00      0.00      0.00      1329\n",
      "           5       0.52      0.48      0.50      1001\n",
      "\n",
      "    accuracy                           0.40     10896\n",
      "   macro avg       0.26      0.27      0.25     10896\n",
      "weighted avg       0.29      0.40      0.31     10896\n",
      "\n",
      "val loss:\t1.5068 | val acc:\t0.4744\n",
      "\n",
      "Epoch 4/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.4778 | train acc:\t0.4050\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.80      0.54      3943\n",
      "           1       0.31      0.14      0.19      1671\n",
      "           2       1.00      0.00      0.00      1174\n",
      "           3       0.33      0.27      0.30      1778\n",
      "           4       0.36      0.01      0.01      1329\n",
      "           5       0.54      0.55      0.55      1001\n",
      "\n",
      "    accuracy                           0.41     10896\n",
      "   macro avg       0.49      0.29      0.27     10896\n",
      "weighted avg       0.45      0.41      0.33     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.5003 | val acc:\t0.4891\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.83      0.66      1518\n",
      "           1       0.27      0.03      0.06       468\n",
      "           2       0.00      0.00      0.00       321\n",
      "           3       0.27      0.41      0.32       397\n",
      "           4       0.00      0.00      0.00       309\n",
      "           5       0.52      0.65      0.57       252\n",
      "\n",
      "    accuracy                           0.49      3265\n",
      "   macro avg       0.27      0.32      0.27      3265\n",
      "weighted avg       0.37      0.49      0.40      3265\n",
      "\n",
      "Epoch 5/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.4516 | train acc:\t0.4170\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.78      0.55      3943\n",
      "           1       0.33      0.17      0.23      1671\n",
      "           2       0.62      0.00      0.01      1174\n",
      "           3       0.35      0.31      0.33      1778\n",
      "           4       0.42      0.03      0.06      1329\n",
      "           5       0.55      0.57      0.56      1001\n",
      "\n",
      "    accuracy                           0.42     10896\n",
      "   macro avg       0.45      0.31      0.29     10896\n",
      "weighted avg       0.43      0.42      0.35     10896\n",
      "\n",
      "val loss:\t1.4418 | val acc:\t0.5133\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.96      0.67      1518\n",
      "           1       0.62      0.04      0.08       468\n",
      "           2       0.00      0.00      0.00       321\n",
      "           3       0.34      0.19      0.24       397\n",
      "           4       0.55      0.07      0.13       309\n",
      "           5       0.74      0.42      0.53       252\n",
      "\n",
      "    accuracy                           0.51      3265\n",
      "   macro avg       0.46      0.28      0.28      3265\n",
      "weighted avg       0.48      0.51      0.41      3265\n",
      "\n",
      "Epoch 6/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.4334 | train acc:\t0.4290\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.79      0.56      3943\n",
      "           1       0.38      0.19      0.26      1671\n",
      "           2       0.25      0.00      0.01      1174\n",
      "           3       0.37      0.33      0.35      1778\n",
      "           4       0.42      0.06      0.11      1329\n",
      "           5       0.56      0.57      0.56      1001\n",
      "\n",
      "    accuracy                           0.43     10896\n",
      "   macro avg       0.40      0.32      0.31     10896\n",
      "weighted avg       0.40      0.43      0.36     10896\n",
      "\n",
      "val loss:\t1.4643 | val acc:\t0.5277\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.88      0.68      1518\n",
      "           1       0.47      0.07      0.12       468\n",
      "           2       0.60      0.07      0.12       321\n",
      "           3       0.33      0.30      0.32       397\n",
      "           4       0.38      0.17      0.24       309\n",
      "           5       0.67      0.60      0.63       252\n",
      "\n",
      "    accuracy                           0.53      3265\n",
      "   macro avg       0.50      0.35      0.35      3265\n",
      "weighted avg       0.51      0.53      0.46      3265\n",
      "\n",
      "Epoch 7/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.4113 | train acc:\t0.4427\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.78      0.57      3943\n",
      "           1       0.40      0.21      0.27      1671\n",
      "           2       0.47      0.01      0.03      1174\n",
      "           3       0.37      0.37      0.37      1778\n",
      "           4       0.42      0.08      0.14      1329\n",
      "           5       0.60      0.60      0.60      1001\n",
      "\n",
      "    accuracy                           0.44     10896\n",
      "   macro avg       0.45      0.34      0.33     10896\n",
      "weighted avg       0.44      0.44      0.38     10896\n",
      "\n",
      "val loss:\t1.4219 | val acc:\t0.5342\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.89      0.68      1518\n",
      "           1       0.46      0.13      0.21       468\n",
      "           2       0.58      0.03      0.06       321\n",
      "           3       0.35      0.37      0.36       397\n",
      "           4       0.59      0.08      0.14       309\n",
      "           5       0.67      0.61      0.64       252\n",
      "\n",
      "    accuracy                           0.53      3265\n",
      "   macro avg       0.53      0.35      0.35      3265\n",
      "weighted avg       0.53      0.53      0.46      3265\n",
      "\n",
      "Epoch 8/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3903 | train acc:\t0.4518\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.78      0.57      3943\n",
      "           1       0.43      0.26      0.32      1671\n",
      "           2       0.52      0.03      0.05      1174\n",
      "           3       0.38      0.38      0.38      1778\n",
      "           4       0.40      0.09      0.15      1329\n",
      "           5       0.60      0.59      0.60      1001\n",
      "\n",
      "    accuracy                           0.45     10896\n",
      "   macro avg       0.46      0.35      0.35     10896\n",
      "weighted avg       0.45      0.45      0.40     10896\n",
      "\n",
      "val loss:\t1.4179 | val acc:\t0.5397\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.85      0.69      1518\n",
      "           1       0.39      0.28      0.33       468\n",
      "           2       0.71      0.07      0.12       321\n",
      "           3       0.37      0.31      0.34       397\n",
      "           4       0.40      0.26      0.31       309\n",
      "           5       0.75      0.49      0.59       252\n",
      "\n",
      "    accuracy                           0.54      3265\n",
      "   macro avg       0.53      0.38      0.40      3265\n",
      "weighted avg       0.54      0.54      0.50      3265\n",
      "\n",
      "Epoch 9/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3679 | train acc:\t0.4653\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.78      0.58      3943\n",
      "           1       0.43      0.28      0.34      1671\n",
      "           2       0.51      0.04      0.08      1174\n",
      "           3       0.40      0.39      0.40      1778\n",
      "           4       0.44      0.13      0.20      1329\n",
      "           5       0.61      0.61      0.61      1001\n",
      "\n",
      "    accuracy                           0.47     10896\n",
      "   macro avg       0.48      0.37      0.37     10896\n",
      "weighted avg       0.46      0.47      0.42     10896\n",
      "\n",
      "val loss:\t1.3909 | val acc:\t0.5489\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.82      0.70      1518\n",
      "           1       0.48      0.21      0.30       468\n",
      "           2       0.68      0.08      0.15       321\n",
      "           3       0.35      0.54      0.42       397\n",
      "           4       0.40      0.18      0.25       309\n",
      "           5       0.73      0.59      0.65       252\n",
      "\n",
      "    accuracy                           0.55      3265\n",
      "   macro avg       0.54      0.40      0.41      3265\n",
      "weighted avg       0.55      0.55      0.51      3265\n",
      "\n",
      "Epoch 10/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3449 | train acc:\t0.4715\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.77      0.59      3943\n",
      "           1       0.47      0.31      0.37      1671\n",
      "           2       0.52      0.06      0.10      1174\n",
      "           3       0.39      0.39      0.39      1778\n",
      "           4       0.40      0.15      0.21      1329\n",
      "           5       0.63      0.64      0.64      1001\n",
      "\n",
      "    accuracy                           0.47     10896\n",
      "   macro avg       0.48      0.38      0.38     10896\n",
      "weighted avg       0.47      0.47      0.43     10896\n",
      "\n",
      "val loss:\t1.4212 | val acc:\t0.5342\n",
      "\n",
      "Epoch 11/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3299 | train acc:\t0.4878\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.76      0.59      3943\n",
      "           1       0.50      0.36      0.42      1671\n",
      "           2       0.57      0.08      0.14      1174\n",
      "           3       0.40      0.42      0.41      1778\n",
      "           4       0.46      0.19      0.27      1329\n",
      "           5       0.63      0.64      0.63      1001\n",
      "\n",
      "    accuracy                           0.49     10896\n",
      "   macro avg       0.51      0.41      0.41     10896\n",
      "weighted avg       0.49      0.49      0.45     10896\n",
      "\n",
      "val loss:\t1.3807 | val acc:\t0.5498\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.77      0.69      1518\n",
      "           1       0.39      0.38      0.38       468\n",
      "           2       0.66      0.10      0.18       321\n",
      "           3       0.37      0.41      0.39       397\n",
      "           4       0.41      0.28      0.34       309\n",
      "           5       0.73      0.64      0.68       252\n",
      "\n",
      "    accuracy                           0.55      3265\n",
      "   macro avg       0.53      0.43      0.44      3265\n",
      "weighted avg       0.55      0.55      0.52      3265\n",
      "\n",
      "Epoch 12/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t1.3205 | train acc:\t0.4849\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.76      0.59      3943\n",
      "           1       0.48      0.35      0.40      1671\n",
      "           2       0.53      0.09      0.15      1174\n",
      "           3       0.42      0.41      0.42      1778\n",
      "           4       0.43      0.18      0.26      1329\n",
      "           5       0.64      0.62      0.63      1001\n",
      "\n",
      "    accuracy                           0.48     10896\n",
      "   macro avg       0.50      0.40      0.41     10896\n",
      "weighted avg       0.48      0.48      0.45     10896\n",
      "\n",
      "val loss:\t1.3663 | val acc:\t0.5544\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.79      0.69      1518\n",
      "           1       0.48      0.23      0.31       468\n",
      "           2       0.66      0.13      0.21       321\n",
      "           3       0.38      0.50      0.43       397\n",
      "           4       0.37      0.38      0.37       309\n",
      "           5       0.76      0.59      0.66       252\n",
      "\n",
      "    accuracy                           0.55      3265\n",
      "   macro avg       0.55      0.44      0.45      3265\n",
      "weighted avg       0.56      0.55      0.53      3265\n",
      "\n",
      "Epoch 13/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3040 | train acc:\t0.4929\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.75      0.59      3943\n",
      "           1       0.49      0.36      0.42      1671\n",
      "           2       0.53      0.10      0.17      1174\n",
      "           3       0.42      0.42      0.42      1778\n",
      "           4       0.44      0.21      0.28      1329\n",
      "           5       0.67      0.67      0.67      1001\n",
      "\n",
      "    accuracy                           0.49     10896\n",
      "   macro avg       0.51      0.42      0.43     10896\n",
      "weighted avg       0.49      0.49      0.46     10896\n",
      "\n",
      "val loss:\t1.3512 | val acc:\t0.5611\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.82      0.70      1518\n",
      "           1       0.54      0.22      0.31       468\n",
      "           2       0.61      0.17      0.27       321\n",
      "           3       0.41      0.35      0.38       397\n",
      "           4       0.35      0.38      0.36       309\n",
      "           5       0.74      0.66      0.70       252\n",
      "\n",
      "    accuracy                           0.56      3265\n",
      "   macro avg       0.54      0.43      0.45      3265\n",
      "weighted avg       0.56      0.56      0.53      3265\n",
      "\n",
      "Epoch 14/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2953 | train acc:\t0.4983\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.75      0.60      3943\n",
      "           1       0.52      0.40      0.45      1671\n",
      "           2       0.57      0.12      0.19      1174\n",
      "           3       0.41      0.41      0.41      1778\n",
      "           4       0.45      0.23      0.30      1329\n",
      "           5       0.64      0.64      0.64      1001\n",
      "\n",
      "    accuracy                           0.50     10896\n",
      "   macro avg       0.52      0.42      0.43     10896\n",
      "weighted avg       0.50      0.50      0.47     10896\n",
      "\n",
      "val loss:\t1.3413 | val acc:\t0.5663\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.81      0.70      1518\n",
      "           1       0.45      0.31      0.37       468\n",
      "           2       0.56      0.19      0.29       321\n",
      "           3       0.40      0.39      0.39       397\n",
      "           4       0.41      0.28      0.34       309\n",
      "           5       0.73      0.70      0.71       252\n",
      "\n",
      "    accuracy                           0.57      3265\n",
      "   macro avg       0.53      0.45      0.46      3265\n",
      "weighted avg       0.55      0.57      0.54      3265\n",
      "\n",
      "Epoch 15/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2847 | train acc:\t0.5039\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.74      0.59      3943\n",
      "           1       0.54      0.40      0.46      1671\n",
      "           2       0.56      0.11      0.18      1174\n",
      "           3       0.43      0.44      0.44      1778\n",
      "           4       0.47      0.26      0.33      1329\n",
      "           5       0.66      0.66      0.66      1001\n",
      "\n",
      "    accuracy                           0.50     10896\n",
      "   macro avg       0.53      0.43      0.44     10896\n",
      "weighted avg       0.51      0.50      0.48     10896\n",
      "\n",
      "val loss:\t1.3262 | val acc:\t0.5626\n",
      "\n",
      "Epoch 16/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2750 | train acc:\t0.5052\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.74      0.60      3943\n",
      "           1       0.52      0.40      0.45      1671\n",
      "           2       0.61      0.13      0.21      1174\n",
      "           3       0.43      0.44      0.43      1778\n",
      "           4       0.45      0.25      0.32      1329\n",
      "           5       0.66      0.66      0.66      1001\n",
      "\n",
      "    accuracy                           0.51     10896\n",
      "   macro avg       0.53      0.44      0.45     10896\n",
      "weighted avg       0.51      0.51      0.48     10896\n",
      "\n",
      "val loss:\t1.3097 | val acc:\t0.5593\n",
      "\n",
      "Epoch 17/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2676 | train acc:\t0.5073\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.73      0.60      3943\n",
      "           1       0.54      0.42      0.47      1671\n",
      "           2       0.52      0.15      0.24      1174\n",
      "           3       0.43      0.43      0.43      1778\n",
      "           4       0.44      0.26      0.33      1329\n",
      "           5       0.66      0.65      0.65      1001\n",
      "\n",
      "    accuracy                           0.51     10896\n",
      "   macro avg       0.52      0.44      0.45     10896\n",
      "weighted avg       0.51      0.51      0.48     10896\n",
      "\n",
      "val loss:\t1.3157 | val acc:\t0.5559\n",
      "\n",
      "Epoch 18/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2647 | train acc:\t0.5127\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.74      0.60      3943\n",
      "           1       0.54      0.42      0.47      1671\n",
      "           2       0.52      0.15      0.23      1174\n",
      "           3       0.44      0.43      0.43      1778\n",
      "           4       0.47      0.28      0.35      1329\n",
      "           5       0.67      0.67      0.67      1001\n",
      "\n",
      "    accuracy                           0.51     10896\n",
      "   macro avg       0.52      0.45      0.46     10896\n",
      "weighted avg       0.51      0.51      0.49     10896\n",
      "\n",
      "val loss:\t1.3196 | val acc:\t0.5587\n",
      "\n",
      "Epoch 19/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2556 | train acc:\t0.5148\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.73      0.60      3943\n",
      "           1       0.53      0.42      0.47      1671\n",
      "           2       0.57      0.16      0.25      1174\n",
      "           3       0.43      0.43      0.43      1778\n",
      "           4       0.47      0.29      0.35      1329\n",
      "           5       0.68      0.68      0.68      1001\n",
      "\n",
      "    accuracy                           0.51     10896\n",
      "   macro avg       0.53      0.45      0.46     10896\n",
      "weighted avg       0.52      0.51      0.49     10896\n",
      "\n",
      "val loss:\t1.3423 | val acc:\t0.5565\n",
      "\n",
      "Epoch 20/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2525 | train acc:\t0.5136\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.74      0.60      3943\n",
      "           1       0.55      0.43      0.48      1671\n",
      "           2       0.52      0.16      0.24      1174\n",
      "           3       0.44      0.43      0.43      1778\n",
      "           4       0.46      0.27      0.34      1329\n",
      "           5       0.66      0.66      0.66      1001\n",
      "\n",
      "    accuracy                           0.51     10896\n",
      "   macro avg       0.52      0.45      0.46     10896\n",
      "weighted avg       0.51      0.51      0.49     10896\n",
      "\n",
      "val loss:\t1.3156 | val acc:\t0.5657\n",
      "\n",
      "Epoch 21/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2429 | train acc:\t0.5222\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.73      0.61      3943\n",
      "           1       0.56      0.44      0.49      1671\n",
      "           2       0.55      0.17      0.26      1174\n",
      "           3       0.45      0.44      0.44      1778\n",
      "           4       0.47      0.29      0.36      1329\n",
      "           5       0.69      0.69      0.69      1001\n",
      "\n",
      "    accuracy                           0.52     10896\n",
      "   macro avg       0.54      0.46      0.47     10896\n",
      "weighted avg       0.52      0.52      0.50     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.3266 | val acc:\t0.5602\n",
      "\n",
      "Epoch 22/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2410 | train acc:\t0.5211\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.73      0.60      3943\n",
      "           1       0.56      0.43      0.49      1671\n",
      "           2       0.55      0.19      0.28      1174\n",
      "           3       0.45      0.44      0.45      1778\n",
      "           4       0.46      0.29      0.36      1329\n",
      "           5       0.68      0.67      0.67      1001\n",
      "\n",
      "    accuracy                           0.52     10896\n",
      "   macro avg       0.54      0.46      0.48     10896\n",
      "weighted avg       0.52      0.52      0.50     10896\n",
      "\n",
      "val loss:\t1.3265 | val acc:\t0.5516\n",
      "\n",
      "Epoch 23/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2383 | train acc:\t0.5231\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.73      0.60      3943\n",
      "           1       0.56      0.43      0.49      1671\n",
      "           2       0.50      0.18      0.26      1174\n",
      "           3       0.46      0.45      0.46      1778\n",
      "           4       0.46      0.31      0.37      1329\n",
      "           5       0.68      0.69      0.68      1001\n",
      "\n",
      "    accuracy                           0.52     10896\n",
      "   macro avg       0.53      0.46      0.48     10896\n",
      "weighted avg       0.52      0.52      0.50     10896\n",
      "\n",
      "val loss:\t1.2820 | val acc:\t0.5798\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.83      0.71      1518\n",
      "           1       0.53      0.28      0.37       468\n",
      "           2       0.54      0.27      0.36       321\n",
      "           3       0.42      0.41      0.42       397\n",
      "           4       0.40      0.35      0.37       309\n",
      "           5       0.88      0.60      0.71       252\n",
      "\n",
      "    accuracy                           0.58      3265\n",
      "   macro avg       0.57      0.46      0.49      3265\n",
      "weighted avg       0.57      0.58      0.56      3265\n",
      "\n",
      "Epoch 24/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2235 | train acc:\t0.5259\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.73      0.61      3943\n",
      "           1       0.56      0.42      0.48      1671\n",
      "           2       0.52      0.19      0.28      1174\n",
      "           3       0.45      0.46      0.45      1778\n",
      "           4       0.48      0.32      0.38      1329\n",
      "           5       0.69      0.69      0.69      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.54      0.47      0.48     10896\n",
      "weighted avg       0.53      0.53      0.51     10896\n",
      "\n",
      "val loss:\t1.2577 | val acc:\t0.5737\n",
      "\n",
      "Epoch 25/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2205 | train acc:\t0.5252\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.73      0.60      3943\n",
      "           1       0.56      0.44      0.50      1671\n",
      "           2       0.57      0.22      0.31      1174\n",
      "           3       0.44      0.43      0.44      1778\n",
      "           4       0.46      0.31      0.37      1329\n",
      "           5       0.70      0.68      0.69      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.54      0.47      0.49     10896\n",
      "weighted avg       0.53      0.53      0.51     10896\n",
      "\n",
      "val loss:\t1.3004 | val acc:\t0.5568\n",
      "\n",
      "Epoch 26/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2152 | train acc:\t0.5271\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.73      0.61      3943\n",
      "           1       0.57      0.44      0.50      1671\n",
      "           2       0.53      0.21      0.30      1174\n",
      "           3       0.45      0.44      0.45      1778\n",
      "           4       0.47      0.31      0.37      1329\n",
      "           5       0.69      0.69      0.69      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.54      0.47      0.49     10896\n",
      "weighted avg       0.53      0.53      0.51     10896\n",
      "\n",
      "val loss:\t1.2562 | val acc:\t0.5881\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.83      0.71      1518\n",
      "           1       0.49      0.36      0.42       468\n",
      "           2       0.54      0.28      0.37       321\n",
      "           3       0.43      0.35      0.39       397\n",
      "           4       0.46      0.29      0.36       309\n",
      "           5       0.86      0.68      0.76       252\n",
      "\n",
      "    accuracy                           0.59      3265\n",
      "   macro avg       0.57      0.47      0.50      3265\n",
      "weighted avg       0.57      0.59      0.57      3265\n",
      "\n",
      "Epoch 27/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2132 | train acc:\t0.5308\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.73      0.61      3943\n",
      "           1       0.57      0.43      0.49      1671\n",
      "           2       0.53      0.22      0.31      1174\n",
      "           3       0.45      0.45      0.45      1778\n",
      "           4       0.48      0.32      0.38      1329\n",
      "           5       0.70      0.70      0.70      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.54      0.47      0.49     10896\n",
      "weighted avg       0.53      0.53      0.51     10896\n",
      "\n",
      "val loss:\t1.2833 | val acc:\t0.5623\n",
      "\n",
      "Epoch 28/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2079 | train acc:\t0.5345\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.74      0.61      3943\n",
      "           1       0.57      0.44      0.49      1671\n",
      "           2       0.55      0.21      0.30      1174\n",
      "           3       0.46      0.46      0.46      1778\n",
      "           4       0.49      0.32      0.39      1329\n",
      "           5       0.70      0.69      0.70      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.55      0.48      0.49     10896\n",
      "weighted avg       0.54      0.53      0.52     10896\n",
      "\n",
      "val loss:\t1.2721 | val acc:\t0.5798\n",
      "\n",
      "Epoch 29/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1967 | train acc:\t0.5384\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.62      3943\n",
      "           1       0.57      0.43      0.49      1671\n",
      "           2       0.58      0.24      0.34      1174\n",
      "           3       0.46      0.45      0.46      1778\n",
      "           4       0.48      0.34      0.40      1329\n",
      "           5       0.71      0.71      0.71      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.55      0.49      0.50     10896\n",
      "weighted avg       0.54      0.54      0.52     10896\n",
      "\n",
      "val loss:\t1.3231 | val acc:\t0.5534\n",
      "\n",
      "Epoch 30/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1981 | train acc:\t0.5373\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.61      3943\n",
      "           1       0.58      0.44      0.50      1671\n",
      "           2       0.54      0.24      0.33      1174\n",
      "           3       0.47      0.46      0.46      1778\n",
      "           4       0.48      0.34      0.40      1329\n",
      "           5       0.70      0.70      0.70      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.55      0.48      0.50     10896\n",
      "weighted avg       0.54      0.54      0.52     10896\n",
      "\n",
      "val loss:\t1.2561 | val acc:\t0.5813\n",
      "\n",
      "Epoch 31/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1966 | train acc:\t0.5348\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.73      0.61      3943\n",
      "           1       0.58      0.43      0.50      1671\n",
      "           2       0.55      0.23      0.33      1174\n",
      "           3       0.47      0.46      0.47      1778\n",
      "           4       0.47      0.32      0.38      1329\n",
      "           5       0.70      0.70      0.70      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.55      0.48      0.50     10896\n",
      "weighted avg       0.54      0.53      0.52     10896\n",
      "\n",
      "val loss:\t1.2272 | val acc:\t0.5850\n",
      "\n",
      "Epoch 32/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1868 | train acc:\t0.5394\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.61      3943\n",
      "           1       0.57      0.44      0.50      1671\n",
      "           2       0.55      0.24      0.33      1174\n",
      "           3       0.47      0.45      0.46      1778\n",
      "           4       0.48      0.35      0.41      1329\n",
      "           5       0.71      0.72      0.71      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.55      0.49      0.50     10896\n",
      "weighted avg       0.54      0.54      0.52     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.3253 | val acc:\t0.5329\n",
      "\n",
      "Epoch 33/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1874 | train acc:\t0.5413\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.62      3943\n",
      "           1       0.59      0.44      0.50      1671\n",
      "           2       0.55      0.25      0.34      1174\n",
      "           3       0.46      0.46      0.46      1778\n",
      "           4       0.49      0.35      0.41      1329\n",
      "           5       0.70      0.73      0.71      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.55      0.49      0.51     10896\n",
      "weighted avg       0.54      0.54      0.53     10896\n",
      "\n",
      "val loss:\t1.2478 | val acc:\t0.5874\n",
      "\n",
      "Epoch 34/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1847 | train acc:\t0.5422\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.74      0.62      3943\n",
      "           1       0.59      0.44      0.51      1671\n",
      "           2       0.57      0.25      0.35      1174\n",
      "           3       0.47      0.46      0.47      1778\n",
      "           4       0.48      0.33      0.39      1329\n",
      "           5       0.70      0.71      0.70      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.56      0.49      0.51     10896\n",
      "weighted avg       0.54      0.54      0.53     10896\n",
      "\n",
      "val loss:\t1.2160 | val acc:\t0.5807\n",
      "\n",
      "Epoch 35/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1802 | train acc:\t0.5434\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.61      3943\n",
      "           1       0.60      0.44      0.51      1671\n",
      "           2       0.56      0.26      0.36      1174\n",
      "           3       0.47      0.47      0.47      1778\n",
      "           4       0.49      0.33      0.40      1329\n",
      "           5       0.71      0.72      0.72      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.56      0.49      0.51     10896\n",
      "weighted avg       0.55      0.54      0.53     10896\n",
      "\n",
      "val loss:\t1.2296 | val acc:\t0.5942\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.83      0.71      1518\n",
      "           1       0.58      0.22      0.31       468\n",
      "           2       0.56      0.32      0.41       321\n",
      "           3       0.42      0.57      0.48       397\n",
      "           4       0.51      0.28      0.36       309\n",
      "           5       0.87      0.67      0.76       252\n",
      "\n",
      "    accuracy                           0.59      3265\n",
      "   macro avg       0.59      0.48      0.51      3265\n",
      "weighted avg       0.60      0.59      0.57      3265\n",
      "\n",
      "Epoch 36/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1739 | train acc:\t0.5434\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.62      3943\n",
      "           1       0.59      0.44      0.51      1671\n",
      "           2       0.53      0.24      0.33      1174\n",
      "           3       0.48      0.47      0.47      1778\n",
      "           4       0.48      0.34      0.40      1329\n",
      "           5       0.72      0.72      0.72      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.56      0.49      0.51     10896\n",
      "weighted avg       0.54      0.54      0.53     10896\n",
      "\n",
      "val loss:\t1.2595 | val acc:\t0.5795\n",
      "\n",
      "Epoch 37/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1698 | train acc:\t0.5499\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.59      0.43      0.50      1671\n",
      "           2       0.57      0.29      0.38      1174\n",
      "           3       0.48      0.47      0.47      1778\n",
      "           4       0.50      0.35      0.41      1329\n",
      "           5       0.71      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.56      0.50      0.52     10896\n",
      "weighted avg       0.55      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.2311 | val acc:\t0.5926\n",
      "\n",
      "Epoch 38/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1664 | train acc:\t0.5531\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.63      3943\n",
      "           1       0.59      0.44      0.51      1671\n",
      "           2       0.56      0.26      0.36      1174\n",
      "           3       0.49      0.48      0.48      1778\n",
      "           4       0.52      0.36      0.42      1329\n",
      "           5       0.71      0.72      0.72      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.57      0.50      0.52     10896\n",
      "weighted avg       0.56      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.2581 | val acc:\t0.5749\n",
      "\n",
      "Epoch 39/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1583 | train acc:\t0.5556\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.60      0.45      0.52      1671\n",
      "           2       0.57      0.28      0.37      1174\n",
      "           3       0.50      0.49      0.49      1778\n",
      "           4       0.51      0.36      0.42      1329\n",
      "           5       0.71      0.73      0.72      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.57      0.51      0.52     10896\n",
      "weighted avg       0.56      0.56      0.54     10896\n",
      "\n",
      "val loss:\t1.2403 | val acc:\t0.5877\n",
      "\n",
      "Epoch 40/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1586 | train acc:\t0.5518\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.60      0.44      0.51      1671\n",
      "           2       0.58      0.28      0.38      1174\n",
      "           3       0.49      0.49      0.49      1778\n",
      "           4       0.50      0.35      0.41      1329\n",
      "           5       0.71      0.72      0.71      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.57      0.50      0.52     10896\n",
      "weighted avg       0.55      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.2189 | val acc:\t0.5856\n",
      "\n",
      "Epoch 41/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1630 | train acc:\t0.5502\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.61      0.44      0.51      1671\n",
      "           2       0.58      0.28      0.38      1174\n",
      "           3       0.49      0.48      0.48      1778\n",
      "           4       0.49      0.36      0.41      1329\n",
      "           5       0.70      0.71      0.70      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.57      0.50      0.52     10896\n",
      "weighted avg       0.55      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.2301 | val acc:\t0.5939\n",
      "\n",
      "Epoch 42/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1529 | train acc:\t0.5539\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.60      0.44      0.51      1671\n",
      "           2       0.56      0.28      0.37      1174\n",
      "           3       0.49      0.48      0.49      1778\n",
      "           4       0.51      0.37      0.43      1329\n",
      "           5       0.71      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.57      0.51      0.52     10896\n",
      "weighted avg       0.56      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.2343 | val acc:\t0.5899\n",
      "\n",
      "Epoch 43/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1562 | train acc:\t0.5530\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.63      0.45      0.52      1671\n",
      "           2       0.55      0.28      0.37      1174\n",
      "           3       0.48      0.48      0.48      1778\n",
      "           4       0.51      0.35      0.42      1329\n",
      "           5       0.71      0.73      0.72      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.57      0.50      0.52     10896\n",
      "weighted avg       0.56      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.2378 | val acc:\t0.5816\n",
      "\n",
      "Epoch 44/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t1.1433 | train acc:\t0.5566\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.63      3943\n",
      "           1       0.61      0.45      0.51      1671\n",
      "           2       0.58      0.29      0.39      1174\n",
      "           3       0.50      0.48      0.49      1778\n",
      "           4       0.51      0.38      0.43      1329\n",
      "           5       0.70      0.72      0.71      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.57      0.51      0.53     10896\n",
      "weighted avg       0.56      0.56      0.54     10896\n",
      "\n",
      "val loss:\t1.2346 | val acc:\t0.5930\n",
      "\n",
      "Epoch 45/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1472 | train acc:\t0.5573\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.61      0.42      0.50      1671\n",
      "           2       0.58      0.29      0.38      1174\n",
      "           3       0.47      0.49      0.48      1778\n",
      "           4       0.52      0.38      0.44      1329\n",
      "           5       0.72      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.57      0.51      0.53     10896\n",
      "weighted avg       0.56      0.56      0.54     10896\n",
      "\n",
      "val loss:\t1.2183 | val acc:\t0.5942\n",
      "\n",
      "Epoch 46/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1420 | train acc:\t0.5580\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.61      0.45      0.52      1671\n",
      "           2       0.59      0.29      0.39      1174\n",
      "           3       0.49      0.49      0.49      1778\n",
      "           4       0.51      0.38      0.44      1329\n",
      "           5       0.72      0.73      0.72      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.51      0.53     10896\n",
      "weighted avg       0.56      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.2357 | val acc:\t0.5783\n",
      "\n",
      "Epoch 47/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1386 | train acc:\t0.5550\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.63      3943\n",
      "           1       0.60      0.44      0.51      1671\n",
      "           2       0.58      0.29      0.39      1174\n",
      "           3       0.48      0.47      0.48      1778\n",
      "           4       0.50      0.36      0.42      1329\n",
      "           5       0.72      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.57      0.51      0.53     10896\n",
      "weighted avg       0.56      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.2017 | val acc:\t0.5942\n",
      "\n",
      "Epoch 48/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1335 | train acc:\t0.5598\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.61      0.45      0.51      1671\n",
      "           2       0.58      0.30      0.39      1174\n",
      "           3       0.49      0.47      0.48      1778\n",
      "           4       0.51      0.38      0.44      1329\n",
      "           5       0.72      0.73      0.73      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.51      0.53     10896\n",
      "weighted avg       0.56      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.2040 | val acc:\t0.6031\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.80      0.72      1518\n",
      "           1       0.53      0.37      0.44       468\n",
      "           2       0.68      0.25      0.36       321\n",
      "           3       0.42      0.55      0.48       397\n",
      "           4       0.45      0.33      0.38       309\n",
      "           5       0.88      0.69      0.78       252\n",
      "\n",
      "    accuracy                           0.60      3265\n",
      "   macro avg       0.60      0.50      0.53      3265\n",
      "weighted avg       0.61      0.60      0.59      3265\n",
      "\n",
      "Epoch 49/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1359 | train acc:\t0.5613\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.61      0.44      0.52      1671\n",
      "           2       0.58      0.30      0.39      1174\n",
      "           3       0.49      0.49      0.49      1778\n",
      "           4       0.52      0.38      0.44      1329\n",
      "           5       0.73      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.52      0.53     10896\n",
      "weighted avg       0.56      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.1862 | val acc:\t0.5917\n",
      "\n",
      "Epoch 50/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1266 | train acc:\t0.5735\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.75      0.64      3943\n",
      "           1       0.63      0.45      0.52      1671\n",
      "           2       0.63      0.31      0.42      1174\n",
      "           3       0.51      0.52      0.51      1778\n",
      "           4       0.53      0.40      0.46      1329\n",
      "           5       0.73      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.60      0.53      0.55     10896\n",
      "weighted avg       0.58      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.2041 | val acc:\t0.5945\n",
      "\n",
      "Epoch 51/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1288 | train acc:\t0.5633\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.62      0.44      0.52      1671\n",
      "           2       0.58      0.31      0.41      1174\n",
      "           3       0.50      0.49      0.49      1778\n",
      "           4       0.51      0.40      0.45      1329\n",
      "           5       0.71      0.73      0.72      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.52      0.54     10896\n",
      "weighted avg       0.57      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.2029 | val acc:\t0.5954\n",
      "\n",
      "Epoch 52/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1263 | train acc:\t0.5677\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.75      0.63      3943\n",
      "           1       0.63      0.45      0.52      1671\n",
      "           2       0.59      0.30      0.40      1174\n",
      "           3       0.50      0.49      0.49      1778\n",
      "           4       0.52      0.40      0.45      1329\n",
      "           5       0.73      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.52      0.54     10896\n",
      "weighted avg       0.57      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.1971 | val acc:\t0.5975\n",
      "\n",
      "Epoch 53/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1209 | train acc:\t0.5652\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63      3943\n",
      "           1       0.61      0.45      0.51      1671\n",
      "           2       0.59      0.32      0.41      1174\n",
      "           3       0.50      0.49      0.50      1778\n",
      "           4       0.53      0.40      0.46      1329\n",
      "           5       0.72      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.58      0.52      0.54     10896\n",
      "weighted avg       0.57      0.57      0.55     10896\n",
      "\n",
      "val loss:\t1.1906 | val acc:\t0.5994\n",
      "\n",
      "Epoch 54/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1226 | train acc:\t0.5675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.63      0.44      0.51      1671\n",
      "           2       0.61      0.30      0.40      1174\n",
      "           3       0.50      0.49      0.50      1778\n",
      "           4       0.53      0.40      0.45      1329\n",
      "           5       0.72      0.76      0.74      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.52      0.54     10896\n",
      "weighted avg       0.57      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.2423 | val acc:\t0.5700\n",
      "\n",
      "Epoch 55/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1170 | train acc:\t0.5707\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.63      0.45      0.53      1671\n",
      "           2       0.60      0.32      0.42      1174\n",
      "           3       0.51      0.50      0.50      1778\n",
      "           4       0.52      0.40      0.45      1329\n",
      "           5       0.73      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.53      0.54     10896\n",
      "weighted avg       0.57      0.57      0.56     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1681 | val acc:\t0.6064\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.85      0.71      1518\n",
      "           1       0.53      0.38      0.44       468\n",
      "           2       0.67      0.27      0.38       321\n",
      "           3       0.47      0.45      0.46       397\n",
      "           4       0.54      0.24      0.33       309\n",
      "           5       0.87      0.71      0.79       252\n",
      "\n",
      "    accuracy                           0.61      3265\n",
      "   macro avg       0.62      0.48      0.52      3265\n",
      "weighted avg       0.60      0.61      0.58      3265\n",
      "\n",
      "Epoch 56/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1126 | train acc:\t0.5751\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.63      0.46      0.53      1671\n",
      "           2       0.62      0.32      0.42      1174\n",
      "           3       0.52      0.51      0.51      1778\n",
      "           4       0.54      0.41      0.46      1329\n",
      "           5       0.73      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.53      0.55     10896\n",
      "weighted avg       0.58      0.58      0.56     10896\n",
      "\n",
      "val loss:\t1.2131 | val acc:\t0.5994\n",
      "\n",
      "Epoch 57/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1016 | train acc:\t0.5759\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      3943\n",
      "           1       0.63      0.45      0.53      1671\n",
      "           2       0.62      0.33      0.43      1174\n",
      "           3       0.51      0.51      0.51      1778\n",
      "           4       0.55      0.40      0.47      1329\n",
      "           5       0.72      0.75      0.73      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.53      0.55     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2141 | val acc:\t0.5816\n",
      "\n",
      "Epoch 58/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1057 | train acc:\t0.5732\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.63      3943\n",
      "           1       0.62      0.47      0.53      1671\n",
      "           2       0.59      0.32      0.42      1174\n",
      "           3       0.52      0.50      0.51      1778\n",
      "           4       0.52      0.41      0.46      1329\n",
      "           5       0.73      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.53      0.55     10896\n",
      "weighted avg       0.58      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.1869 | val acc:\t0.6104\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.82      0.72      1518\n",
      "           1       0.54      0.35      0.42       468\n",
      "           2       0.64      0.31      0.42       321\n",
      "           3       0.44      0.53      0.48       397\n",
      "           4       0.51      0.28      0.37       309\n",
      "           5       0.85      0.74      0.79       252\n",
      "\n",
      "    accuracy                           0.61      3265\n",
      "   macro avg       0.60      0.51      0.53      3265\n",
      "weighted avg       0.61      0.61      0.59      3265\n",
      "\n",
      "Epoch 59/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1011 | train acc:\t0.5721\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.75      0.64      3943\n",
      "           1       0.65      0.46      0.54      1671\n",
      "           2       0.59      0.30      0.40      1174\n",
      "           3       0.50      0.51      0.50      1778\n",
      "           4       0.52      0.38      0.44      1329\n",
      "           5       0.73      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.53      0.54     10896\n",
      "weighted avg       0.58      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.1935 | val acc:\t0.5988\n",
      "\n",
      "Epoch 60/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1017 | train acc:\t0.5771\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      3943\n",
      "           1       0.63      0.46      0.53      1671\n",
      "           2       0.60      0.31      0.41      1174\n",
      "           3       0.52      0.50      0.51      1778\n",
      "           4       0.54      0.42      0.47      1329\n",
      "           5       0.74      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.53      0.55     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.1751 | val acc:\t0.5942\n",
      "\n",
      "Epoch 61/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0980 | train acc:\t0.5782\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.64      0.47      0.54      1671\n",
      "           2       0.61      0.33      0.43      1174\n",
      "           3       0.51      0.50      0.51      1778\n",
      "           4       0.53      0.42      0.47      1329\n",
      "           5       0.74      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.55     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.1865 | val acc:\t0.5979\n",
      "\n",
      "Epoch 62/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0886 | train acc:\t0.5821\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      3943\n",
      "           1       0.65      0.47      0.55      1671\n",
      "           2       0.61      0.33      0.43      1174\n",
      "           3       0.51      0.51      0.51      1778\n",
      "           4       0.55      0.41      0.47      1329\n",
      "           5       0.74      0.77      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.59      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2282 | val acc:\t0.5737\n",
      "\n",
      "Epoch 63/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0895 | train acc:\t0.5809\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      3943\n",
      "           1       0.63      0.48      0.54      1671\n",
      "           2       0.60      0.32      0.42      1174\n",
      "           3       0.52      0.50      0.51      1778\n",
      "           4       0.55      0.41      0.47      1329\n",
      "           5       0.74      0.77      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2200 | val acc:\t0.5874\n",
      "\n",
      "Epoch 64/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0891 | train acc:\t0.5792\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      3943\n",
      "           1       0.63      0.47      0.54      1671\n",
      "           2       0.63      0.34      0.45      1174\n",
      "           3       0.52      0.50      0.51      1778\n",
      "           4       0.53      0.40      0.46      1329\n",
      "           5       0.75      0.76      0.76      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.1862 | val acc:\t0.6070\n",
      "\n",
      "Epoch 65/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0856 | train acc:\t0.5763\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.62      0.46      0.53      1671\n",
      "           2       0.58      0.34      0.43      1174\n",
      "           3       0.51      0.50      0.51      1778\n",
      "           4       0.53      0.41      0.46      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.59      0.54      0.55     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.1924 | val acc:\t0.6080\n",
      "\n",
      "Epoch 66/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0804 | train acc:\t0.5848\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      3943\n",
      "           1       0.64      0.48      0.55      1671\n",
      "           2       0.63      0.33      0.43      1174\n",
      "           3       0.55      0.52      0.53      1778\n",
      "           4       0.52      0.41      0.46      1329\n",
      "           5       0.74      0.75      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.61      0.54      0.56     10896\n",
      "weighted avg       0.59      0.58      0.57     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.2024 | val acc:\t0.5997\n",
      "\n",
      "Epoch 67/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0734 | train acc:\t0.5831\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.64      0.46      0.54      1671\n",
      "           2       0.61      0.36      0.45      1174\n",
      "           3       0.52      0.51      0.52      1778\n",
      "           4       0.54      0.43      0.48      1329\n",
      "           5       0.75      0.75      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.59      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.1873 | val acc:\t0.6110\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      1518\n",
      "           1       0.55      0.38      0.45       468\n",
      "           2       0.59      0.39      0.47       321\n",
      "           3       0.46      0.51      0.48       397\n",
      "           4       0.44      0.40      0.42       309\n",
      "           5       0.80      0.77      0.79       252\n",
      "\n",
      "    accuracy                           0.61      3265\n",
      "   macro avg       0.58      0.54      0.55      3265\n",
      "weighted avg       0.61      0.61      0.60      3265\n",
      "\n",
      "Epoch 68/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0746 | train acc:\t0.5827\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      3943\n",
      "           1       0.63      0.47      0.54      1671\n",
      "           2       0.62      0.34      0.44      1174\n",
      "           3       0.52      0.50      0.51      1778\n",
      "           4       0.54      0.43      0.48      1329\n",
      "           5       0.74      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.59      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.1900 | val acc:\t0.6018\n",
      "\n",
      "Epoch 69/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0734 | train acc:\t0.5831\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      3943\n",
      "           1       0.64      0.47      0.54      1671\n",
      "           2       0.61      0.35      0.45      1174\n",
      "           3       0.54      0.51      0.53      1778\n",
      "           4       0.54      0.42      0.47      1329\n",
      "           5       0.74      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.59      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.1957 | val acc:\t0.6031\n",
      "\n",
      "Epoch 70/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0678 | train acc:\t0.5850\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      3943\n",
      "           1       0.64      0.46      0.54      1671\n",
      "           2       0.61      0.35      0.44      1174\n",
      "           3       0.52      0.51      0.52      1778\n",
      "           4       0.56      0.43      0.49      1329\n",
      "           5       0.76      0.76      0.76      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.61      0.54      0.56     10896\n",
      "weighted avg       0.59      0.58      0.58     10896\n",
      "\n",
      "val loss:\t1.1577 | val acc:\t0.6101\n",
      "\n",
      "Epoch 71/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0719 | train acc:\t0.5872\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      3943\n",
      "           1       0.64      0.48      0.55      1671\n",
      "           2       0.60      0.34      0.44      1174\n",
      "           3       0.53      0.51      0.52      1778\n",
      "           4       0.54      0.44      0.49      1329\n",
      "           5       0.76      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2076 | val acc:\t0.5988\n",
      "\n",
      "Epoch 72/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0641 | train acc:\t0.5894\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.75      0.64      3943\n",
      "           1       0.66      0.47      0.55      1671\n",
      "           2       0.60      0.36      0.45      1174\n",
      "           3       0.54      0.51      0.52      1778\n",
      "           4       0.53      0.44      0.48      1329\n",
      "           5       0.76      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.1553 | val acc:\t0.6144\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.80      0.72      1518\n",
      "           1       0.55      0.40      0.46       468\n",
      "           2       0.63      0.37      0.47       321\n",
      "           3       0.48      0.48      0.48       397\n",
      "           4       0.44      0.40      0.42       309\n",
      "           5       0.89      0.67      0.76       252\n",
      "\n",
      "    accuracy                           0.61      3265\n",
      "   macro avg       0.61      0.52      0.55      3265\n",
      "weighted avg       0.61      0.61      0.60      3265\n",
      "\n",
      "Epoch 73/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0626 | train acc:\t0.5885\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.75      0.64      3943\n",
      "           1       0.65      0.48      0.55      1671\n",
      "           2       0.62      0.35      0.45      1174\n",
      "           3       0.53      0.51      0.52      1778\n",
      "           4       0.54      0.43      0.48      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.1745 | val acc:\t0.5951\n",
      "\n",
      "Epoch 74/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0518 | train acc:\t0.5959\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.76      0.65      3943\n",
      "           1       0.65      0.48      0.55      1671\n",
      "           2       0.64      0.37      0.47      1174\n",
      "           3       0.54      0.52      0.53      1778\n",
      "           4       0.56      0.44      0.49      1329\n",
      "           5       0.75      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.55      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.1754 | val acc:\t0.5963\n",
      "\n",
      "Epoch 75/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0538 | train acc:\t0.5951\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.75      0.65      3943\n",
      "           1       0.65      0.47      0.55      1671\n",
      "           2       0.63      0.37      0.47      1174\n",
      "           3       0.54      0.53      0.53      1778\n",
      "           4       0.55      0.45      0.49      1329\n",
      "           5       0.76      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.1770 | val acc:\t0.6043\n",
      "\n",
      "Epoch 76/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0482 | train acc:\t0.5973\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.76      0.65      3943\n",
      "           1       0.66      0.48      0.55      1671\n",
      "           2       0.66      0.38      0.48      1174\n",
      "           3       0.53      0.52      0.53      1778\n",
      "           4       0.56      0.44      0.49      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.1346 | val acc:\t0.6141\n",
      "\n",
      "Epoch 77/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0524 | train acc:\t0.5945\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.75      0.65      3943\n",
      "           1       0.66      0.48      0.56      1671\n",
      "           2       0.60      0.35      0.44      1174\n",
      "           3       0.54      0.53      0.53      1778\n",
      "           4       0.56      0.44      0.49      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.60      0.59      0.59     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1579 | val acc:\t0.6150\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.80      0.72      1518\n",
      "           1       0.59      0.33      0.42       468\n",
      "           2       0.76      0.29      0.42       321\n",
      "           3       0.45      0.54      0.49       397\n",
      "           4       0.42      0.43      0.42       309\n",
      "           5       0.79      0.81      0.80       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.61      0.53      0.55      3265\n",
      "weighted avg       0.62      0.62      0.60      3265\n",
      "\n",
      "Epoch 78/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0508 | train acc:\t0.5974\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.76      0.65      3943\n",
      "           1       0.65      0.47      0.55      1671\n",
      "           2       0.64      0.38      0.48      1174\n",
      "           3       0.54      0.52      0.53      1778\n",
      "           4       0.58      0.43      0.50      1329\n",
      "           5       0.75      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.1606 | val acc:\t0.6049\n",
      "\n",
      "Epoch 79/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0476 | train acc:\t0.5905\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      3943\n",
      "           1       0.64      0.49      0.55      1671\n",
      "           2       0.61      0.38      0.47      1174\n",
      "           3       0.52      0.51      0.51      1778\n",
      "           4       0.58      0.44      0.50      1329\n",
      "           5       0.75      0.76      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.1323 | val acc:\t0.6064\n",
      "\n",
      "Epoch 80/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0423 | train acc:\t0.5981\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      3943\n",
      "           1       0.66      0.48      0.55      1671\n",
      "           2       0.61      0.38      0.47      1174\n",
      "           3       0.54      0.53      0.54      1778\n",
      "           4       0.57      0.45      0.50      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2081 | val acc:\t0.5675\n",
      "\n",
      "Epoch 81/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0384 | train acc:\t0.6001\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      3943\n",
      "           1       0.65      0.49      0.56      1671\n",
      "           2       0.63      0.37      0.47      1174\n",
      "           3       0.55      0.53      0.54      1778\n",
      "           4       0.56      0.47      0.51      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.1860 | val acc:\t0.5972\n",
      "\n",
      "Epoch 82/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0292 | train acc:\t0.6041\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      3943\n",
      "           1       0.66      0.48      0.56      1671\n",
      "           2       0.65      0.40      0.49      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.57      0.46      0.51      1329\n",
      "           5       0.76      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.1354 | val acc:\t0.6129\n",
      "\n",
      "Epoch 83/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0348 | train acc:\t0.6012\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      3943\n",
      "           1       0.65      0.50      0.56      1671\n",
      "           2       0.63      0.40      0.49      1174\n",
      "           3       0.54      0.52      0.53      1778\n",
      "           4       0.57      0.45      0.51      1329\n",
      "           5       0.75      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.57      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.1365 | val acc:\t0.6172\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.81      0.72      1518\n",
      "           1       0.50      0.43      0.46       468\n",
      "           2       0.66      0.37      0.47       321\n",
      "           3       0.51      0.41      0.45       397\n",
      "           4       0.46      0.39      0.42       309\n",
      "           5       0.84      0.75      0.79       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.60      0.53      0.55      3265\n",
      "weighted avg       0.61      0.62      0.60      3265\n",
      "\n",
      "Epoch 84/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0342 | train acc:\t0.5988\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.64      0.51      0.57      1671\n",
      "           2       0.64      0.38      0.47      1174\n",
      "           3       0.55      0.53      0.54      1778\n",
      "           4       0.56      0.45      0.50      1329\n",
      "           5       0.76      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.1362 | val acc:\t0.6110\n",
      "\n",
      "Epoch 85/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0294 | train acc:\t0.6047\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      3943\n",
      "           1       0.65      0.50      0.57      1671\n",
      "           2       0.64      0.40      0.49      1174\n",
      "           3       0.55      0.53      0.54      1778\n",
      "           4       0.57      0.45      0.50      1329\n",
      "           5       0.77      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.1247 | val acc:\t0.6150\n",
      "\n",
      "Epoch 86/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0333 | train acc:\t0.6038\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.66      3943\n",
      "           1       0.66      0.49      0.56      1671\n",
      "           2       0.65      0.38      0.48      1174\n",
      "           3       0.54      0.53      0.54      1778\n",
      "           4       0.56      0.46      0.51      1329\n",
      "           5       0.76      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.1664 | val acc:\t0.6061\n",
      "\n",
      "Epoch 87/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0206 | train acc:\t0.6099\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.67      0.50      0.58      1671\n",
      "           2       0.62      0.40      0.48      1174\n",
      "           3       0.55      0.54      0.55      1778\n",
      "           4       0.59      0.47      0.52      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1429 | val acc:\t0.6190\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.80      0.72      1518\n",
      "           1       0.51      0.48      0.49       468\n",
      "           2       0.66      0.38      0.48       321\n",
      "           3       0.50      0.39      0.44       397\n",
      "           4       0.48      0.38      0.42       309\n",
      "           5       0.87      0.72      0.79       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.61      0.53      0.56      3265\n",
      "weighted avg       0.61      0.62      0.61      3265\n",
      "\n",
      "Epoch 88/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t1.0257 | train acc:\t0.6049\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65      3943\n",
      "           1       0.64      0.50      0.56      1671\n",
      "           2       0.64      0.39      0.48      1174\n",
      "           3       0.56      0.52      0.54      1778\n",
      "           4       0.56      0.47      0.51      1329\n",
      "           5       0.78      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.60      0.60     10896\n",
      "\n",
      "val loss:\t1.1443 | val acc:\t0.6181\n",
      "\n",
      "Epoch 89/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0174 | train acc:\t0.6067\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.68      0.52      0.59      1671\n",
      "           2       0.64      0.40      0.49      1174\n",
      "           3       0.55      0.54      0.54      1778\n",
      "           4       0.57      0.46      0.51      1329\n",
      "           5       0.77      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1516 | val acc:\t0.6129\n",
      "\n",
      "Epoch 90/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0151 | train acc:\t0.6058\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.66      0.51      0.57      1671\n",
      "           2       0.66      0.41      0.50      1174\n",
      "           3       0.55      0.54      0.54      1778\n",
      "           4       0.57      0.47      0.52      1329\n",
      "           5       0.76      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1617 | val acc:\t0.6126\n",
      "\n",
      "Epoch 91/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0079 | train acc:\t0.6086\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.76      0.66      3943\n",
      "           1       0.67      0.50      0.57      1671\n",
      "           2       0.61      0.38      0.47      1174\n",
      "           3       0.56      0.54      0.55      1778\n",
      "           4       0.57      0.47      0.52      1329\n",
      "           5       0.76      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1104 | val acc:\t0.6217\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.84      0.72      1518\n",
      "           1       0.56      0.37      0.44       468\n",
      "           2       0.69      0.38      0.49       321\n",
      "           3       0.53      0.43      0.48       397\n",
      "           4       0.49      0.38      0.43       309\n",
      "           5       0.87      0.71      0.78       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.63      0.52      0.56      3265\n",
      "weighted avg       0.62      0.62      0.60      3265\n",
      "\n",
      "Epoch 92/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0087 | train acc:\t0.6104\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.61      0.41      0.49      1174\n",
      "           3       0.55      0.55      0.55      1778\n",
      "           4       0.57      0.46      0.51      1329\n",
      "           5       0.76      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1753 | val acc:\t0.5930\n",
      "\n",
      "Epoch 93/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0095 | train acc:\t0.6092\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.66      3943\n",
      "           1       0.66      0.50      0.57      1671\n",
      "           2       0.65      0.41      0.50      1174\n",
      "           3       0.56      0.53      0.55      1778\n",
      "           4       0.56      0.47      0.51      1329\n",
      "           5       0.78      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1343 | val acc:\t0.6199\n",
      "\n",
      "Epoch 94/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0113 | train acc:\t0.6116\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.65      0.42      0.51      1174\n",
      "           3       0.57      0.54      0.55      1778\n",
      "           4       0.58      0.48      0.52      1329\n",
      "           5       0.77      0.76      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.1806 | val acc:\t0.5914\n",
      "\n",
      "Epoch 95/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0050 | train acc:\t0.6126\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.68      0.52      0.59      1671\n",
      "           2       0.63      0.40      0.49      1174\n",
      "           3       0.56      0.55      0.55      1778\n",
      "           4       0.57      0.46      0.51      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.1343 | val acc:\t0.6270\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.81      0.72      1518\n",
      "           1       0.58      0.33      0.42       468\n",
      "           2       0.64      0.39      0.48       321\n",
      "           3       0.51      0.50      0.51       397\n",
      "           4       0.48      0.44      0.46       309\n",
      "           5       0.84      0.79      0.81       252\n",
      "\n",
      "    accuracy                           0.63      3265\n",
      "   macro avg       0.62      0.54      0.57      3265\n",
      "weighted avg       0.62      0.63      0.61      3265\n",
      "\n",
      "Epoch 96/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0026 | train acc:\t0.6156\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.76      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.66      0.42      0.51      1174\n",
      "           3       0.57      0.53      0.55      1778\n",
      "           4       0.57      0.47      0.52      1329\n",
      "           5       0.79      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.58      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1461 | val acc:\t0.6077\n",
      "\n",
      "Epoch 97/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0061 | train acc:\t0.6113\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.65      0.40      0.50      1174\n",
      "           3       0.55      0.54      0.54      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.77      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.1744 | val acc:\t0.5856\n",
      "\n",
      "Epoch 98/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0048 | train acc:\t0.6149\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.68      0.51      0.58      1671\n",
      "           2       0.63      0.43      0.51      1174\n",
      "           3       0.56      0.56      0.56      1778\n",
      "           4       0.59      0.47      0.52      1329\n",
      "           5       0.77      0.77      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.64      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1265 | val acc:\t0.6233\n",
      "\n",
      "Epoch 99/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0041 | train acc:\t0.6146\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.65      0.43      0.52      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.57      0.47      0.51      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.64      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.1296 | val acc:\t0.6224\n",
      "\n",
      "Epoch 100/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9872 | train acc:\t0.6116\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.66      0.43      0.52      1174\n",
      "           3       0.55      0.54      0.55      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.1223 | val acc:\t0.6236\n",
      "\n",
      "Epoch 101/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9960 | train acc:\t0.6177\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.65      0.43      0.52      1174\n",
      "           3       0.57      0.54      0.56      1778\n",
      "           4       0.58      0.48      0.53      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1230 | val acc:\t0.6168\n",
      "\n",
      "Epoch 102/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9816 | train acc:\t0.6219\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.66      0.43      0.52      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.58      0.47      0.52      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1455 | val acc:\t0.5988\n",
      "\n",
      "Epoch 103/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9847 | train acc:\t0.6190\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.64      0.43      0.52      1174\n",
      "           3       0.58      0.55      0.56      1778\n",
      "           4       0.59      0.50      0.54      1329\n",
      "           5       0.78      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.1669 | val acc:\t0.5871\n",
      "\n",
      "Epoch 104/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9889 | train acc:\t0.6227\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.67      3943\n",
      "           1       0.67      0.52      0.59      1671\n",
      "           2       0.66      0.46      0.54      1174\n",
      "           3       0.56      0.55      0.56      1778\n",
      "           4       0.57      0.48      0.52      1329\n",
      "           5       0.77      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1743 | val acc:\t0.5902\n",
      "\n",
      "Epoch 105/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9766 | train acc:\t0.6257\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.67      0.52      0.59      1671\n",
      "           2       0.64      0.42      0.51      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.59      0.61     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1262 | val acc:\t0.6233\n",
      "\n",
      "Epoch 106/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9807 | train acc:\t0.6272\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.67      0.54      0.60      1671\n",
      "           2       0.66      0.45      0.53      1174\n",
      "           3       0.57      0.55      0.56      1778\n",
      "           4       0.59      0.50      0.54      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1399 | val acc:\t0.6110\n",
      "\n",
      "Epoch 107/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9733 | train acc:\t0.6280\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.67      3943\n",
      "           1       0.68      0.53      0.59      1671\n",
      "           2       0.64      0.44      0.53      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1093 | val acc:\t0.6214\n",
      "\n",
      "Epoch 108/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9771 | train acc:\t0.6255\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.66      3943\n",
      "           1       0.68      0.53      0.60      1671\n",
      "           2       0.66      0.44      0.53      1174\n",
      "           3       0.58      0.55      0.57      1778\n",
      "           4       0.59      0.50      0.54      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1215 | val acc:\t0.6141\n",
      "\n",
      "Epoch 109/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9745 | train acc:\t0.6265\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.66      0.44      0.53      1174\n",
      "           3       0.58      0.55      0.57      1778\n",
      "           4       0.60      0.49      0.54      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1473 | val acc:\t0.5982\n",
      "\n",
      "Epoch 110/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9715 | train acc:\t0.6293\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.65      0.45      0.53      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.78      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1067 | val acc:\t0.6165\n",
      "\n",
      "Epoch 111/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9743 | train acc:\t0.6268\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.65      0.43      0.52      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.59      0.50      0.54      1329\n",
      "           5       0.79      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1076 | val acc:\t0.6266\n",
      "\n",
      "Epoch 112/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9651 | train acc:\t0.6268\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.67      0.52      0.58      1671\n",
      "           2       0.65      0.46      0.54      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.58      0.49      0.53      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1316 | val acc:\t0.6129\n",
      "\n",
      "Epoch 113/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9662 | train acc:\t0.6288\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.69      0.54      0.60      1671\n",
      "           2       0.66      0.45      0.53      1174\n",
      "           3       0.58      0.57      0.58      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1252 | val acc:\t0.6141\n",
      "\n",
      "Epoch 114/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9619 | train acc:\t0.6312\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.67      3943\n",
      "           1       0.67      0.55      0.60      1671\n",
      "           2       0.64      0.44      0.52      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.60      0.50      0.54      1329\n",
      "           5       0.80      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1329 | val acc:\t0.6236\n",
      "\n",
      "Epoch 115/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9630 | train acc:\t0.6258\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.66      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.66      0.46      0.55      1174\n",
      "           3       0.58      0.55      0.57      1778\n",
      "           4       0.59      0.50      0.54      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1545 | val acc:\t0.6086\n",
      "\n",
      "Epoch 116/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9660 | train acc:\t0.6313\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.68      0.53      0.60      1671\n",
      "           2       0.66      0.45      0.53      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.61      0.51      0.56      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1213 | val acc:\t0.6187\n",
      "\n",
      "Epoch 117/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9603 | train acc:\t0.6326\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.68      3943\n",
      "           1       0.69      0.55      0.61      1671\n",
      "           2       0.64      0.46      0.53      1174\n",
      "           3       0.58      0.56      0.57      1778\n",
      "           4       0.59      0.50      0.54      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1438 | val acc:\t0.6144\n",
      "\n",
      "Epoch 118/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9533 | train acc:\t0.6337\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67      3943\n",
      "           1       0.69      0.54      0.60      1671\n",
      "           2       0.66      0.47      0.55      1174\n",
      "           3       0.59      0.56      0.57      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.66      0.61      0.62     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1283 | val acc:\t0.6098\n",
      "\n",
      "Epoch 119/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9477 | train acc:\t0.6432\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      3943\n",
      "           1       0.70      0.55      0.61      1671\n",
      "           2       0.66      0.48      0.56      1174\n",
      "           3       0.62      0.59      0.61      1778\n",
      "           4       0.61      0.51      0.55      1329\n",
      "           5       0.79      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.1473 | val acc:\t0.5957\n",
      "\n",
      "Epoch 120/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9519 | train acc:\t0.6340\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.69      0.54      0.60      1671\n",
      "           2       0.66      0.45      0.54      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.62      0.52      0.56      1329\n",
      "           5       0.80      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.1212 | val acc:\t0.6156\n",
      "\n",
      "Epoch 121/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9443 | train acc:\t0.6378\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      3943\n",
      "           1       0.69      0.55      0.61      1671\n",
      "           2       0.67      0.48      0.56      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.61      0.50      0.55      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.1450 | val acc:\t0.5923\n",
      "\n",
      "Epoch 122/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9499 | train acc:\t0.6417\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.69      0.55      0.61      1671\n",
      "           2       0.65      0.46      0.54      1174\n",
      "           3       0.61      0.58      0.59      1778\n",
      "           4       0.61      0.52      0.56      1329\n",
      "           5       0.79      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1007 | val acc:\t0.6279\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.83      0.72      1518\n",
      "           1       0.60      0.38      0.46       468\n",
      "           2       0.62      0.40      0.49       321\n",
      "           3       0.54      0.42      0.47       397\n",
      "           4       0.48      0.46      0.47       309\n",
      "           5       0.85      0.72      0.78       252\n",
      "\n",
      "    accuracy                           0.63      3265\n",
      "   macro avg       0.62      0.53      0.57      3265\n",
      "weighted avg       0.62      0.63      0.61      3265\n",
      "\n",
      "Epoch 123/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9453 | train acc:\t0.6432\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.69      0.54      0.61      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.60      0.59      0.59      1778\n",
      "           4       0.61      0.51      0.55      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.1305 | val acc:\t0.6074\n",
      "\n",
      "Epoch 124/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9453 | train acc:\t0.6398\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      3943\n",
      "           1       0.70      0.56      0.62      1671\n",
      "           2       0.66      0.46      0.55      1174\n",
      "           3       0.60      0.57      0.58      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.1449 | val acc:\t0.6025\n",
      "\n",
      "Epoch 125/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9358 | train acc:\t0.6409\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.67      0.48      0.56      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.1694 | val acc:\t0.5887\n",
      "\n",
      "Epoch 126/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9434 | train acc:\t0.6427\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.69      0.49      0.57      1174\n",
      "           3       0.59      0.56      0.58      1778\n",
      "           4       0.61      0.53      0.57      1329\n",
      "           5       0.78      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.1034 | val acc:\t0.6227\n",
      "\n",
      "Epoch 127/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9388 | train acc:\t0.6427\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.68      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.67      0.47      0.55      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.62      0.51      0.56      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.65      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.1418 | val acc:\t0.5960\n",
      "\n",
      "Epoch 128/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9352 | train acc:\t0.6412\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.69      0.55      0.61      1671\n",
      "           2       0.66      0.47      0.55      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.58      0.52      0.55      1329\n",
      "           5       0.80      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.1023 | val acc:\t0.6190\n",
      "\n",
      "Epoch 129/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9314 | train acc:\t0.6407\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.68      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.64      0.46      0.54      1174\n",
      "           3       0.61      0.58      0.59      1778\n",
      "           4       0.61      0.53      0.57      1329\n",
      "           5       0.79      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.1003 | val acc:\t0.6144\n",
      "\n",
      "Epoch 130/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9270 | train acc:\t0.6463\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.67      0.48      0.56      1174\n",
      "           3       0.60      0.57      0.59      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.80      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.1059 | val acc:\t0.6273\n",
      "\n",
      "Epoch 131/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9310 | train acc:\t0.6390\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      3943\n",
      "           1       0.69      0.55      0.61      1671\n",
      "           2       0.66      0.48      0.56      1174\n",
      "           3       0.59      0.57      0.58      1778\n",
      "           4       0.61      0.51      0.55      1329\n",
      "           5       0.80      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.0918 | val acc:\t0.6260\n",
      "\n",
      "Epoch 132/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9173 | train acc:\t0.6505\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.68      0.49      0.57      1174\n",
      "           3       0.61      0.58      0.60      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.80      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0898 | val acc:\t0.6101\n",
      "\n",
      "Epoch 133/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9218 | train acc:\t0.6486\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.69      3943\n",
      "           1       0.70      0.56      0.62      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.60      0.59      0.59      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.81      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0822 | val acc:\t0.6242\n",
      "\n",
      "Epoch 134/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.9187 | train acc:\t0.6523\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.77      0.69      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.67      0.49      0.56      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.63      0.53      0.58      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1081 | val acc:\t0.6028\n",
      "\n",
      "Epoch 135/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9206 | train acc:\t0.6481\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.67      0.50      0.57      1174\n",
      "           3       0.60      0.59      0.59      1778\n",
      "           4       0.63      0.53      0.57      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0871 | val acc:\t0.6159\n",
      "\n",
      "Epoch 136/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9221 | train acc:\t0.6456\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.67      0.50      0.57      1174\n",
      "           3       0.60      0.57      0.58      1778\n",
      "           4       0.61      0.51      0.56      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.1241 | val acc:\t0.6052\n",
      "\n",
      "Epoch 137/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9185 | train acc:\t0.6480\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.68      0.57      0.62      1671\n",
      "           2       0.65      0.49      0.56      1174\n",
      "           3       0.61      0.58      0.59      1778\n",
      "           4       0.62      0.52      0.57      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.1260 | val acc:\t0.6116\n",
      "\n",
      "Epoch 138/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9175 | train acc:\t0.6465\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.69      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.68      0.50      0.58      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0977 | val acc:\t0.6221\n",
      "\n",
      "Epoch 139/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9197 | train acc:\t0.6479\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.71      0.57      0.63      1671\n",
      "           2       0.66      0.49      0.57      1174\n",
      "           3       0.60      0.59      0.60      1778\n",
      "           4       0.60      0.53      0.56      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.0915 | val acc:\t0.6132\n",
      "\n",
      "Epoch 140/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9155 | train acc:\t0.6534\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.67      0.50      0.57      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.62      0.51      0.56      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0783 | val acc:\t0.6217\n",
      "\n",
      "Epoch 141/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9084 | train acc:\t0.6555\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.58      0.64      1671\n",
      "           2       0.67      0.49      0.57      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.64      0.54      0.59      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1057 | val acc:\t0.6074\n",
      "\n",
      "Epoch 142/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9056 | train acc:\t0.6537\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.64      0.55      0.59      1329\n",
      "           5       0.80      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.68      0.63      0.65     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.0840 | val acc:\t0.6288\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.80      0.72      1518\n",
      "           1       0.61      0.43      0.50       468\n",
      "           2       0.55      0.45      0.49       321\n",
      "           3       0.52      0.41      0.45       397\n",
      "           4       0.49      0.49      0.49       309\n",
      "           5       0.85      0.74      0.79       252\n",
      "\n",
      "    accuracy                           0.63      3265\n",
      "   macro avg       0.61      0.55      0.58      3265\n",
      "weighted avg       0.62      0.63      0.62      3265\n",
      "\n",
      "Epoch 143/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9036 | train acc:\t0.6592\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.71      0.58      0.64      1671\n",
      "           2       0.69      0.51      0.59      1174\n",
      "           3       0.63      0.60      0.61      1778\n",
      "           4       0.62      0.55      0.58      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1095 | val acc:\t0.6208\n",
      "\n",
      "Epoch 144/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8898 | train acc:\t0.6593\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.67      0.49      0.57      1174\n",
      "           3       0.64      0.61      0.62      1778\n",
      "           4       0.63      0.56      0.59      1329\n",
      "           5       0.80      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1104 | val acc:\t0.6119\n",
      "\n",
      "Epoch 145/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9143 | train acc:\t0.6489\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.69      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.68      0.50      0.58      1174\n",
      "           3       0.61      0.58      0.59      1778\n",
      "           4       0.62      0.52      0.57      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1174 | val acc:\t0.6040\n",
      "\n",
      "Epoch 146/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9014 | train acc:\t0.6592\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.69      0.57      0.63      1671\n",
      "           2       0.66      0.51      0.57      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.63      0.55      0.59      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.0971 | val acc:\t0.6132\n",
      "\n",
      "Epoch 147/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9034 | train acc:\t0.6556\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.67      0.52      0.58      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.61      0.53      0.57      1329\n",
      "           5       0.80      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1045 | val acc:\t0.6089\n",
      "\n",
      "Epoch 148/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9056 | train acc:\t0.6521\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.77      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.66      0.49      0.57      1174\n",
      "           3       0.62      0.59      0.61      1778\n",
      "           4       0.62      0.52      0.57      1329\n",
      "           5       0.81      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1074 | val acc:\t0.6046\n",
      "\n",
      "Epoch 149/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9080 | train acc:\t0.6552\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.70      0.58      0.64      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.61      0.53      0.57      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1089 | val acc:\t0.6147\n",
      "\n",
      "Epoch 150/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9040 | train acc:\t0.6554\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.75      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.67      0.52      0.58      1174\n",
      "           3       0.61      0.60      0.61      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.0899 | val acc:\t0.6181\n",
      "\n",
      "Epoch 151/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8928 | train acc:\t0.6579\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.69      0.56      0.62      1671\n",
      "           2       0.70      0.52      0.59      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1274 | val acc:\t0.5917\n",
      "\n",
      "Epoch 152/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8911 | train acc:\t0.6630\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.70      0.58      0.64      1671\n",
      "           2       0.69      0.53      0.60      1174\n",
      "           3       0.61      0.60      0.61      1778\n",
      "           4       0.63      0.54      0.59      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1073 | val acc:\t0.5994\n",
      "\n",
      "Epoch 153/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8862 | train acc:\t0.6659\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.72      0.60      0.65      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.64      0.55      0.59      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.0752 | val acc:\t0.6184\n",
      "\n",
      "Epoch 154/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8862 | train acc:\t0.6630\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.71      0.60      0.65      1671\n",
      "           2       0.69      0.52      0.59      1174\n",
      "           3       0.63      0.59      0.61      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.80      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.65     10896\n",
      "weighted avg       0.67      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.0680 | val acc:\t0.6202\n",
      "\n",
      "Epoch 155/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8793 | train acc:\t0.6696\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.68      0.51      0.58      1174\n",
      "           3       0.64      0.61      0.63      1778\n",
      "           4       0.64      0.57      0.60      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.0612 | val acc:\t0.6205\n",
      "\n",
      "Epoch 156/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8836 | train acc:\t0.6627\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.58      0.63      1671\n",
      "           2       0.70      0.52      0.60      1174\n",
      "           3       0.62      0.61      0.61      1778\n",
      "           4       0.63      0.56      0.59      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.0716 | val acc:\t0.6230\n",
      "\n",
      "Epoch 157/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8809 | train acc:\t0.6628\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.69      0.52      0.59      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.66      0.66     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1015 | val acc:\t0.6208\n",
      "\n",
      "Epoch 158/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8772 | train acc:\t0.6658\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.72      0.60      0.65      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.64      0.61      0.63      1778\n",
      "           4       0.63      0.55      0.58      1329\n",
      "           5       0.82      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.64      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.1290 | val acc:\t0.5933\n",
      "\n",
      "Epoch 159/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8811 | train acc:\t0.6650\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.71      0.61      0.66      1671\n",
      "           2       0.68      0.51      0.58      1174\n",
      "           3       0.63      0.59      0.61      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.1092 | val acc:\t0.6000\n",
      "\n",
      "Epoch 160/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8800 | train acc:\t0.6661\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.69      0.54      0.60      1174\n",
      "           3       0.63      0.62      0.63      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.1095 | val acc:\t0.6046\n",
      "\n",
      "Epoch 161/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8720 | train acc:\t0.6703\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.71      0.58      0.64      1671\n",
      "           2       0.70      0.54      0.61      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.63      0.55      0.59      1329\n",
      "           5       0.82      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.0712 | val acc:\t0.6193\n",
      "\n",
      "Epoch 162/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8684 | train acc:\t0.6691\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.69      0.53      0.60      1174\n",
      "           3       0.64      0.61      0.62      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.0896 | val acc:\t0.6129\n",
      "\n",
      "Epoch 163/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8747 | train acc:\t0.6692\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.71      0.60      0.65      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.82      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.0695 | val acc:\t0.6196\n",
      "\n",
      "Epoch 164/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8753 | train acc:\t0.6636\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.68      0.53      0.60      1174\n",
      "           3       0.62      0.59      0.61      1778\n",
      "           4       0.63      0.56      0.59      1329\n",
      "           5       0.81      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.0894 | val acc:\t0.6175\n",
      "\n",
      "Epoch 165/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8684 | train acc:\t0.6705\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.68      0.54      0.60      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.65      0.55      0.60      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.0718 | val acc:\t0.6251\n",
      "\n",
      "Epoch 166/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8671 | train acc:\t0.6713\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.71      0.61      0.66      1671\n",
      "           2       0.67      0.53      0.59      1174\n",
      "           3       0.64      0.61      0.63      1778\n",
      "           4       0.65      0.54      0.59      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.0791 | val acc:\t0.6101\n",
      "\n",
      "Epoch 167/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8582 | train acc:\t0.6776\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.71      0.60      0.65      1671\n",
      "           2       0.69      0.53      0.60      1174\n",
      "           3       0.64      0.63      0.64      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.0819 | val acc:\t0.6208\n",
      "\n",
      "Epoch 168/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8652 | train acc:\t0.6732\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.62      0.55      0.58      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.68      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1109 | val acc:\t0.5969\n",
      "\n",
      "Epoch 169/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8645 | train acc:\t0.6706\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.72      0.62      0.66      1671\n",
      "           2       0.70      0.53      0.60      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0713 | val acc:\t0.6181\n",
      "\n",
      "Epoch 170/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8579 | train acc:\t0.6736\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.71      0.56      0.63      1174\n",
      "           3       0.64      0.61      0.62      1778\n",
      "           4       0.63      0.55      0.59      1329\n",
      "           5       0.82      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.68      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.0713 | val acc:\t0.6251\n",
      "\n",
      "Epoch 171/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8523 | train acc:\t0.6772\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.71      0.60      0.65      1671\n",
      "           2       0.71      0.55      0.62      1174\n",
      "           3       0.65      0.62      0.64      1778\n",
      "           4       0.65      0.57      0.61      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.0891 | val acc:\t0.6077\n",
      "\n",
      "Epoch 172/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8497 | train acc:\t0.6799\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.78      0.71      3943\n",
      "           1       0.73      0.61      0.67      1671\n",
      "           2       0.69      0.54      0.60      1174\n",
      "           3       0.65      0.62      0.63      1778\n",
      "           4       0.66      0.58      0.62      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0881 | val acc:\t0.6141\n",
      "\n",
      "Epoch 173/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8562 | train acc:\t0.6722\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.71      0.59      0.65      1671\n",
      "           2       0.70      0.55      0.61      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.64      0.57      0.60      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.0789 | val acc:\t0.6150\n",
      "\n",
      "Epoch 174/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8546 | train acc:\t0.6692\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.70      0.60      0.64      1671\n",
      "           2       0.69      0.52      0.60      1174\n",
      "           3       0.63      0.62      0.62      1778\n",
      "           4       0.62      0.56      0.59      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.0991 | val acc:\t0.6061\n",
      "\n",
      "Epoch 175/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8581 | train acc:\t0.6769\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.71      3943\n",
      "           1       0.72      0.60      0.66      1671\n",
      "           2       0.69      0.55      0.61      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.64      0.54      0.59      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.1006 | val acc:\t0.6132\n",
      "\n",
      "Epoch 176/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8542 | train acc:\t0.6843\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71      3943\n",
      "           1       0.71      0.61      0.66      1671\n",
      "           2       0.73      0.57      0.64      1174\n",
      "           3       0.65      0.62      0.64      1778\n",
      "           4       0.64      0.58      0.61      1329\n",
      "           5       0.82      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.69      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0689 | val acc:\t0.6279\n",
      "\n",
      "Epoch 177/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8410 | train acc:\t0.6793\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.70      0.55      0.61      1174\n",
      "           3       0.65      0.65      0.65      1778\n",
      "           4       0.65      0.57      0.60      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0711 | val acc:\t0.6285\n",
      "\n",
      "Epoch 178/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8523 | train acc:\t0.6760\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.70      0.60      0.65      1671\n",
      "           2       0.71      0.54      0.61      1174\n",
      "           3       0.65      0.62      0.63      1778\n",
      "           4       0.64      0.57      0.60      1329\n",
      "           5       0.83      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.65      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.0921 | val acc:\t0.6129\n",
      "\n",
      "Epoch 179/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8406 | train acc:\t0.6810\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.64      0.63      0.64      1778\n",
      "           4       0.65      0.57      0.61      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0761 | val acc:\t0.6159\n",
      "\n",
      "Epoch 180/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8425 | train acc:\t0.6799\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.71      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.70      0.55      0.62      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.81      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0883 | val acc:\t0.6089\n",
      "\n",
      "Epoch 181/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8483 | train acc:\t0.6769\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.71      0.60      0.65      1671\n",
      "           2       0.70      0.55      0.62      1174\n",
      "           3       0.64      0.63      0.63      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.81      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0569 | val acc:\t0.6273\n",
      "\n",
      "Epoch 182/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8385 | train acc:\t0.6815\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.71      0.54      0.61      1174\n",
      "           3       0.64      0.64      0.64      1778\n",
      "           4       0.65      0.58      0.61      1329\n",
      "           5       0.81      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0764 | val acc:\t0.6175\n",
      "\n",
      "Epoch 183/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8416 | train acc:\t0.6816\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.70      0.55      0.61      1174\n",
      "           3       0.66      0.63      0.65      1778\n",
      "           4       0.64      0.57      0.60      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0733 | val acc:\t0.6227\n",
      "\n",
      "Epoch 184/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8414 | train acc:\t0.6780\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.71      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.65      0.62      0.64      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0716 | val acc:\t0.6196\n",
      "\n",
      "Epoch 185/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8363 | train acc:\t0.6867\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.66      0.58      0.62      1329\n",
      "           5       0.81      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.0811 | val acc:\t0.6260\n",
      "\n",
      "Epoch 186/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8398 | train acc:\t0.6819\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.73      0.63      0.67      1671\n",
      "           2       0.69      0.55      0.62      1174\n",
      "           3       0.66      0.63      0.64      1778\n",
      "           4       0.65      0.58      0.61      1329\n",
      "           5       0.81      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1020 | val acc:\t0.6184\n",
      "\n",
      "Epoch 187/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8399 | train acc:\t0.6789\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.78      0.71      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.69      0.53      0.60      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.65      0.56      0.60      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.65      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0818 | val acc:\t0.6144\n",
      "\n",
      "Epoch 188/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8361 | train acc:\t0.6847\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.71      0.63      0.67      1671\n",
      "           2       0.71      0.56      0.62      1174\n",
      "           3       0.64      0.61      0.63      1778\n",
      "           4       0.65      0.60      0.62      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0649 | val acc:\t0.6291\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      1518\n",
      "           1       0.59      0.45      0.51       468\n",
      "           2       0.59      0.46      0.52       321\n",
      "           3       0.46      0.46      0.46       397\n",
      "           4       0.49      0.54      0.51       309\n",
      "           5       0.88      0.73      0.80       252\n",
      "\n",
      "    accuracy                           0.63      3265\n",
      "   macro avg       0.61      0.57      0.59      3265\n",
      "weighted avg       0.63      0.63      0.62      3265\n",
      "\n",
      "Epoch 189/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8262 | train acc:\t0.6901\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.72      0.56      0.63      1174\n",
      "           3       0.66      0.63      0.65      1778\n",
      "           4       0.66      0.58      0.62      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.0923 | val acc:\t0.6080\n",
      "\n",
      "Epoch 190/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8420 | train acc:\t0.6780\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.71      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.68      0.54      0.60      1174\n",
      "           3       0.66      0.63      0.64      1778\n",
      "           4       0.63      0.56      0.60      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0602 | val acc:\t0.6199\n",
      "\n",
      "Epoch 191/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8264 | train acc:\t0.6891\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.72      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.71      0.55      0.62      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.67      0.59      0.63      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1173 | val acc:\t0.6018\n",
      "\n",
      "Epoch 192/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8325 | train acc:\t0.6890\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.66      0.59      0.63      1329\n",
      "           5       0.81      0.84      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.0774 | val acc:\t0.6159\n",
      "\n",
      "Epoch 193/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.8283 | train acc:\t0.6860\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.70      0.55      0.61      1174\n",
      "           3       0.66      0.62      0.64      1778\n",
      "           4       0.66      0.58      0.62      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.1125 | val acc:\t0.5936\n",
      "\n",
      "Epoch 194/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8339 | train acc:\t0.6858\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.66      0.63      0.65      1778\n",
      "           4       0.66      0.57      0.62      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.0801 | val acc:\t0.6205\n",
      "\n",
      "Epoch 195/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8256 | train acc:\t0.6862\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.73      0.63      0.68      1671\n",
      "           2       0.71      0.57      0.63      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.64      0.57      0.60      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.0802 | val acc:\t0.6061\n",
      "\n",
      "Epoch 196/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8156 | train acc:\t0.6920\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.73      0.63      0.68      1671\n",
      "           2       0.72      0.57      0.63      1174\n",
      "           3       0.66      0.63      0.65      1778\n",
      "           4       0.65      0.59      0.62      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.0922 | val acc:\t0.6015\n",
      "\n",
      "Epoch 197/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8308 | train acc:\t0.6874\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.71      0.62      0.67      1671\n",
      "           2       0.70      0.55      0.62      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.65      0.59      0.62      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.0545 | val acc:\t0.6309\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.79      0.72      1518\n",
      "           1       0.59      0.42      0.49       468\n",
      "           2       0.60      0.47      0.53       321\n",
      "           3       0.48      0.46      0.47       397\n",
      "           4       0.54      0.48      0.51       309\n",
      "           5       0.88      0.73      0.79       252\n",
      "\n",
      "    accuracy                           0.63      3265\n",
      "   macro avg       0.62      0.56      0.58      3265\n",
      "weighted avg       0.63      0.63      0.62      3265\n",
      "\n",
      "Epoch 198/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8195 | train acc:\t0.6843\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.73      0.63      0.67      1671\n",
      "           2       0.71      0.57      0.63      1174\n",
      "           3       0.66      0.63      0.64      1778\n",
      "           4       0.64      0.57      0.60      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.0766 | val acc:\t0.6205\n",
      "\n",
      "Epoch 199/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8123 | train acc:\t0.6874\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.66      0.63      0.65      1778\n",
      "           4       0.65      0.58      0.62      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.0468 | val acc:\t0.6263\n",
      "\n",
      "Epoch 200/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8211 | train acc:\t0.6892\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.72      0.57      0.64      1174\n",
      "           3       0.66      0.63      0.64      1778\n",
      "           4       0.65      0.59      0.62      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.0506 | val acc:\t0.6132\n",
      "\n",
      "Training complete in 30m 8s\n",
      "Best val acc: 0.630934\n",
      "\t conv1.weight\n",
      "\t conv1.bias\n",
      "\t conv2.weight\n",
      "\t conv2.bias\n",
      "\t conv3.weight\n",
      "\t conv3.bias\n",
      "\t linear1.weight\n",
      "\t linear1.bias\n",
      "\t linear2.weight\n",
      "\t linear2.bias\n",
      "Epoch 1/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.5592 | train acc:\t0.3837\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.89      0.54      3943\n",
      "           1       0.30      0.06      0.10      1671\n",
      "           2       0.00      0.00      0.00      1174\n",
      "           3       0.34      0.16      0.21      1778\n",
      "           4       0.16      0.01      0.01      1329\n",
      "           5       0.50      0.28      0.36      1001\n",
      "\n",
      "    accuracy                           0.38     10896\n",
      "   macro avg       0.28      0.23      0.20     10896\n",
      "weighted avg       0.31      0.38      0.28     10896\n",
      "\n",
      "val loss:\t1.5230 | val acc:\t0.5149\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.96      0.67      1518\n",
      "           1       0.53      0.10      0.17       468\n",
      "           2       0.00      0.00      0.00       321\n",
      "           3       0.37      0.20      0.26       397\n",
      "           4       0.00      0.00      0.00       309\n",
      "           5       0.72      0.41      0.52       252\n",
      "\n",
      "    accuracy                           0.51      3265\n",
      "   macro avg       0.35      0.28      0.27      3265\n",
      "weighted avg       0.42      0.51      0.41      3265\n",
      "\n",
      "Epoch 2/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.4143 | train acc:\t0.4403\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.79      0.58      3943\n",
      "           1       0.38      0.21      0.27      1671\n",
      "           2       0.42      0.01      0.02      1174\n",
      "           3       0.36      0.38      0.37      1778\n",
      "           4       0.41      0.05      0.09      1329\n",
      "           5       0.56      0.57      0.57      1001\n",
      "\n",
      "    accuracy                           0.44     10896\n",
      "   macro avg       0.43      0.34      0.31     10896\n",
      "weighted avg       0.43      0.44      0.37     10896\n",
      "\n",
      "val loss:\t1.5089 | val acc:\t0.5038\n",
      "\n",
      "Epoch 3/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.3364 | train acc:\t0.4811\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.76      0.59      3943\n",
      "           1       0.48      0.36      0.41      1671\n",
      "           2       0.51      0.07      0.12      1174\n",
      "           3       0.39      0.42      0.41      1778\n",
      "           4       0.41      0.14      0.20      1329\n",
      "           5       0.63      0.64      0.64      1001\n",
      "\n",
      "    accuracy                           0.48     10896\n",
      "   macro avg       0.48      0.40      0.39     10896\n",
      "weighted avg       0.48      0.48      0.44     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.4511 | val acc:\t0.5734\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.79      0.71      1518\n",
      "           1       0.47      0.35      0.40       468\n",
      "           2       0.55      0.19      0.29       321\n",
      "           3       0.38      0.51      0.43       397\n",
      "           4       0.45      0.19      0.26       309\n",
      "           5       0.65      0.75      0.69       252\n",
      "\n",
      "    accuracy                           0.57      3265\n",
      "   macro avg       0.52      0.46      0.46      3265\n",
      "weighted avg       0.56      0.57      0.55      3265\n",
      "\n",
      "Epoch 4/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2954 | train acc:\t0.4975\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.76      0.60      3943\n",
      "           1       0.50      0.40      0.45      1671\n",
      "           2       0.54      0.11      0.19      1174\n",
      "           3       0.42      0.43      0.43      1778\n",
      "           4       0.40      0.16      0.23      1329\n",
      "           5       0.64      0.66      0.65      1001\n",
      "\n",
      "    accuracy                           0.50     10896\n",
      "   macro avg       0.50      0.42      0.42     10896\n",
      "weighted avg       0.49      0.50      0.46     10896\n",
      "\n",
      "val loss:\t1.3630 | val acc:\t0.5786\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.91      0.71      1518\n",
      "           1       0.56      0.21      0.30       468\n",
      "           2       0.78      0.12      0.22       321\n",
      "           3       0.46      0.31      0.37       397\n",
      "           4       0.45      0.26      0.33       309\n",
      "           5       0.82      0.67      0.74       252\n",
      "\n",
      "    accuracy                           0.58      3265\n",
      "   macro avg       0.61      0.41      0.44      3265\n",
      "weighted avg       0.59      0.58      0.53      3265\n",
      "\n",
      "Epoch 5/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2799 | train acc:\t0.5015\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.74      0.60      3943\n",
      "           1       0.51      0.39      0.44      1671\n",
      "           2       0.47      0.14      0.21      1174\n",
      "           3       0.42      0.43      0.43      1778\n",
      "           4       0.42      0.23      0.30      1329\n",
      "           5       0.66      0.68      0.67      1001\n",
      "\n",
      "    accuracy                           0.50     10896\n",
      "   macro avg       0.50      0.43      0.44     10896\n",
      "weighted avg       0.49      0.50      0.48     10896\n",
      "\n",
      "val loss:\t1.4674 | val acc:\t0.5562\n",
      "\n",
      "Epoch 6/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2562 | train acc:\t0.5163\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.73      0.61      3943\n",
      "           1       0.55      0.41      0.47      1671\n",
      "           2       0.48      0.18      0.26      1174\n",
      "           3       0.44      0.46      0.45      1778\n",
      "           4       0.43      0.25      0.32      1329\n",
      "           5       0.66      0.69      0.68      1001\n",
      "\n",
      "    accuracy                           0.52     10896\n",
      "   macro avg       0.52      0.45      0.46     10896\n",
      "weighted avg       0.51      0.52      0.49     10896\n",
      "\n",
      "val loss:\t1.3843 | val acc:\t0.5939\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.84      0.72      1518\n",
      "           1       0.56      0.30      0.39       468\n",
      "           2       0.60      0.14      0.23       321\n",
      "           3       0.46      0.43      0.44       397\n",
      "           4       0.42      0.33      0.37       309\n",
      "           5       0.71      0.81      0.75       252\n",
      "\n",
      "    accuracy                           0.59      3265\n",
      "   macro avg       0.56      0.48      0.48      3265\n",
      "weighted avg       0.58      0.59      0.56      3265\n",
      "\n",
      "Epoch 7/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2339 | train acc:\t0.5192\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.73      0.61      3943\n",
      "           1       0.54      0.41      0.47      1671\n",
      "           2       0.52      0.20      0.29      1174\n",
      "           3       0.44      0.44      0.44      1778\n",
      "           4       0.47      0.29      0.36      1329\n",
      "           5       0.65      0.68      0.67      1001\n",
      "\n",
      "    accuracy                           0.52     10896\n",
      "   macro avg       0.52      0.46      0.47     10896\n",
      "weighted avg       0.52      0.52      0.50     10896\n",
      "\n",
      "val loss:\t1.3592 | val acc:\t0.6006\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.84      0.72      1518\n",
      "           1       0.62      0.31      0.42       468\n",
      "           2       0.45      0.34      0.38       321\n",
      "           3       0.42      0.52      0.46       397\n",
      "           4       0.61      0.17      0.26       309\n",
      "           5       0.81      0.70      0.75       252\n",
      "\n",
      "    accuracy                           0.60      3265\n",
      "   macro avg       0.59      0.48      0.50      3265\n",
      "weighted avg       0.60      0.60      0.57      3265\n",
      "\n",
      "Epoch 8/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2101 | train acc:\t0.5293\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.74      0.62      3943\n",
      "           1       0.56      0.42      0.48      1671\n",
      "           2       0.51      0.22      0.31      1174\n",
      "           3       0.45      0.46      0.45      1778\n",
      "           4       0.46      0.30      0.37      1329\n",
      "           5       0.69      0.69      0.69      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.53      0.47      0.49     10896\n",
      "weighted avg       0.53      0.53      0.51     10896\n",
      "\n",
      "val loss:\t1.3649 | val acc:\t0.5972\n",
      "\n",
      "Epoch 9/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.2093 | train acc:\t0.5309\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.61      3943\n",
      "           1       0.57      0.41      0.48      1671\n",
      "           2       0.53      0.22      0.31      1174\n",
      "           3       0.46      0.48      0.47      1778\n",
      "           4       0.46      0.31      0.37      1329\n",
      "           5       0.70      0.70      0.70      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.54      0.48      0.49     10896\n",
      "weighted avg       0.53      0.53      0.51     10896\n",
      "\n",
      "val loss:\t1.3276 | val acc:\t0.5816\n",
      "\n",
      "Epoch 10/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1928 | train acc:\t0.5345\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.62      3943\n",
      "           1       0.56      0.43      0.49      1671\n",
      "           2       0.54      0.23      0.32      1174\n",
      "           3       0.46      0.46      0.46      1778\n",
      "           4       0.46      0.32      0.38      1329\n",
      "           5       0.69      0.70      0.70      1001\n",
      "\n",
      "    accuracy                           0.53     10896\n",
      "   macro avg       0.54      0.48      0.49     10896\n",
      "weighted avg       0.53      0.53      0.52     10896\n",
      "\n",
      "val loss:\t1.4138 | val acc:\t0.5838\n",
      "\n",
      "Epoch 11/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1746 | train acc:\t0.5499\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.60      0.45      0.51      1671\n",
      "           2       0.56      0.27      0.36      1174\n",
      "           3       0.48      0.49      0.49      1778\n",
      "           4       0.48      0.34      0.40      1329\n",
      "           5       0.70      0.72      0.71      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.56      0.50      0.52     10896\n",
      "weighted avg       0.55      0.55      0.54     10896\n",
      "\n",
      "val loss:\t1.3326 | val acc:\t0.5881\n",
      "\n",
      "Epoch 12/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1656 | train acc:\t0.5476\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63      3943\n",
      "           1       0.59      0.43      0.50      1671\n",
      "           2       0.55      0.27      0.36      1174\n",
      "           3       0.47      0.49      0.48      1778\n",
      "           4       0.49      0.34      0.40      1329\n",
      "           5       0.69      0.71      0.70      1001\n",
      "\n",
      "    accuracy                           0.55     10896\n",
      "   macro avg       0.56      0.50      0.51     10896\n",
      "weighted avg       0.55      0.55      0.53     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.3778 | val acc:\t0.5629\n",
      "\n",
      "Epoch 13/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1635 | train acc:\t0.5449\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.73      0.62      3943\n",
      "           1       0.59      0.44      0.51      1671\n",
      "           2       0.55      0.26      0.36      1174\n",
      "           3       0.48      0.47      0.47      1778\n",
      "           4       0.48      0.36      0.41      1329\n",
      "           5       0.69      0.71      0.70      1001\n",
      "\n",
      "    accuracy                           0.54     10896\n",
      "   macro avg       0.55      0.50      0.51     10896\n",
      "weighted avg       0.55      0.54      0.53     10896\n",
      "\n",
      "val loss:\t1.3549 | val acc:\t0.6003\n",
      "\n",
      "Epoch 14/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1489 | train acc:\t0.5572\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.61      0.45      0.52      1671\n",
      "           2       0.55      0.28      0.37      1174\n",
      "           3       0.48      0.49      0.49      1778\n",
      "           4       0.49      0.35      0.41      1329\n",
      "           5       0.71      0.74      0.73      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.57      0.51      0.52     10896\n",
      "weighted avg       0.56      0.56      0.54     10896\n",
      "\n",
      "val loss:\t1.3096 | val acc:\t0.6123\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.79      0.73      1518\n",
      "           1       0.53      0.50      0.51       468\n",
      "           2       0.57      0.26      0.35       321\n",
      "           3       0.47      0.43      0.45       397\n",
      "           4       0.41      0.38      0.40       309\n",
      "           5       0.77      0.78      0.78       252\n",
      "\n",
      "    accuracy                           0.61      3265\n",
      "   macro avg       0.57      0.52      0.54      3265\n",
      "weighted avg       0.60      0.61      0.60      3265\n",
      "\n",
      "Epoch 15/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1342 | train acc:\t0.5618\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63      3943\n",
      "           1       0.62      0.44      0.52      1671\n",
      "           2       0.56      0.31      0.40      1174\n",
      "           3       0.49      0.49      0.49      1778\n",
      "           4       0.50      0.37      0.42      1329\n",
      "           5       0.73      0.74      0.74      1001\n",
      "\n",
      "    accuracy                           0.56     10896\n",
      "   macro avg       0.58      0.52      0.53     10896\n",
      "weighted avg       0.56      0.56      0.55     10896\n",
      "\n",
      "val loss:\t1.3295 | val acc:\t0.5982\n",
      "\n",
      "Epoch 16/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1277 | train acc:\t0.5673\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.63      3943\n",
      "           1       0.63      0.46      0.53      1671\n",
      "           2       0.56      0.32      0.41      1174\n",
      "           3       0.51      0.52      0.51      1778\n",
      "           4       0.51      0.39      0.44      1329\n",
      "           5       0.71      0.73      0.72      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.58      0.52      0.54     10896\n",
      "weighted avg       0.57      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.3496 | val acc:\t0.5939\n",
      "\n",
      "Epoch 17/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1176 | train acc:\t0.5703\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64      3943\n",
      "           1       0.60      0.45      0.52      1671\n",
      "           2       0.59      0.30      0.40      1174\n",
      "           3       0.50      0.49      0.50      1778\n",
      "           4       0.52      0.39      0.45      1329\n",
      "           5       0.75      0.75      0.75      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.52      0.54     10896\n",
      "weighted avg       0.57      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.2856 | val acc:\t0.6162\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.84      0.73      1518\n",
      "           1       0.65      0.32      0.43       468\n",
      "           2       0.48      0.38      0.43       321\n",
      "           3       0.50      0.48      0.49       397\n",
      "           4       0.49      0.26      0.34       309\n",
      "           5       0.82      0.75      0.79       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.60      0.51      0.53      3265\n",
      "weighted avg       0.61      0.62      0.59      3265\n",
      "\n",
      "Epoch 18/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1183 | train acc:\t0.5692\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.61      0.45      0.52      1671\n",
      "           2       0.56      0.31      0.40      1174\n",
      "           3       0.51      0.51      0.51      1778\n",
      "           4       0.51      0.39      0.44      1329\n",
      "           5       0.71      0.73      0.72      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.58      0.52      0.54     10896\n",
      "weighted avg       0.57      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.2633 | val acc:\t0.6025\n",
      "\n",
      "Epoch 19/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.1025 | train acc:\t0.5743\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.64      3943\n",
      "           1       0.63      0.47      0.54      1671\n",
      "           2       0.57      0.32      0.41      1174\n",
      "           3       0.51      0.51      0.51      1778\n",
      "           4       0.52      0.39      0.45      1329\n",
      "           5       0.73      0.76      0.74      1001\n",
      "\n",
      "    accuracy                           0.57     10896\n",
      "   macro avg       0.59      0.53      0.55     10896\n",
      "weighted avg       0.58      0.57      0.56     10896\n",
      "\n",
      "val loss:\t1.2961 | val acc:\t0.5966\n",
      "\n",
      "Epoch 20/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0966 | train acc:\t0.5775\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      3943\n",
      "           1       0.62      0.47      0.54      1671\n",
      "           2       0.60      0.34      0.43      1174\n",
      "           3       0.52      0.51      0.52      1778\n",
      "           4       0.51      0.40      0.45      1329\n",
      "           5       0.73      0.75      0.74      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.59      0.54      0.55     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2503 | val acc:\t0.6141\n",
      "\n",
      "Epoch 21/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0889 | train acc:\t0.5785\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.73      0.64      3943\n",
      "           1       0.62      0.47      0.54      1671\n",
      "           2       0.58      0.32      0.42      1174\n",
      "           3       0.52      0.52      0.52      1778\n",
      "           4       0.52      0.42      0.46      1329\n",
      "           5       0.74      0.77      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.59      0.54      0.55     10896\n",
      "weighted avg       0.58      0.58      0.57     10896\n",
      "\n",
      "val loss:\t1.2670 | val acc:\t0.6141\n",
      "\n",
      "Epoch 22/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0775 | train acc:\t0.5838\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      3943\n",
      "           1       0.64      0.47      0.54      1671\n",
      "           2       0.59      0.35      0.44      1174\n",
      "           3       0.52      0.53      0.53      1778\n",
      "           4       0.53      0.42      0.47      1329\n",
      "           5       0.74      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.59      0.58      0.58     10896\n",
      "\n",
      "val loss:\t1.2386 | val acc:\t0.6165\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.91      0.72      1518\n",
      "           1       0.69      0.28      0.40       468\n",
      "           2       0.69      0.26      0.38       321\n",
      "           3       0.52      0.40      0.45       397\n",
      "           4       0.55      0.26      0.35       309\n",
      "           5       0.89      0.69      0.78       252\n",
      "\n",
      "    accuracy                           0.62      3265\n",
      "   macro avg       0.66      0.47      0.51      3265\n",
      "weighted avg       0.63      0.62      0.58      3265\n",
      "\n",
      "Epoch 23/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t1.0740 | train acc:\t0.5852\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      3943\n",
      "           1       0.63      0.47      0.54      1671\n",
      "           2       0.60      0.33      0.43      1174\n",
      "           3       0.54      0.53      0.53      1778\n",
      "           4       0.53      0.41      0.46      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.60      0.54      0.56     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2945 | val acc:\t0.6159\n",
      "\n",
      "Epoch 24/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0641 | train acc:\t0.5850\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.61      0.47      0.53      1671\n",
      "           2       0.61      0.36      0.45      1174\n",
      "           3       0.51      0.52      0.52      1778\n",
      "           4       0.52      0.42      0.46      1329\n",
      "           5       0.75      0.76      0.75      1001\n",
      "\n",
      "    accuracy                           0.58     10896\n",
      "   macro avg       0.60      0.55      0.56     10896\n",
      "weighted avg       0.59      0.58      0.58     10896\n",
      "\n",
      "val loss:\t1.2397 | val acc:\t0.6398\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.87      0.74      1518\n",
      "           1       0.70      0.34      0.46       468\n",
      "           2       0.72      0.31      0.44       321\n",
      "           3       0.54      0.49      0.52       397\n",
      "           4       0.52      0.34      0.41       309\n",
      "           5       0.75      0.80      0.78       252\n",
      "\n",
      "    accuracy                           0.64      3265\n",
      "   macro avg       0.65      0.53      0.56      3265\n",
      "weighted avg       0.64      0.64      0.61      3265\n",
      "\n",
      "Epoch 25/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0638 | train acc:\t0.5898\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      3943\n",
      "           1       0.64      0.48      0.55      1671\n",
      "           2       0.60      0.38      0.46      1174\n",
      "           3       0.53      0.52      0.53      1778\n",
      "           4       0.54      0.43      0.48      1329\n",
      "           5       0.75      0.78      0.76      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.61      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2855 | val acc:\t0.6251\n",
      "\n",
      "Epoch 26/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0558 | train acc:\t0.5910\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.74      0.65      3943\n",
      "           1       0.64      0.49      0.55      1671\n",
      "           2       0.60      0.37      0.46      1174\n",
      "           3       0.53      0.52      0.53      1778\n",
      "           4       0.53      0.43      0.48      1329\n",
      "           5       0.73      0.78      0.75      1001\n",
      "\n",
      "    accuracy                           0.59     10896\n",
      "   macro avg       0.60      0.55      0.57     10896\n",
      "weighted avg       0.59      0.59      0.58     10896\n",
      "\n",
      "val loss:\t1.2925 | val acc:\t0.6196\n",
      "\n",
      "Epoch 27/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0410 | train acc:\t0.6005\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.64      0.49      0.55      1671\n",
      "           2       0.62      0.39      0.48      1174\n",
      "           3       0.55      0.54      0.55      1778\n",
      "           4       0.55      0.44      0.49      1329\n",
      "           5       0.74      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2570 | val acc:\t0.6395\n",
      "\n",
      "Epoch 28/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0374 | train acc:\t0.6009\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.64      0.49      0.56      1671\n",
      "           2       0.63      0.40      0.49      1174\n",
      "           3       0.53      0.52      0.53      1778\n",
      "           4       0.56      0.44      0.50      1329\n",
      "           5       0.75      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.62      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2926 | val acc:\t0.6141\n",
      "\n",
      "Epoch 29/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0296 | train acc:\t0.6007\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.64      0.49      0.56      1671\n",
      "           2       0.61      0.40      0.48      1174\n",
      "           3       0.54      0.54      0.54      1778\n",
      "           4       0.55      0.45      0.50      1329\n",
      "           5       0.75      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.61      0.57      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2294 | val acc:\t0.6343\n",
      "\n",
      "Epoch 30/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0341 | train acc:\t0.5984\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66      3943\n",
      "           1       0.62      0.48      0.54      1671\n",
      "           2       0.62      0.41      0.49      1174\n",
      "           3       0.53      0.52      0.53      1778\n",
      "           4       0.55      0.44      0.49      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.60     10896\n",
      "   macro avg       0.61      0.56      0.58     10896\n",
      "weighted avg       0.60      0.60      0.59     10896\n",
      "\n",
      "val loss:\t1.2719 | val acc:\t0.6233\n",
      "\n",
      "Epoch 31/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0167 | train acc:\t0.6086\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66      3943\n",
      "           1       0.66      0.50      0.57      1671\n",
      "           2       0.64      0.41      0.50      1174\n",
      "           3       0.55      0.55      0.55      1778\n",
      "           4       0.57      0.47      0.52      1329\n",
      "           5       0.74      0.77      0.76      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.57      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2592 | val acc:\t0.6184\n",
      "\n",
      "Epoch 32/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0171 | train acc:\t0.6134\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.64      0.42      0.51      1174\n",
      "           3       0.55      0.54      0.55      1778\n",
      "           4       0.57      0.46      0.51      1329\n",
      "           5       0.75      0.79      0.77      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.61      0.61     10896\n",
      "\n",
      "val loss:\t1.2401 | val acc:\t0.6368\n",
      "\n",
      "Epoch 33/200\n",
      "------------------------------------------------------\n",
      "train loss:\t1.0126 | train acc:\t0.6073\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.73      0.66      3943\n",
      "           1       0.65      0.50      0.57      1671\n",
      "           2       0.61      0.43      0.51      1174\n",
      "           3       0.54      0.54      0.54      1778\n",
      "           4       0.56      0.46      0.51      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.62      0.58      0.59     10896\n",
      "weighted avg       0.61      0.61      0.60     10896\n",
      "\n",
      "val loss:\t1.2594 | val acc:\t0.6199\n",
      "\n",
      "Epoch 34/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9999 | train acc:\t0.6125\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.74      0.66      3943\n",
      "           1       0.66      0.51      0.58      1671\n",
      "           2       0.61      0.41      0.49      1174\n",
      "           3       0.56      0.55      0.55      1778\n",
      "           4       0.56      0.48      0.52      1329\n",
      "           5       0.77      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.61     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.61      0.61      0.61     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.2572 | val acc:\t0.6199\n",
      "\n",
      "Epoch 35/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9970 | train acc:\t0.6159\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67      3943\n",
      "           1       0.65      0.51      0.57      1671\n",
      "           2       0.62      0.43      0.51      1174\n",
      "           3       0.57      0.56      0.56      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.76      0.78      0.77      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.63      0.58      0.60     10896\n",
      "weighted avg       0.62      0.62      0.61     10896\n",
      "\n",
      "val loss:\t1.2227 | val acc:\t0.6346\n",
      "\n",
      "Epoch 36/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9844 | train acc:\t0.6222\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.63      0.44      0.52      1174\n",
      "           3       0.57      0.56      0.57      1778\n",
      "           4       0.57      0.48      0.52      1329\n",
      "           5       0.77      0.80      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.2078 | val acc:\t0.6371\n",
      "\n",
      "Epoch 37/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9939 | train acc:\t0.6229\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.67      3943\n",
      "           1       0.67      0.51      0.58      1671\n",
      "           2       0.65      0.45      0.53      1174\n",
      "           3       0.56      0.55      0.55      1778\n",
      "           4       0.59      0.49      0.53      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.64      0.59      0.61     10896\n",
      "weighted avg       0.63      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1980 | val acc:\t0.6346\n",
      "\n",
      "Epoch 38/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9750 | train acc:\t0.6283\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.68      3943\n",
      "           1       0.67      0.54      0.60      1671\n",
      "           2       0.63      0.45      0.52      1174\n",
      "           3       0.57      0.58      0.57      1778\n",
      "           4       0.59      0.48      0.53      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.2157 | val acc:\t0.6371\n",
      "\n",
      "Epoch 39/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9792 | train acc:\t0.6295\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.64      0.44      0.53      1174\n",
      "           3       0.57      0.56      0.57      1778\n",
      "           4       0.59      0.50      0.54      1329\n",
      "           5       0.77      0.79      0.78      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.64      0.60      0.61     10896\n",
      "weighted avg       0.63      0.63      0.62     10896\n",
      "\n",
      "val loss:\t1.1820 | val acc:\t0.6334\n",
      "\n",
      "Epoch 40/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9785 | train acc:\t0.6233\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      3943\n",
      "           1       0.66      0.52      0.58      1671\n",
      "           2       0.61      0.44      0.51      1174\n",
      "           3       0.58      0.57      0.57      1778\n",
      "           4       0.58      0.48      0.53      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.62     10896\n",
      "   macro avg       0.63      0.59      0.61     10896\n",
      "weighted avg       0.62      0.62      0.62     10896\n",
      "\n",
      "val loss:\t1.1990 | val acc:\t0.6404\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.87      0.74      1518\n",
      "           1       0.67      0.38      0.49       468\n",
      "           2       0.66      0.34      0.45       321\n",
      "           3       0.52      0.48      0.50       397\n",
      "           4       0.57      0.33      0.42       309\n",
      "           5       0.86      0.74      0.80       252\n",
      "\n",
      "    accuracy                           0.64      3265\n",
      "   macro avg       0.65      0.53      0.57      3265\n",
      "weighted avg       0.64      0.64      0.62      3265\n",
      "\n",
      "Epoch 41/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9675 | train acc:\t0.6353\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.66      0.47      0.55      1174\n",
      "           3       0.58      0.57      0.58      1778\n",
      "           4       0.62      0.49      0.55      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.2039 | val acc:\t0.6221\n",
      "\n",
      "Epoch 42/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9595 | train acc:\t0.6354\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.65      0.45      0.53      1174\n",
      "           3       0.58      0.58      0.58      1778\n",
      "           4       0.59      0.50      0.54      1329\n",
      "           5       0.78      0.79      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.60      0.62     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.1945 | val acc:\t0.6398\n",
      "\n",
      "Epoch 43/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9517 | train acc:\t0.6386\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.77      0.69      3943\n",
      "           1       0.67      0.53      0.59      1671\n",
      "           2       0.64      0.47      0.55      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.60      0.51      0.55      1329\n",
      "           5       0.77      0.78      0.78      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.2028 | val acc:\t0.6404\n",
      "\n",
      "Epoch 44/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9502 | train acc:\t0.6399\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.77      0.69      3943\n",
      "           1       0.67      0.55      0.61      1671\n",
      "           2       0.65      0.45      0.53      1174\n",
      "           3       0.61      0.57      0.59      1778\n",
      "           4       0.60      0.50      0.55      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.65      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.63     10896\n",
      "\n",
      "val loss:\t1.2268 | val acc:\t0.6319\n",
      "\n",
      "Epoch 45/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9507 | train acc:\t0.6334\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68      3943\n",
      "           1       0.66      0.53      0.59      1671\n",
      "           2       0.65      0.47      0.55      1174\n",
      "           3       0.58      0.58      0.58      1778\n",
      "           4       0.58      0.49      0.53      1329\n",
      "           5       0.77      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.63     10896\n",
      "   macro avg       0.65      0.61      0.62     10896\n",
      "weighted avg       0.63      0.63      0.63     10896\n",
      "\n",
      "val loss:\t1.2179 | val acc:\t0.6380\n",
      "\n",
      "Epoch 46/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.9441 | train acc:\t0.6419\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.65      0.46      0.54      1174\n",
      "           3       0.60      0.58      0.59      1778\n",
      "           4       0.59      0.50      0.54      1329\n",
      "           5       0.78      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.61      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.2133 | val acc:\t0.6288\n",
      "\n",
      "Epoch 47/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9366 | train acc:\t0.6452\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.68      0.56      0.61      1671\n",
      "           2       0.66      0.48      0.55      1174\n",
      "           3       0.60      0.57      0.59      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.78      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.2444 | val acc:\t0.6242\n",
      "\n",
      "Epoch 48/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9428 | train acc:\t0.6417\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69      3943\n",
      "           1       0.68      0.53      0.60      1671\n",
      "           2       0.66      0.49      0.56      1174\n",
      "           3       0.58      0.57      0.58      1778\n",
      "           4       0.61      0.53      0.56      1329\n",
      "           5       0.78      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.64     10896\n",
      "   macro avg       0.66      0.62      0.63     10896\n",
      "weighted avg       0.64      0.64      0.64     10896\n",
      "\n",
      "val loss:\t1.2638 | val acc:\t0.5997\n",
      "\n",
      "Epoch 49/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9324 | train acc:\t0.6484\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.77      0.69      3943\n",
      "           1       0.68      0.55      0.61      1671\n",
      "           2       0.67      0.49      0.57      1174\n",
      "           3       0.60      0.59      0.60      1778\n",
      "           4       0.61      0.51      0.56      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.64     10896\n",
      "\n",
      "val loss:\t1.2053 | val acc:\t0.6233\n",
      "\n",
      "Epoch 50/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9208 | train acc:\t0.6508\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.68      0.54      0.60      1671\n",
      "           2       0.69      0.50      0.58      1174\n",
      "           3       0.61      0.60      0.61      1778\n",
      "           4       0.59      0.52      0.55      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.62      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.2511 | val acc:\t0.6306\n",
      "\n",
      "Epoch 51/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9205 | train acc:\t0.6530\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.68      0.55      0.60      1671\n",
      "           2       0.67      0.51      0.58      1174\n",
      "           3       0.61      0.59      0.60      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1922 | val acc:\t0.6340\n",
      "\n",
      "Epoch 52/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9104 | train acc:\t0.6530\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.77      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.67      0.49      0.56      1174\n",
      "           3       0.61      0.58      0.59      1778\n",
      "           4       0.62      0.53      0.57      1329\n",
      "           5       0.79      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1856 | val acc:\t0.6187\n",
      "\n",
      "Epoch 53/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9034 | train acc:\t0.6548\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.70      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.60      0.60      0.60      1778\n",
      "           4       0.60      0.52      0.56      1329\n",
      "           5       0.78      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.66      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.2026 | val acc:\t0.6288\n",
      "\n",
      "Epoch 54/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9065 | train acc:\t0.6526\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.55      0.62      1671\n",
      "           2       0.67      0.53      0.59      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.60      0.53      0.56      1329\n",
      "           5       0.79      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.67      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.2143 | val acc:\t0.6199\n",
      "\n",
      "Epoch 55/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9034 | train acc:\t0.6516\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.68      0.56      0.62      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.62      0.59      0.60      1778\n",
      "           4       0.61      0.54      0.57      1329\n",
      "           5       0.77      0.80      0.79      1001\n",
      "\n",
      "    accuracy                           0.65     10896\n",
      "   macro avg       0.66      0.63      0.64     10896\n",
      "weighted avg       0.65      0.65      0.65     10896\n",
      "\n",
      "val loss:\t1.1757 | val acc:\t0.6315\n",
      "\n",
      "Epoch 56/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8991 | train acc:\t0.6571\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.61      0.51      0.56      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.2150 | val acc:\t0.6401\n",
      "\n",
      "Epoch 57/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.9002 | train acc:\t0.6579\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.66      0.50      0.57      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.61      0.53      0.57      1329\n",
      "           5       0.79      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.2288 | val acc:\t0.6196\n",
      "\n",
      "Epoch 58/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.8978 | train acc:\t0.6578\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.76      0.69      3943\n",
      "           1       0.70      0.57      0.63      1671\n",
      "           2       0.67      0.52      0.59      1174\n",
      "           3       0.62      0.60      0.61      1778\n",
      "           4       0.61      0.54      0.57      1329\n",
      "           5       0.80      0.80      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.2138 | val acc:\t0.6116\n",
      "\n",
      "Epoch 59/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8915 | train acc:\t0.6587\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.68      0.58      0.62      1671\n",
      "           2       0.69      0.53      0.60      1174\n",
      "           3       0.61      0.60      0.60      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.80      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.2409 | val acc:\t0.6334\n",
      "\n",
      "Epoch 60/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8895 | train acc:\t0.6590\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.78      0.70      3943\n",
      "           1       0.68      0.56      0.62      1671\n",
      "           2       0.68      0.53      0.60      1174\n",
      "           3       0.63      0.58      0.60      1778\n",
      "           4       0.63      0.53      0.58      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.63      0.65     10896\n",
      "weighted avg       0.66      0.66      0.65     10896\n",
      "\n",
      "val loss:\t1.1506 | val acc:\t0.6312\n",
      "\n",
      "Epoch 61/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8899 | train acc:\t0.6615\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.71      3943\n",
      "           1       0.69      0.57      0.63      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.61      0.61      0.61      1778\n",
      "           4       0.62      0.52      0.57      1329\n",
      "           5       0.78      0.82      0.80      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.67      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.2116 | val acc:\t0.6300\n",
      "\n",
      "Epoch 62/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8819 | train acc:\t0.6634\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.69      0.54      0.61      1174\n",
      "           3       0.62      0.62      0.62      1778\n",
      "           4       0.62      0.54      0.58      1329\n",
      "           5       0.78      0.81      0.79      1001\n",
      "\n",
      "    accuracy                           0.66     10896\n",
      "   macro avg       0.68      0.64      0.65     10896\n",
      "weighted avg       0.66      0.66      0.66     10896\n",
      "\n",
      "val loss:\t1.1563 | val acc:\t0.6322\n",
      "\n",
      "Epoch 63/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8824 | train acc:\t0.6664\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.69      0.58      0.63      1671\n",
      "           2       0.68      0.53      0.60      1174\n",
      "           3       0.64      0.61      0.62      1778\n",
      "           4       0.64      0.55      0.59      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.2168 | val acc:\t0.6285\n",
      "\n",
      "Epoch 64/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8797 | train acc:\t0.6660\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70      3943\n",
      "           1       0.70      0.58      0.63      1671\n",
      "           2       0.70      0.54      0.61      1174\n",
      "           3       0.63      0.61      0.62      1778\n",
      "           4       0.63      0.54      0.58      1329\n",
      "           5       0.80      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.64      0.66     10896\n",
      "weighted avg       0.67      0.67      0.66     10896\n",
      "\n",
      "val loss:\t1.2040 | val acc:\t0.6150\n",
      "\n",
      "Epoch 65/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8761 | train acc:\t0.6687\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      3943\n",
      "           1       0.71      0.59      0.65      1671\n",
      "           2       0.68      0.54      0.60      1174\n",
      "           3       0.62      0.61      0.62      1778\n",
      "           4       0.63      0.56      0.59      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.68      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1591 | val acc:\t0.6282\n",
      "\n",
      "Epoch 66/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8625 | train acc:\t0.6719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70      3943\n",
      "           1       0.71      0.60      0.65      1671\n",
      "           2       0.70      0.55      0.61      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.62      0.56      0.59      1329\n",
      "           5       0.79      0.81      0.80      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1255 | val acc:\t0.6297\n",
      "\n",
      "Epoch 67/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8615 | train acc:\t0.6747\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.71      0.61      0.65      1671\n",
      "           2       0.69      0.54      0.61      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.62      0.55      0.59      1329\n",
      "           5       0.79      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.68      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1844 | val acc:\t0.6309\n",
      "\n",
      "Epoch 68/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8567 | train acc:\t0.6728\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.68      0.52      0.59      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.62      0.56      0.59      1329\n",
      "           5       0.80      0.83      0.81      1001\n",
      "\n",
      "    accuracy                           0.67     10896\n",
      "   macro avg       0.69      0.65      0.66     10896\n",
      "weighted avg       0.67      0.67      0.67     10896\n",
      "\n",
      "val loss:\t1.1887 | val acc:\t0.6248\n",
      "\n",
      "Epoch 69/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8520 | train acc:\t0.6778\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71      3943\n",
      "           1       0.70      0.59      0.64      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.64      0.55      0.60      1329\n",
      "           5       0.81      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.1555 | val acc:\t0.6352\n",
      "\n",
      "Epoch 70/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.8475 | train acc:\t0.6776\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71      3943\n",
      "           1       0.71      0.60      0.65      1671\n",
      "           2       0.68      0.55      0.61      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.64      0.55      0.59      1329\n",
      "           5       0.81      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.1819 | val acc:\t0.6297\n",
      "\n",
      "Epoch 71/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8484 | train acc:\t0.6823\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.72      0.62      0.66      1671\n",
      "           2       0.69      0.53      0.60      1174\n",
      "           3       0.64      0.63      0.64      1778\n",
      "           4       0.65      0.57      0.61      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1407 | val acc:\t0.6343\n",
      "\n",
      "Epoch 72/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8407 | train acc:\t0.6830\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.70      0.56      0.63      1174\n",
      "           3       0.64      0.63      0.64      1778\n",
      "           4       0.66      0.57      0.61      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1721 | val acc:\t0.6138\n",
      "\n",
      "Epoch 73/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8473 | train acc:\t0.6769\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71      3943\n",
      "           1       0.71      0.59      0.64      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.1343 | val acc:\t0.6364\n",
      "\n",
      "Epoch 74/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8363 | train acc:\t0.6817\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.79      0.72      3943\n",
      "           1       0.72      0.62      0.66      1671\n",
      "           2       0.69      0.55      0.61      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.64      0.55      0.59      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1672 | val acc:\t0.6297\n",
      "\n",
      "Epoch 75/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8488 | train acc:\t0.6817\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.72      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.65      0.62      0.64      1778\n",
      "           4       0.63      0.56      0.60      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1147 | val acc:\t0.6175\n",
      "\n",
      "Epoch 76/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8541 | train acc:\t0.6767\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71      3943\n",
      "           1       0.71      0.60      0.65      1671\n",
      "           2       0.69      0.56      0.62      1174\n",
      "           3       0.64      0.62      0.63      1778\n",
      "           4       0.63      0.56      0.59      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.65      0.67     10896\n",
      "weighted avg       0.68      0.68      0.67     10896\n",
      "\n",
      "val loss:\t1.1334 | val acc:\t0.6270\n",
      "\n",
      "Epoch 77/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8388 | train acc:\t0.6816\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.71      0.62      0.66      1671\n",
      "           2       0.69      0.55      0.62      1174\n",
      "           3       0.65      0.62      0.64      1778\n",
      "           4       0.64      0.57      0.60      1329\n",
      "           5       0.82      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.69      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1729 | val acc:\t0.6337\n",
      "\n",
      "Epoch 78/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8162 | train acc:\t0.6912\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.73      0.62      0.67      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.67      0.58      0.62      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.2054 | val acc:\t0.6098\n",
      "\n",
      "Epoch 79/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8401 | train acc:\t0.6814\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.72      3943\n",
      "           1       0.73      0.60      0.66      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.63      0.62      0.62      1778\n",
      "           4       0.65      0.58      0.62      1329\n",
      "           5       0.81      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.68     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1374 | val acc:\t0.6263\n",
      "\n",
      "Epoch 80/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8182 | train acc:\t0.6919\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.79      0.72      3943\n",
      "           1       0.73      0.61      0.67      1671\n",
      "           2       0.71      0.59      0.64      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.64      0.56      0.60      1329\n",
      "           5       0.82      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1805 | val acc:\t0.6309\n",
      "\n",
      "Epoch 81/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8286 | train acc:\t0.6912\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.73      0.63      0.67      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.66      0.65      0.66      1778\n",
      "           4       0.65      0.58      0.61      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1678 | val acc:\t0.6077\n",
      "\n",
      "Epoch 82/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.8321 | train acc:\t0.6807\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71      3943\n",
      "           1       0.72      0.60      0.66      1671\n",
      "           2       0.69      0.55      0.61      1174\n",
      "           3       0.64      0.63      0.64      1778\n",
      "           4       0.65      0.57      0.61      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.68     10896\n",
      "   macro avg       0.70      0.66      0.67     10896\n",
      "weighted avg       0.68      0.68      0.68     10896\n",
      "\n",
      "val loss:\t1.1707 | val acc:\t0.6141\n",
      "\n",
      "Epoch 83/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8226 | train acc:\t0.6872\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.71      0.63      0.67      1671\n",
      "           2       0.69      0.56      0.62      1174\n",
      "           3       0.67      0.62      0.64      1778\n",
      "           4       0.65      0.59      0.62      1329\n",
      "           5       0.81      0.81      0.81      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.68     10896\n",
      "\n",
      "val loss:\t1.1415 | val acc:\t0.6346\n",
      "\n",
      "Epoch 84/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8181 | train acc:\t0.6900\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.73      0.61      0.66      1671\n",
      "           2       0.73      0.58      0.65      1174\n",
      "           3       0.64      0.64      0.64      1778\n",
      "           4       0.65      0.59      0.62      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1556 | val acc:\t0.6346\n",
      "\n",
      "Epoch 85/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8181 | train acc:\t0.6881\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.71      0.58      0.63      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.65      0.56      0.60      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1228 | val acc:\t0.6410\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.81      0.73      1518\n",
      "           1       0.75      0.37      0.50       468\n",
      "           2       0.54      0.43      0.48       321\n",
      "           3       0.47      0.60      0.53       397\n",
      "           4       0.55      0.45      0.50       309\n",
      "           5       0.91      0.69      0.79       252\n",
      "\n",
      "    accuracy                           0.64      3265\n",
      "   macro avg       0.65      0.56      0.59      3265\n",
      "weighted avg       0.65      0.64      0.63      3265\n",
      "\n",
      "Epoch 86/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8116 | train acc:\t0.6974\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73      3943\n",
      "           1       0.73      0.63      0.68      1671\n",
      "           2       0.70      0.57      0.63      1174\n",
      "           3       0.65      0.65      0.65      1778\n",
      "           4       0.68      0.60      0.63      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.69     10896\n",
      "\n",
      "val loss:\t1.1415 | val acc:\t0.6325\n",
      "\n",
      "Epoch 87/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8053 | train acc:\t0.6931\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.72      0.61      0.66      1671\n",
      "           2       0.71      0.59      0.65      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1515 | val acc:\t0.6319\n",
      "\n",
      "Epoch 88/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8189 | train acc:\t0.6920\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.72      0.62      0.67      1671\n",
      "           2       0.72      0.58      0.64      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.68      0.58      0.62      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.2139 | val acc:\t0.6196\n",
      "\n",
      "Epoch 89/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8187 | train acc:\t0.6899\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.73      0.63      0.68      1671\n",
      "           2       0.70      0.56      0.62      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.65      0.59      0.62      1329\n",
      "           5       0.81      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.70      0.67      0.68     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1534 | val acc:\t0.6328\n",
      "\n",
      "Epoch 90/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8052 | train acc:\t0.6947\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.72      0.57      0.63      1174\n",
      "           3       0.67      0.66      0.67      1778\n",
      "           4       0.66      0.59      0.62      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1370 | val acc:\t0.6273\n",
      "\n",
      "Epoch 91/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8049 | train acc:\t0.6920\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72      3943\n",
      "           1       0.74      0.63      0.68      1671\n",
      "           2       0.72      0.58      0.64      1174\n",
      "           3       0.65      0.63      0.64      1778\n",
      "           4       0.65      0.58      0.62      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1069 | val acc:\t0.6303\n",
      "\n",
      "Epoch 92/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8041 | train acc:\t0.6958\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.70      0.58      0.64      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.66      0.58      0.62      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.69     10896\n",
      "\n",
      "val loss:\t1.1659 | val acc:\t0.6211\n",
      "\n",
      "Epoch 93/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7944 | train acc:\t0.6999\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73      3943\n",
      "           1       0.73      0.63      0.68      1671\n",
      "           2       0.72      0.59      0.65      1174\n",
      "           3       0.66      0.66      0.66      1778\n",
      "           4       0.66      0.60      0.63      1329\n",
      "           5       0.81      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1236 | val acc:\t0.6266\n",
      "\n",
      "Epoch 94/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8037 | train acc:\t0.7024\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.72      0.60      0.65      1174\n",
      "           3       0.68      0.67      0.67      1778\n",
      "           4       0.67      0.57      0.62      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.68      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1593 | val acc:\t0.6144\n",
      "\n",
      "Epoch 95/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7904 | train acc:\t0.7075\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.80      0.74      3943\n",
      "           1       0.75      0.64      0.69      1671\n",
      "           2       0.72      0.58      0.64      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.68      0.60      0.64      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1675 | val acc:\t0.6214\n",
      "\n",
      "Epoch 96/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8025 | train acc:\t0.6961\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.73      3943\n",
      "           1       0.73      0.63      0.68      1671\n",
      "           2       0.69      0.59      0.63      1174\n",
      "           3       0.67      0.65      0.66      1778\n",
      "           4       0.66      0.60      0.63      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.70      0.70      0.69     10896\n",
      "\n",
      "val loss:\t1.1619 | val acc:\t0.6135\n",
      "\n",
      "Epoch 97/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8003 | train acc:\t0.6952\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.79      0.73      3943\n",
      "           1       0.73      0.61      0.67      1671\n",
      "           2       0.71      0.57      0.64      1174\n",
      "           3       0.66      0.65      0.65      1778\n",
      "           4       0.64      0.60      0.62      1329\n",
      "           5       0.80      0.82      0.81      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.67      0.69     10896\n",
      "weighted avg       0.70      0.70      0.69     10896\n",
      "\n",
      "val loss:\t1.1398 | val acc:\t0.6184\n",
      "\n",
      "Epoch 98/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.8017 | train acc:\t0.6936\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.72      3943\n",
      "           1       0.73      0.64      0.68      1671\n",
      "           2       0.71      0.57      0.63      1174\n",
      "           3       0.65      0.65      0.65      1778\n",
      "           4       0.65      0.57      0.61      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.69     10896\n",
      "   macro avg       0.71      0.68      0.69     10896\n",
      "weighted avg       0.69      0.69      0.69     10896\n",
      "\n",
      "val loss:\t1.1418 | val acc:\t0.6251\n",
      "\n",
      "Epoch 99/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7861 | train acc:\t0.7053\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.79      0.74      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.72      0.59      0.65      1174\n",
      "           3       0.68      0.67      0.68      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.71      0.70     10896\n",
      "\n",
      "val loss:\t1.1276 | val acc:\t0.6309\n",
      "\n",
      "Epoch 100/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7782 | train acc:\t0.7077\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.79      0.74      3943\n",
      "           1       0.74      0.64      0.69      1671\n",
      "           2       0.73      0.62      0.67      1174\n",
      "           3       0.68      0.65      0.67      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1736 | val acc:\t0.6009\n",
      "\n",
      "Epoch 101/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7709 | train acc:\t0.7113\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3943\n",
      "           1       0.74      0.65      0.70      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.69      0.67      0.68      1778\n",
      "           4       0.67      0.61      0.64      1329\n",
      "           5       0.81      0.82      0.82      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1442 | val acc:\t0.6331\n",
      "\n",
      "Epoch 102/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7888 | train acc:\t0.7032\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.73      0.60      0.66      1174\n",
      "           3       0.67      0.64      0.65      1778\n",
      "           4       0.69      0.62      0.65      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1225 | val acc:\t0.6153\n",
      "\n",
      "Epoch 103/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7696 | train acc:\t0.7124\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.78      0.74      3943\n",
      "           1       0.73      0.66      0.69      1671\n",
      "           2       0.73      0.60      0.66      1174\n",
      "           3       0.66      0.66      0.66      1778\n",
      "           4       0.69      0.63      0.66      1329\n",
      "           5       0.84      0.85      0.85      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.0861 | val acc:\t0.6196\n",
      "\n",
      "Epoch 104/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7923 | train acc:\t0.7037\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73      3943\n",
      "           1       0.72      0.63      0.67      1671\n",
      "           2       0.70      0.59      0.64      1174\n",
      "           3       0.68      0.66      0.67      1778\n",
      "           4       0.67      0.61      0.64      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1217 | val acc:\t0.6358\n",
      "\n",
      "Epoch 105/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7766 | train acc:\t0.7098\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.80      0.74      3943\n",
      "           1       0.75      0.64      0.69      1671\n",
      "           2       0.72      0.62      0.67      1174\n",
      "           3       0.68      0.66      0.67      1778\n",
      "           4       0.69      0.60      0.64      1329\n",
      "           5       0.80      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1145 | val acc:\t0.6288\n",
      "\n",
      "Epoch 106/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7735 | train acc:\t0.7103\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3943\n",
      "           1       0.74      0.66      0.70      1671\n",
      "           2       0.71      0.60      0.65      1174\n",
      "           3       0.68      0.66      0.67      1778\n",
      "           4       0.66      0.60      0.63      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1660 | val acc:\t0.6015\n",
      "\n",
      "Epoch 107/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7787 | train acc:\t0.7036\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.78      0.74      3943\n",
      "           1       0.73      0.65      0.69      1671\n",
      "           2       0.72      0.60      0.66      1174\n",
      "           3       0.66      0.64      0.65      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.81      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.70     10896\n",
      "   macro avg       0.71      0.69      0.70     10896\n",
      "weighted avg       0.70      0.70      0.70     10896\n",
      "\n",
      "val loss:\t1.1319 | val acc:\t0.6190\n",
      "\n",
      "Epoch 108/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7711 | train acc:\t0.7146\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.80      0.75      3943\n",
      "           1       0.74      0.65      0.69      1671\n",
      "           2       0.70      0.60      0.65      1174\n",
      "           3       0.68      0.67      0.68      1778\n",
      "           4       0.69      0.61      0.65      1329\n",
      "           5       0.83      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.71     10896\n",
      "weighted avg       0.72      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1115 | val acc:\t0.6300\n",
      "\n",
      "Epoch 109/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7673 | train acc:\t0.7087\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3943\n",
      "           1       0.75      0.65      0.69      1671\n",
      "           2       0.71      0.58      0.64      1174\n",
      "           3       0.67      0.68      0.67      1778\n",
      "           4       0.67      0.61      0.64      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.70     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1127 | val acc:\t0.6291\n",
      "\n",
      "Epoch 110/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7640 | train acc:\t0.7148\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3943\n",
      "           1       0.75      0.66      0.70      1671\n",
      "           2       0.72      0.63      0.67      1174\n",
      "           3       0.69      0.68      0.68      1778\n",
      "           4       0.68      0.59      0.64      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1304 | val acc:\t0.6260\n",
      "\n",
      "Epoch 111/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7612 | train acc:\t0.7171\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3943\n",
      "           1       0.76      0.67      0.71      1671\n",
      "           2       0.70      0.61      0.66      1174\n",
      "           3       0.70      0.68      0.69      1778\n",
      "           4       0.67      0.62      0.64      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1110 | val acc:\t0.6172\n",
      "\n",
      "Epoch 112/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7628 | train acc:\t0.7142\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3943\n",
      "           1       0.74      0.65      0.69      1671\n",
      "           2       0.73      0.60      0.66      1174\n",
      "           3       0.68      0.67      0.67      1778\n",
      "           4       0.69      0.62      0.65      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1532 | val acc:\t0.6306\n",
      "\n",
      "Epoch 113/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7597 | train acc:\t0.7193\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.80      0.75      3943\n",
      "           1       0.76      0.66      0.71      1671\n",
      "           2       0.74      0.62      0.67      1174\n",
      "           3       0.68      0.67      0.68      1778\n",
      "           4       0.68      0.60      0.64      1329\n",
      "           5       0.82      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1328 | val acc:\t0.6217\n",
      "\n",
      "Epoch 114/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7584 | train acc:\t0.7183\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.79      0.75      3943\n",
      "           1       0.74      0.66      0.70      1671\n",
      "           2       0.73      0.62      0.67      1174\n",
      "           3       0.68      0.68      0.68      1778\n",
      "           4       0.69      0.63      0.66      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1164 | val acc:\t0.6395\n",
      "\n",
      "Epoch 115/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7595 | train acc:\t0.7127\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3943\n",
      "           1       0.74      0.66      0.70      1671\n",
      "           2       0.72      0.60      0.65      1174\n",
      "           3       0.68      0.68      0.68      1778\n",
      "           4       0.68      0.62      0.65      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1314 | val acc:\t0.6291\n",
      "\n",
      "Epoch 116/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7474 | train acc:\t0.7178\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3943\n",
      "           1       0.75      0.66      0.70      1671\n",
      "           2       0.74      0.63      0.68      1174\n",
      "           3       0.68      0.67      0.68      1778\n",
      "           4       0.69      0.62      0.65      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1335 | val acc:\t0.6260\n",
      "\n",
      "Epoch 117/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7556 | train acc:\t0.7171\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.79      0.74      3943\n",
      "           1       0.74      0.68      0.71      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.69      0.67      0.68      1778\n",
      "           4       0.69      0.61      0.65      1329\n",
      "           5       0.81      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0949 | val acc:\t0.6328\n",
      "\n",
      "Epoch 118/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7511 | train acc:\t0.7184\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.75      3943\n",
      "           1       0.75      0.65      0.70      1671\n",
      "           2       0.73      0.62      0.67      1174\n",
      "           3       0.69      0.69      0.69      1778\n",
      "           4       0.67      0.61      0.64      1329\n",
      "           5       0.82      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1283 | val acc:\t0.6282\n",
      "\n",
      "Epoch 119/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7626 | train acc:\t0.7125\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.75      3943\n",
      "           1       0.73      0.67      0.70      1671\n",
      "           2       0.72      0.60      0.65      1174\n",
      "           3       0.67      0.64      0.66      1778\n",
      "           4       0.69      0.63      0.66      1329\n",
      "           5       0.82      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1305 | val acc:\t0.6251\n",
      "\n",
      "Epoch 120/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7502 | train acc:\t0.7193\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3943\n",
      "           1       0.74      0.66      0.70      1671\n",
      "           2       0.72      0.63      0.67      1174\n",
      "           3       0.69      0.68      0.69      1778\n",
      "           4       0.69      0.62      0.65      1329\n",
      "           5       0.84      0.84      0.84      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.72     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1162 | val acc:\t0.6322\n",
      "\n",
      "Epoch 121/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7550 | train acc:\t0.7152\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3943\n",
      "           1       0.75      0.66      0.70      1671\n",
      "           2       0.74      0.61      0.67      1174\n",
      "           3       0.70      0.67      0.69      1778\n",
      "           4       0.67      0.62      0.64      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.71     10896\n",
      "\n",
      "val loss:\t1.1109 | val acc:\t0.6456\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.83      0.74      1518\n",
      "           1       0.63      0.46      0.53       468\n",
      "           2       0.68      0.34      0.45       321\n",
      "           3       0.51      0.55      0.53       397\n",
      "           4       0.54      0.39      0.45       309\n",
      "           5       0.89      0.71      0.79       252\n",
      "\n",
      "    accuracy                           0.65      3265\n",
      "   macro avg       0.65      0.55      0.58      3265\n",
      "weighted avg       0.65      0.65      0.63      3265\n",
      "\n",
      "Epoch 122/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7469 | train acc:\t0.7215\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.80      0.75      3943\n",
      "           1       0.74      0.66      0.70      1671\n",
      "           2       0.72      0.63      0.67      1174\n",
      "           3       0.69      0.67      0.68      1778\n",
      "           4       0.70      0.63      0.66      1329\n",
      "           5       0.82      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.71      0.72     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1207 | val acc:\t0.6328\n",
      "\n",
      "Epoch 123/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7574 | train acc:\t0.7125\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3943\n",
      "           1       0.74      0.66      0.70      1671\n",
      "           2       0.72      0.60      0.66      1174\n",
      "           3       0.68      0.66      0.67      1778\n",
      "           4       0.68      0.62      0.65      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.69      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1150 | val acc:\t0.6270\n",
      "\n",
      "Epoch 124/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7538 | train acc:\t0.7136\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3943\n",
      "           1       0.73      0.63      0.68      1671\n",
      "           2       0.72      0.61      0.66      1174\n",
      "           3       0.68      0.69      0.68      1778\n",
      "           4       0.69      0.62      0.65      1329\n",
      "           5       0.83      0.84      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1182 | val acc:\t0.6297\n",
      "\n",
      "Epoch 125/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7397 | train acc:\t0.7229\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.80      0.75      3943\n",
      "           1       0.75      0.66      0.71      1671\n",
      "           2       0.74      0.63      0.68      1174\n",
      "           3       0.69      0.69      0.69      1778\n",
      "           4       0.69      0.61      0.65      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.72     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.0879 | val acc:\t0.6300\n",
      "\n",
      "Epoch 126/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7458 | train acc:\t0.7182\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.75      3943\n",
      "           1       0.74      0.66      0.69      1671\n",
      "           2       0.73      0.62      0.67      1174\n",
      "           3       0.69      0.68      0.69      1778\n",
      "           4       0.69      0.63      0.66      1329\n",
      "           5       0.82      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1099 | val acc:\t0.6432\n",
      "\n",
      "Epoch 127/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7549 | train acc:\t0.7154\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.79      0.75      3943\n",
      "           1       0.75      0.66      0.70      1671\n",
      "           2       0.71      0.61      0.66      1174\n",
      "           3       0.67      0.68      0.67      1778\n",
      "           4       0.68      0.60      0.64      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.71     10896\n",
      "\n",
      "val loss:\t1.0976 | val acc:\t0.6184\n",
      "\n",
      "Epoch 128/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7517 | train acc:\t0.7125\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3943\n",
      "           1       0.73      0.67      0.70      1671\n",
      "           2       0.71      0.59      0.64      1174\n",
      "           3       0.69      0.67      0.68      1778\n",
      "           4       0.68      0.62      0.65      1329\n",
      "           5       0.84      0.84      0.84      1001\n",
      "\n",
      "    accuracy                           0.71     10896\n",
      "   macro avg       0.72      0.70      0.71     10896\n",
      "weighted avg       0.71      0.71      0.71     10896\n",
      "\n",
      "val loss:\t1.1227 | val acc:\t0.6315\n",
      "\n",
      "Epoch 129/200\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:\t0.7467 | train acc:\t0.7182\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.79      0.75      3943\n",
      "           1       0.74      0.65      0.69      1671\n",
      "           2       0.73      0.62      0.67      1174\n",
      "           3       0.68      0.67      0.67      1778\n",
      "           4       0.70      0.63      0.66      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1455 | val acc:\t0.6395\n",
      "\n",
      "Epoch 130/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7520 | train acc:\t0.7173\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.75      3943\n",
      "           1       0.75      0.69      0.72      1671\n",
      "           2       0.71      0.59      0.64      1174\n",
      "           3       0.69      0.68      0.69      1778\n",
      "           4       0.67      0.60      0.63      1329\n",
      "           5       0.82      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1336 | val acc:\t0.6328\n",
      "\n",
      "Epoch 131/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7505 | train acc:\t0.7171\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3943\n",
      "           1       0.73      0.66      0.69      1671\n",
      "           2       0.73      0.61      0.66      1174\n",
      "           3       0.69      0.68      0.68      1778\n",
      "           4       0.70      0.64      0.67      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.0928 | val acc:\t0.6233\n",
      "\n",
      "Epoch 132/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7464 | train acc:\t0.7190\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.75      3943\n",
      "           1       0.75      0.66      0.70      1671\n",
      "           2       0.74      0.61      0.67      1174\n",
      "           3       0.69      0.67      0.68      1778\n",
      "           4       0.68      0.62      0.65      1329\n",
      "           5       0.83      0.86      0.84      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1260 | val acc:\t0.6242\n",
      "\n",
      "Epoch 133/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7322 | train acc:\t0.7260\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.80      0.75      3943\n",
      "           1       0.77      0.67      0.72      1671\n",
      "           2       0.74      0.63      0.68      1174\n",
      "           3       0.69      0.68      0.69      1778\n",
      "           4       0.69      0.63      0.66      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.71      0.72     10896\n",
      "weighted avg       0.73      0.73      0.72     10896\n",
      "\n",
      "val loss:\t1.1108 | val acc:\t0.6441\n",
      "\n",
      "Epoch 134/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7401 | train acc:\t0.7191\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.75      3943\n",
      "           1       0.75      0.66      0.70      1671\n",
      "           2       0.73      0.63      0.67      1174\n",
      "           3       0.69      0.69      0.69      1778\n",
      "           4       0.67      0.60      0.64      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1231 | val acc:\t0.6395\n",
      "\n",
      "Epoch 135/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7264 | train acc:\t0.7246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.80      0.75      3943\n",
      "           1       0.76      0.68      0.72      1671\n",
      "           2       0.72      0.62      0.67      1174\n",
      "           3       0.69      0.67      0.68      1778\n",
      "           4       0.69      0.63      0.66      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.71      0.72     10896\n",
      "weighted avg       0.73      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.0601 | val acc:\t0.6508\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.83      0.75      1518\n",
      "           1       0.61      0.53      0.56       468\n",
      "           2       0.62      0.37      0.47       321\n",
      "           3       0.51      0.43      0.47       397\n",
      "           4       0.57      0.43      0.49       309\n",
      "           5       0.85      0.75      0.79       252\n",
      "\n",
      "    accuracy                           0.65      3265\n",
      "   macro avg       0.64      0.56      0.59      3265\n",
      "weighted avg       0.64      0.65      0.64      3265\n",
      "\n",
      "Epoch 136/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7214 | train acc:\t0.7295\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76      3943\n",
      "           1       0.76      0.69      0.72      1671\n",
      "           2       0.72      0.63      0.67      1174\n",
      "           3       0.71      0.68      0.69      1778\n",
      "           4       0.69      0.64      0.66      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.71      0.72     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.1114 | val acc:\t0.6334\n",
      "\n",
      "Epoch 137/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7317 | train acc:\t0.7253\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.80      0.75      3943\n",
      "           1       0.75      0.67      0.71      1671\n",
      "           2       0.75      0.63      0.68      1174\n",
      "           3       0.69      0.68      0.68      1778\n",
      "           4       0.71      0.64      0.67      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.71      0.72     10896\n",
      "weighted avg       0.73      0.73      0.72     10896\n",
      "\n",
      "val loss:\t1.1419 | val acc:\t0.6236\n",
      "\n",
      "Epoch 138/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7347 | train acc:\t0.7283\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.80      0.75      3943\n",
      "           1       0.75      0.67      0.71      1671\n",
      "           2       0.75      0.63      0.68      1174\n",
      "           3       0.70      0.69      0.70      1778\n",
      "           4       0.70      0.64      0.67      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.71      0.72     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.1007 | val acc:\t0.6331\n",
      "\n",
      "Epoch 139/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7410 | train acc:\t0.7275\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.80      0.75      3943\n",
      "           1       0.77      0.69      0.73      1671\n",
      "           2       0.74      0.63      0.68      1174\n",
      "           3       0.70      0.68      0.69      1778\n",
      "           4       0.68      0.62      0.65      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.71      0.72     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.0988 | val acc:\t0.6288\n",
      "\n",
      "Epoch 140/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7319 | train acc:\t0.7263\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76      3943\n",
      "           1       0.76      0.68      0.72      1671\n",
      "           2       0.73      0.62      0.67      1174\n",
      "           3       0.69      0.69      0.69      1778\n",
      "           4       0.68      0.63      0.65      1329\n",
      "           5       0.83      0.83      0.83      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.73      0.71      0.72     10896\n",
      "weighted avg       0.73      0.73      0.72     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0891 | val acc:\t0.6331\n",
      "\n",
      "Epoch 141/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7451 | train acc:\t0.7182\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3943\n",
      "           1       0.75      0.65      0.70      1671\n",
      "           2       0.73      0.62      0.67      1174\n",
      "           3       0.68      0.67      0.68      1778\n",
      "           4       0.70      0.63      0.66      1329\n",
      "           5       0.82      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1174 | val acc:\t0.6168\n",
      "\n",
      "Epoch 142/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7333 | train acc:\t0.7218\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.79      0.75      3943\n",
      "           1       0.74      0.68      0.71      1671\n",
      "           2       0.73      0.63      0.68      1174\n",
      "           3       0.70      0.67      0.68      1778\n",
      "           4       0.68      0.62      0.65      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.71      0.72     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1505 | val acc:\t0.6178\n",
      "\n",
      "Epoch 143/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7403 | train acc:\t0.7209\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.80      0.75      3943\n",
      "           1       0.72      0.66      0.69      1671\n",
      "           2       0.74      0.64      0.69      1174\n",
      "           3       0.69      0.66      0.67      1778\n",
      "           4       0.70      0.63      0.67      1329\n",
      "           5       0.82      0.85      0.83      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.71      0.72     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1331 | val acc:\t0.6257\n",
      "\n",
      "Epoch 144/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7204 | train acc:\t0.7302\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76      3943\n",
      "           1       0.76      0.67      0.71      1671\n",
      "           2       0.74      0.64      0.69      1174\n",
      "           3       0.69      0.69      0.69      1778\n",
      "           4       0.70      0.65      0.68      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.71      0.73     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.1068 | val acc:\t0.6315\n",
      "\n",
      "Epoch 145/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7265 | train acc:\t0.7279\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.79      0.75      3943\n",
      "           1       0.75      0.67      0.71      1671\n",
      "           2       0.74      0.63      0.68      1174\n",
      "           3       0.69      0.69      0.69      1778\n",
      "           4       0.69      0.63      0.66      1329\n",
      "           5       0.83      0.86      0.85      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.71      0.72     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.1473 | val acc:\t0.6300\n",
      "\n",
      "Epoch 146/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7283 | train acc:\t0.7273\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76      3943\n",
      "           1       0.74      0.67      0.70      1671\n",
      "           2       0.75      0.63      0.69      1174\n",
      "           3       0.70      0.67      0.68      1778\n",
      "           4       0.70      0.64      0.67      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.71      0.72     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.1069 | val acc:\t0.6331\n",
      "\n",
      "Epoch 147/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7090 | train acc:\t0.7331\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76      3943\n",
      "           1       0.77      0.69      0.73      1671\n",
      "           2       0.74      0.64      0.69      1174\n",
      "           3       0.69      0.69      0.69      1778\n",
      "           4       0.69      0.64      0.66      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.72      0.73     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.0702 | val acc:\t0.6337\n",
      "\n",
      "Epoch 148/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7279 | train acc:\t0.7247\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.80      0.75      3943\n",
      "           1       0.76      0.69      0.72      1671\n",
      "           2       0.74      0.63      0.68      1174\n",
      "           3       0.70      0.69      0.69      1778\n",
      "           4       0.68      0.61      0.64      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.71      0.72     10896\n",
      "weighted avg       0.73      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1195 | val acc:\t0.6288\n",
      "\n",
      "Epoch 149/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7284 | train acc:\t0.7259\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.79      0.75      3943\n",
      "           1       0.75      0.67      0.71      1671\n",
      "           2       0.74      0.65      0.69      1174\n",
      "           3       0.69      0.69      0.69      1778\n",
      "           4       0.69      0.64      0.66      1329\n",
      "           5       0.83      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.71      0.72     10896\n",
      "weighted avg       0.73      0.73      0.72     10896\n",
      "\n",
      "val loss:\t1.0894 | val acc:\t0.6364\n",
      "\n",
      "Epoch 150/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7270 | train acc:\t0.7321\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.81      0.76      3943\n",
      "           1       0.75      0.68      0.71      1671\n",
      "           2       0.74      0.65      0.70      1174\n",
      "           3       0.71      0.68      0.69      1778\n",
      "           4       0.70      0.63      0.66      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.72      0.73     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.1267 | val acc:\t0.6319\n",
      "\n",
      "Epoch 151/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7105 | train acc:\t0.7342\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76      3943\n",
      "           1       0.76      0.70      0.72      1671\n",
      "           2       0.74      0.64      0.69      1174\n",
      "           3       0.71      0.69      0.70      1778\n",
      "           4       0.71      0.65      0.68      1329\n",
      "           5       0.84      0.84      0.84      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.72      0.73     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.1148 | val acc:\t0.6217\n",
      "\n",
      "Epoch 152/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7129 | train acc:\t0.7322\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.80      0.75      3943\n",
      "           1       0.75      0.68      0.71      1671\n",
      "           2       0.75      0.66      0.70      1174\n",
      "           3       0.71      0.68      0.69      1778\n",
      "           4       0.71      0.64      0.68      1329\n",
      "           5       0.84      0.86      0.85      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.72      0.73     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.0921 | val acc:\t0.6386\n",
      "\n",
      "Epoch 153/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7107 | train acc:\t0.7381\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.80      0.76      3943\n",
      "           1       0.75      0.67      0.71      1671\n",
      "           2       0.75      0.66      0.70      1174\n",
      "           3       0.72      0.70      0.71      1778\n",
      "           4       0.72      0.66      0.69      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.74     10896\n",
      "   macro avg       0.75      0.72      0.73     10896\n",
      "weighted avg       0.74      0.74      0.74     10896\n",
      "\n",
      "val loss:\t1.1248 | val acc:\t0.6217\n",
      "\n",
      "Epoch 154/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7292 | train acc:\t0.7278\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.80      0.75      3943\n",
      "           1       0.76      0.68      0.72      1671\n",
      "           2       0.74      0.64      0.69      1174\n",
      "           3       0.71      0.68      0.69      1778\n",
      "           4       0.71      0.64      0.67      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.71      0.72     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.1008 | val acc:\t0.6355\n",
      "\n",
      "Epoch 155/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7171 | train acc:\t0.7318\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76      3943\n",
      "           1       0.76      0.68      0.72      1671\n",
      "           2       0.75      0.63      0.68      1174\n",
      "           3       0.71      0.69      0.70      1778\n",
      "           4       0.68      0.64      0.66      1329\n",
      "           5       0.84      0.84      0.84      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.72      0.73     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.1215 | val acc:\t0.6386\n",
      "\n",
      "Epoch 156/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7134 | train acc:\t0.7305\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76      3943\n",
      "           1       0.75      0.67      0.71      1671\n",
      "           2       0.74      0.64      0.69      1174\n",
      "           3       0.71      0.70      0.70      1778\n",
      "           4       0.69      0.64      0.67      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.72      0.73     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.1068 | val acc:\t0.6343\n",
      "\n",
      "Epoch 157/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7134 | train acc:\t0.7352\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76      3943\n",
      "           1       0.77      0.70      0.73      1671\n",
      "           2       0.73      0.63      0.68      1174\n",
      "           3       0.72      0.69      0.71      1778\n",
      "           4       0.69      0.64      0.67      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.74     10896\n",
      "   macro avg       0.74      0.72      0.73     10896\n",
      "weighted avg       0.74      0.74      0.73     10896\n",
      "\n",
      "val loss:\t1.1045 | val acc:\t0.6156\n",
      "\n",
      "Epoch 158/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7141 | train acc:\t0.7301\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.80      0.75      3943\n",
      "           1       0.76      0.68      0.72      1671\n",
      "           2       0.75      0.64      0.69      1174\n",
      "           3       0.70      0.68      0.69      1778\n",
      "           4       0.71      0.65      0.68      1329\n",
      "           5       0.84      0.84      0.84      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.71      0.73     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.1404 | val acc:\t0.6184\n",
      "\n",
      "Epoch 159/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7104 | train acc:\t0.7326\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.81      0.76      3943\n",
      "           1       0.76      0.69      0.72      1671\n",
      "           2       0.74      0.62      0.67      1174\n",
      "           3       0.72      0.70      0.71      1778\n",
      "           4       0.69      0.63      0.66      1329\n",
      "           5       0.84      0.84      0.84      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.71      0.73     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.0999 | val acc:\t0.6444\n",
      "\n",
      "Epoch 160/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7384 | train acc:\t0.7189\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.79      0.75      3943\n",
      "           1       0.74      0.64      0.69      1671\n",
      "           2       0.72      0.63      0.68      1174\n",
      "           3       0.70      0.69      0.69      1778\n",
      "           4       0.68      0.63      0.65      1329\n",
      "           5       0.81      0.83      0.82      1001\n",
      "\n",
      "    accuracy                           0.72     10896\n",
      "   macro avg       0.73      0.70      0.71     10896\n",
      "weighted avg       0.72      0.72      0.72     10896\n",
      "\n",
      "val loss:\t1.1072 | val acc:\t0.6413\n",
      "\n",
      "Epoch 161/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7072 | train acc:\t0.7347\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76      3943\n",
      "           1       0.75      0.68      0.71      1671\n",
      "           2       0.76      0.65      0.70      1174\n",
      "           3       0.70      0.70      0.70      1778\n",
      "           4       0.72      0.65      0.68      1329\n",
      "           5       0.82      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.75      0.72      0.73     10896\n",
      "weighted avg       0.74      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.0836 | val acc:\t0.6233\n",
      "\n",
      "Epoch 162/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7213 | train acc:\t0.7300\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76      3943\n",
      "           1       0.75      0.67      0.71      1671\n",
      "           2       0.75      0.65      0.70      1174\n",
      "           3       0.69      0.69      0.69      1778\n",
      "           4       0.69      0.64      0.67      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.72      0.73     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.1135 | val acc:\t0.6358\n",
      "\n",
      "Epoch 163/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7087 | train acc:\t0.7362\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76      3943\n",
      "           1       0.76      0.70      0.73      1671\n",
      "           2       0.76      0.65      0.70      1174\n",
      "           3       0.71      0.69      0.70      1778\n",
      "           4       0.70      0.64      0.67      1329\n",
      "           5       0.83      0.84      0.84      1001\n",
      "\n",
      "    accuracy                           0.74     10896\n",
      "   macro avg       0.75      0.72      0.73     10896\n",
      "weighted avg       0.74      0.74      0.73     10896\n",
      "\n",
      "val loss:\t1.1130 | val acc:\t0.6453\n",
      "\n",
      "Epoch 164/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7008 | train acc:\t0.7389\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.81      0.76      3943\n",
      "           1       0.76      0.68      0.72      1671\n",
      "           2       0.75      0.67      0.71      1174\n",
      "           3       0.71      0.71      0.71      1778\n",
      "           4       0.72      0.65      0.68      1329\n",
      "           5       0.82      0.84      0.83      1001\n",
      "\n",
      "    accuracy                           0.74     10896\n",
      "   macro avg       0.75      0.72      0.73     10896\n",
      "weighted avg       0.74      0.74      0.74     10896\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:\t1.1040 | val acc:\t0.6358\n",
      "\n",
      "Epoch 165/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7080 | train acc:\t0.7338\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.80      0.76      3943\n",
      "           1       0.74      0.68      0.71      1671\n",
      "           2       0.74      0.66      0.70      1174\n",
      "           3       0.71      0.70      0.70      1778\n",
      "           4       0.70      0.63      0.66      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.72      0.73     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.0958 | val acc:\t0.6221\n",
      "\n",
      "Epoch 166/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7186 | train acc:\t0.7338\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.81      0.76      3943\n",
      "           1       0.75      0.68      0.72      1671\n",
      "           2       0.75      0.64      0.69      1174\n",
      "           3       0.71      0.69      0.70      1778\n",
      "           4       0.70      0.63      0.66      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.72      0.73     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.0842 | val acc:\t0.6279\n",
      "\n",
      "Epoch 167/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7098 | train acc:\t0.7337\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.79      0.76      3943\n",
      "           1       0.76      0.69      0.72      1671\n",
      "           2       0.75      0.64      0.69      1174\n",
      "           3       0.70      0.69      0.70      1778\n",
      "           4       0.69      0.65      0.67      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.72      0.73     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.1336 | val acc:\t0.6153\n",
      "\n",
      "Epoch 168/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7082 | train acc:\t0.7368\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.80      0.76      3943\n",
      "           1       0.76      0.71      0.73      1671\n",
      "           2       0.74      0.64      0.69      1174\n",
      "           3       0.70      0.69      0.70      1778\n",
      "           4       0.70      0.65      0.67      1329\n",
      "           5       0.83      0.84      0.84      1001\n",
      "\n",
      "    accuracy                           0.74     10896\n",
      "   macro avg       0.74      0.72      0.73     10896\n",
      "weighted avg       0.74      0.74      0.74     10896\n",
      "\n",
      "val loss:\t1.0987 | val acc:\t0.6190\n",
      "\n",
      "Epoch 169/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.7147 | train acc:\t0.7312\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.79      0.75      3943\n",
      "           1       0.75      0.69      0.72      1671\n",
      "           2       0.74      0.64      0.69      1174\n",
      "           3       0.71      0.68      0.69      1778\n",
      "           4       0.71      0.65      0.68      1329\n",
      "           5       0.83      0.85      0.84      1001\n",
      "\n",
      "    accuracy                           0.73     10896\n",
      "   macro avg       0.74      0.72      0.73     10896\n",
      "weighted avg       0.73      0.73      0.73     10896\n",
      "\n",
      "val loss:\t1.0831 | val acc:\t0.6346\n",
      "\n",
      "Epoch 170/200\n",
      "------------------------------------------------------\n",
      "train loss:\t0.6977 | train acc:\t0.7393\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.81      0.76      3943\n",
      "           1       0.77      0.69      0.73      1671\n",
      "           2       0.75      0.65      0.70      1174\n",
      "           3       0.71      0.71      0.71      1778\n",
      "           4       0.71      0.64      0.67      1329\n",
      "           5       0.83      0.84      0.84      1001\n",
      "\n",
      "    accuracy                           0.74     10896\n",
      "   macro avg       0.75      0.72      0.74     10896\n",
      "weighted avg       0.74      0.74      0.74     10896\n",
      "\n",
      "val loss:\t1.1184 | val acc:\t0.6288\n",
      "\n",
      "Epoch 171/200\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "writer_prefix = \"mar11_ablation\"\n",
    "writer_extra_string = \"_data_augmentation\" \n",
    "conv1_filters_size = [8, 16, 32]\n",
    "conv2_filters_size = [16, 32, 8]\n",
    "conv3_filters_size = [32, 16, 8]\n",
    "linear1_sizes = [256, 512, 1024]\n",
    "dropouts = [0.3, 0.2]\n",
    "\n",
    "# lrs = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3]\n",
    "# weight_decays = [1e-5, 5e-5, 1e-4, 5e-4, 1e-6, 5e-6]\n",
    "lrs = [5e-4, 1e-3, 1e-4]\n",
    "weight_decays = [5e-3]\n",
    "\n",
    "amsgrads=[False]\n",
    "for conv1_filters in conv1_filters_size:\n",
    "    for conv2_filters in conv2_filters_size:\n",
    "        for conv3_filters in conv3_filters_size:\n",
    "            for linear1_size in linear1_sizes:\n",
    "                for dropout in dropouts:\n",
    "                    for lr in lrs:\n",
    "                        for wd in weight_decays:\n",
    "                            for amsgrad in amsgrads:\n",
    "                                config_string = f\"{conv1_filters}_{conv2_filters}_{conv3_filters}_{linear1_size}_{dropout}_{lr}_{wd}_{amsgrad}\"\n",
    "\n",
    "                                model_ft = ViewNet(num_classes, conv1_filters, conv2_filters, conv3_filters, linear1_size, dropout)\n",
    "                                writer = SummaryWriter(writer_prefix + \"/\" + config_string + writer_extra_string)\n",
    "                                SetupAndRunTest(model_ft, writer, lr, wd, amsgrad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJ6ex4Iu0yiy"
   },
   "source": [
    "Comparing without Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ua2ckaFcc-Je"
   },
   "outputs": [],
   "source": [
    "# # Initialize the non-pretrained version of the model used for this run\n",
    "# scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "# scratch_model = scratch_model.to(device)\n",
    "# scratch_optimizer = optim.Adam(scratch_model.parameters())\n",
    "# scratch_criterion = nn.CrossEntropyLoss()\n",
    "# _,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "\n",
    "# # Plot the training curves of validation accuracy vs. number\n",
    "# #  of training epochs for the transfer learning method and\n",
    "# #  the model trained from scratch\n",
    "# ohist = []\n",
    "# shist = []\n",
    "\n",
    "# ohist = [h.cpu().numpy() for h in hist]\n",
    "# shist = [h.cpu().numpy() for h in scratch_hist]\n",
    "\n",
    "# plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "# plt.xlabel(\"Training Epochs\")\n",
    "# plt.ylabel(\"Validation Accuracy\")\n",
    "# plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "# plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sr8JjsG-c-Jh"
   },
   "source": [
    "THE END! :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ozAHtAyc-Ji"
   },
   "outputs": [],
   "source": [
    "# Use dataloader to load the data\n",
    "\n",
    "# Read data from file\n",
    "\n",
    "# tansform the data into the desired shape\n",
    "\n",
    "\n",
    "# split on the train and test\n",
    "\n",
    "\n",
    "# convert into tensor\n",
    "\n",
    "\n",
    "\n",
    "# # Data Loading\n",
    "# batch_size = 100\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WtCwg6zpc-Jk"
   },
   "outputs": [],
   "source": [
    "# # Define a small CNN that processes 2-channel images\n",
    "# # and output the view label of the image, the output should be one of five labels\n",
    "# # kidney_right_sag, kidney_right_trav, kidney_left_sag, kidney_left_trav, bladder\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     # Initialize our layers, i.e. the set of trainable parameters\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         # A convolutional layer\n",
    "#         # The number of input channels is 2 greyscale\n",
    "#         self.conv1 = nn.Conv2d(2, 6, 5)\n",
    "#         # A max pooling layer (will be reused for each conv layer)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         # Another convolutional layer\n",
    "#         self.conv2 = nn.Conv2d(4, 16, 5)\n",
    "#         # Three sets of fully connected (linear) layers \n",
    "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 5)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # For each conv layer: conv -> relu -> pooling\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         # Reshape from higher dimensional tensor to a vector for the FC layers\n",
    "#         x = x.view(-1, 16 * 5 * 5)\n",
    "#         # Pass through fully connected layers\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# model = Net()\n",
    "# model = model.to(device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eu6w4OMac-Jn"
   },
   "outputs": [],
   "source": [
    "# # Look at the network structure\n",
    "# print(model.state_dict().keys())\n",
    "# print(\"Conv1:\",model.conv1.weight.size())\n",
    "# print(\"Conv2:\",model.conv2.weight.size())\n",
    "# print(\"fc1:\",model.fc1.weight.size())\n",
    "# print(\"fc2:\",model.fc2.weight.size())\n",
    "# print(\"fc3:\",model.fc3.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S1c-WZ_mc-Jp"
   },
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# for epoch in range(5):\n",
    "#     running_loss = 0.0\n",
    "#     num_total = 0.0\n",
    "#     num_correct = 0.0\n",
    "\n",
    "#     for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         outputs = model(inputs)              # Forward\n",
    "#         loss = criterion(outputs, labels)  # Compute loss\n",
    "#         optimizer.zero_grad()              # Zero the parameter gradients\n",
    "#         loss.backward()                    # Backward\n",
    "#         optimizer.step()                   # Gradient step\n",
    "\n",
    "#         num_correct += (torch.argmax(outputs, dim=1) == labels).sum().float()\n",
    "#         num_total += labels.size(0)\n",
    "        \n",
    "#         # Print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if (i+1) % 1000 == 0:    # print every 2000 mini-batches\n",
    "#             print('[{}, {:5d}] loss: {:6.4f}, acc: {:6.4f}'.format(\n",
    "#                    epoch + 1, i + 1, running_loss / 2000, num_correct / num_total))\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xHyV6YOjc-Jt"
   },
   "outputs": [],
   "source": [
    "# # Overall accuracy on the test set\n",
    "# correct, total = 0, 0\n",
    "# for images, labels in testloader:\n",
    "#     images, labels = images.to(device), labels.to(device)\n",
    "#     outputs = model(images)\n",
    "#     predicted = torch.argmax(outputs, dim=1)\n",
    "#     total += labels.size(0)\n",
    "#     correct += (predicted == labels).sum()\n",
    "# print('Accuracy on test set: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "image_label.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
