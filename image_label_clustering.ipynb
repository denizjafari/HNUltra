{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZbZRovgc-In"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from torch.utils import data\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "import io\n",
    "import uuid\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "import umap\n",
    "import hdbscan\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#import wandb\n",
    "#wandb_username = 'denizjafari'\n",
    "local_username = 'denizjafari'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2aVSXNyc-Iv"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0') \n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylfRtoN9c-Iy"
   },
   "outputs": [],
   "source": [
    "# root directory\n",
    "root_dir = \"/home/andreasabo/Documents/HNProject/\"\n",
    "split_file_base = \"/home/denizjafari/Documents/HNProject/HNUltra/\"\n",
    "\n",
    "# data directory on current machine: abhishekmoturu, andreasabo, denizjafari, navidkorhani\n",
    "data_dir = \"/home/\" + local_username + \"/Documents/HNProject/all_label_img/\"\n",
    "\n",
    "# read target df\n",
    "csv_path = os.path.join(root_dir, \"all_splits_1000000.csv\")\n",
    "data_df = pd.read_csv(csv_path, usecols=['subj_id', 'image_ids', 'view_label', 'view_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "j_OCR_7uy52w"
   },
   "outputs": [],
   "source": [
    "### **Reading Data Indicies and Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nnwavxcqGBv"
   },
   "outputs": [],
   "source": [
    "label_mapping = {'Other':0, 'Saggital_Right':1, 'Transverse_Right':2, \n",
    "                 'Saggital_Left':3, 'Transverse_Left':4, 'Bladder':5}\n",
    "label_unmapping = {0: 'Other', 1:'Saggital_Right', 2: 'Transverse_Right', \n",
    "                   3:'Saggital_Left', 4:'Transverse_Left', 5: 'Bladder'}\n",
    "\n",
    "data_df['view_label'] = data_df['view_label'].map(label_mapping)\n",
    "\n",
    "train_df = data_df[data_df.view_train == 1]\n",
    "test_df = data_df[data_df.view_train == 0]\n",
    "\n",
    "unique_subj = train_df.subj_id.unique()\n",
    "\n",
    "# Create the splits for 5-fold cross validation based on subj_id\n",
    "data_split_file = split_file_base + 'data_splits.json'\n",
    "if not os.path.isfile(data_split_file):\n",
    "\n",
    "    kf = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    fold = 0\n",
    "    all_folds = {}\n",
    "    for train_subj, val_subj in kf.split(unique_subj):\n",
    "        train_ids  = unique_subj[train_subj]\n",
    "        val_ids = unique_subj[val_subj]\n",
    "\n",
    "        train_images = train_df[train_df.subj_id.isin(train_ids)].image_ids.tolist()\n",
    "        val_images = train_df[train_df.subj_id.isin(val_ids)].image_ids.tolist()\n",
    "        train_labels = train_df[train_df.subj_id.isin(train_ids)].view_label.tolist()\n",
    "        val_labels = train_df[train_df.subj_id.isin(val_ids)].view_label.tolist()\n",
    "        cur_fold = {'train_ids': train_images, 'valid_ids': val_images, 'train_labels': train_labels, 'valid_labels': val_labels}\n",
    "        all_folds[fold] = cur_fold\n",
    "        fold += 1\n",
    "\n",
    "    print(\"Saving data splits\")\n",
    "    with open(data_split_file, 'w') as f:\n",
    "        json.dump(all_folds, f)\n",
    "\n",
    "else: # just load from file\n",
    "    print(\"Reading splits from file\")\n",
    "    with open(data_split_file, 'r') as f:\n",
    "        all_folds = json.load(f)\n",
    "        \n",
    "\n",
    "# just use one folds for now \n",
    "partition = all_folds['0']\n",
    "\n",
    "train_ids = train_df['image_ids'].tolist()\n",
    "train_labels = train_df['view_label'].tolist()\n",
    "test_ids = test_df['image_ids'].tolist()\n",
    "test_labels = test_df['view_label'].tolist()\n",
    "#\n",
    "partition_train = {'train_ids':train_ids, 'train_lables':train_labels}\n",
    "partition_test = {'test_ids':test_ids, 'test_lables':test_labels}\n",
    "#test_ids\n",
    "#test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, layers, feature_extracting):\n",
    "    ct = 0 \n",
    "    if feature_extracting:\n",
    "        for child in model.children():\n",
    "            ct +=1\n",
    "            if ct <layers:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BbgEWoqKc-JO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract,layers, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, layers, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.conv1 =  nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        #model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        model_ft.fc = nn.Sequential( nn.Linear(num_ftrs, 60), nn.Linear(60, num_classes) ) \n",
    "        input_size = 256\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    elif model_name == 'viewnet':\n",
    "        conv1_filters = 8\n",
    "        conv2_filters = 16\n",
    "        conv3_filters = 32\n",
    "        linear1_size = 512\n",
    "        dropout = 0.25\n",
    "        model_ft = ViewNet(num_classes, conv1_filters, conv2_filters, conv3_filters, linear1_size, dropout)\n",
    "        input_size = 256\n",
    "        \n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False, writer = None):\n",
    "    since = time.time()\n",
    "    classnames = ['Other', 'Saggital_Right', 'Transverse_Right', 'Saggital_Left','Transverse_Left', 'Bladder']\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 54)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            running_preds = []\n",
    "            running_labels = []\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                labels = labels.type(torch.long)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        labels = torch.argmax(labels, 1)\n",
    "                        running_preds += torch.argmax(outputs, 1).tolist()\n",
    "                        running_labels += labels.tolist()\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    preds = torch.argmax(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print('{} loss:\\t{:.4f} | {} acc:\\t{:.4f}\\n'.format(phase, epoch_loss, phase, epoch_acc))\n",
    "            # Log to tensorboard for visualization\n",
    "            if writer is not None and phase == 'train':\n",
    "                writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "                writer.add_scalar('Accuracy/train', epoch_acc, epoch)\n",
    "#                 cm = confusion_matrix(running_labels, running_preds)\n",
    "#                 figure = plot_confusion_matrix_local(cm, classnames)\n",
    "#                 writer.add_image('confusion_matrix/train', figure, epoch)\n",
    "                \n",
    "                \n",
    "                \n",
    "            if writer is not None and phase == 'val':\n",
    "                writer.add_scalar('Loss/val', epoch_loss, epoch)\n",
    "                writer.add_scalar('Accuracy/val', epoch_acc, epoch)\n",
    "                \n",
    "#                 cm = confusion_matrix(running_labels, running_preds)\n",
    "#                 figure = plot_confusion_matrix_local(cm, classnames)\n",
    "#                 writer.add_image('confusion_matrix/val', figure, epoch)\n",
    "            # deep copy the model\n",
    "            if phase == 'train':\n",
    "                print(classification_report(running_labels, running_preds))\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print(classification_report(running_labels, running_preds))\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iikM7_G3c-JR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception, viewnet]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset: right_sag, right_trav, left_sag, left_trav, bladder, other\n",
    "num_classes = 6\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 100\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 20\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model; when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "# Flag for whether or not to use pretrained model\n",
    "pretrain = True\n",
    "\n",
    "# number of layers to pre-train\n",
    "layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract=feature_extract, layers, use_pretrained=True)\n",
    "#print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(params_to_update, lr=1e-4, weight_decay=0, amsgrad=True) # lr=1e-4, weight_decay=0, amsgrad=False\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "class Dataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, list_IDs, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        img_path = data_dir + ID + '.jpg'\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        image = ToTensor()(image)\n",
    "        y = torch.FloatTensor([0]*6)\n",
    "       \n",
    "        y[int(self.labels[index])] = 1\n",
    "\n",
    "        return image, y\n",
    "# Parameters\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(partition['train_ids'], partition['train_labels'])\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(partition['valid_ids'], partition['valid_labels'])\n",
    "validation_generator = data.DataLoader(validation_set, **params)\n",
    "\n",
    "dataloaders_dict = {'train':training_generator, 'val':validation_generator}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, 25, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now that the model is fine-tuned, we create a new dataset of train and test for feature extraction and clustering purposes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the last layer features \n",
    "    \n",
    "class I(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(I, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "model = model_ft\n",
    "model = model.to(device)\n",
    "print(model.fc)\n",
    "model.fc[1] = I()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 1,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 6}\n",
    "\n",
    "# Generators of the train and test dataset for clustering purposes \n",
    "training_set = Dataset(partition_train['train_ids'], partition_train['train_lables'])\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "test_set = Dataset(partition_test['test_ids'], partition_test['test_lables'])\n",
    "test_generator = data.DataLoader(test_set, **params)\n",
    "\n",
    "dataloaders = {'train':training_generator, 'test':test_generator}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classnames = ['Other', 'Saggital_Right', 'Transverse_Right', 'Saggital_Left','Transverse_Left', 'Bladder']\n",
    "features_train = []\n",
    "features_test = []\n",
    "#label_predictions = []\n",
    "for image, label in dataloaders['train']:\n",
    "    features_train.append(np.squeeze(model(image.to(device)).tolist()))\n",
    "    #label_predictions.append(torch.argmax(label, 1)[0].item())\n",
    "    \n",
    "for image, label in dataloaders['test']:\n",
    "    features_test.append(np.squeeze(model(image.to(device)).tolist()))\n",
    "    #labels__predictions.append(torch.argmax(label, 1)[0].item())\n",
    "    \n",
    "\n",
    "print(len(features_train))\n",
    "print(len(features_test))\n",
    "print(features_train[-1].shape)\n",
    "print(features_test[-1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTERING DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA \n",
    "\n",
    "y_train = partition_train['train_lables']\n",
    "\n",
    "y_test = partition_test['test_lables']\n",
    "\n",
    "print(len(partition_train['train_lables']))\n",
    "print(len(y_train))\n",
    "print(y_train[0])\n",
    "\n",
    "print(len(partition_test['test_lables']))\n",
    "print(len(y_test))\n",
    "print(y_test[0])\n",
    "\n",
    "\n",
    "print(len(features_train))\n",
    "print(features_train[0].shape)\n",
    "\n",
    "print(len(features_test))\n",
    "print(features_test[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means on ResNet18 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance on train data\n",
    "kmeans = KMeans(n_clusters=6, random_state=0).fit(features_train)\n",
    "print(v_measure_score(y_train, kmeans.labels_))\n",
    "print(adjusted_rand_score(y_train, kmeans.labels_),\n",
    "    adjusted_mutual_info_score(y_train, kmeans.labels_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance on test data \n",
    "\n",
    "y_pred_res = kmeans.predict(features_test)\n",
    "print(v_measure_score(y_pred_res, y_test))\n",
    "print(adjusted_rand_score(y_pred_res, y_test),\n",
    "    adjusted_mutual_info_score(y_pred_res, y_test))\n",
    "\n",
    "#print(confusion_matrix(y_test, y_pred_res))\n",
    "print(classification_report(y_test, y_pred_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMENA visualized in 2D via UMAP \n",
    "\n",
    "# THE TRUE LABLES \n",
    "mapper_k = umap.UMAP(random_state=42).fit(features_train)\n",
    "mapper_k_t = mapper_k.trasform(features_test)\n",
    "plt.scatter(mapper_k.embedding_[:, 0], mapper_k.embedding_[:, 1], c=y_train, s=0.1, cmap='Spectral')\n",
    "plt.title('Train dataset Clustering via Kmean Visualized in 2D via UMAP - True Lables');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= KMeans(n_clusters=6, random_state=0).fit(mapper_k.embedding_)\n",
    "\n",
    "print(adjusted_mutual_info_score(y_train,y ))\n",
    "\n",
    "y= mapper_k.predict(features_test)\n",
    "\n",
    "print(adjusted_mutual_info_score(y_test,y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the lables predicted by kmeans \n",
    "plt.scatter(mapper_k.embedding_[:, 0], mapper_k.embedding_[:, 1], c=kmeans.labels_, s=0.1, cmap='Spectral');\n",
    "plt.title('Train dataset Clustering via Kmean Visualized in 2D via UMAP - Kmean Predicted Lables');"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP CLUSTERING \n",
    "\n",
    "the difference between unsupervised and supervised method of UMAP clustering \n",
    "\n",
    "supervised clstering by fitting the data to the train y and then testing it on val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all the parameters that were tested in a nice forloop for the report \n",
    "\n",
    "neighbours  = [10, 50, 100]\n",
    "distance = [0.0, 0.1, 0.5, 0.9]\n",
    "metrics = [\"euclidean\", \"manhattan\"]\n",
    "\n",
    "\n",
    "\n",
    "for n in neighbours:\n",
    "    for d in distance: \n",
    "        for m in metrics:\n",
    "            u_mapper = umap.UMAP(n_neighbors=n, min_dist=d, metric = m,\n",
    "                                  n_components=2,random_state=42).fit(features_train, y=y_train)\n",
    "            \n",
    "            fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "            plt.scatter(*u_mapper.embedding_.T, s=0.3, c=y_train, cmap='Spectral', alpha=1.0)\n",
    "            plt.setp(ax, xticks=[], yticks=[])\n",
    "            cbar = plt.colorbar(boundaries=np.arange(7)-0.5)\n",
    "            cbar.set_ticks(np.arange(6))\n",
    "            cbar.set_ticklabels(classnames)\n",
    "            plt.title('Unsupersived Train dataset Embedded via UMAP, n: {}, d: {}, m: {}'.format(n, d, m));\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_res = umap.UMAP(n_neighbors=10,  metric = \"manhattan\",\n",
    "                                  n_components=2,random_state=42).fit(features_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_res2 = umap.UMAP(n_neighbors=50, min_dist=0.0, metric = \"manhattan\",\n",
    "                                  n_components=2,random_state=42).fit(features_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embedding_1 = mapper_res.transform(features_test)\n",
    "val_embedding = mapper_res2.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(*mapper_res.embedding_.T, s=0.3, c=y_train, cmap='Spectral', alpha=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(7)-0.5)\n",
    "cbar.set_ticks(np.arange(6))\n",
    "cbar.set_ticklabels(classnames)\n",
    "plt.title('Unsupersived Train dataset Embedded via UMAP');\n",
    "\n",
    "plt.savefig('Unsupersived_Train.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(*mapper_res2.embedding_.T, s=0.3, c=np.array(y_train), cmap='Spectral', alpha=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(7)-0.5)\n",
    "cbar.set_ticks(np.arange(6))\n",
    "cbar.set_ticklabels(classnames)\n",
    "plt.title('Supervised Train Features Embedded via UMAP Transform');\n",
    "plt.savefig('supersived_Train.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(*val_embedding.T, s=10, c=np.array(y_test), cmap='Spectral', alpha=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(7)-0.5)\n",
    "cbar.set_ticks(np.arange(6))\n",
    "cbar.set_ticklabels(classnames)\n",
    "plt.title('Test Data Features Embedded via UMAP Transform');\n",
    "plt.savefig('supersived_Test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(*val_embedding_1.T, s=10, c=np.array(y_test), cmap='Spectral', alpha=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(7)-0.5)\n",
    "cbar.set_ticks(np.arange(6))\n",
    "cbar.set_ticklabels(classnames)\n",
    "plt.title('Test Data Features Embedded via UMAP Transform');\n",
    "plt.savefig('Unsupersived_Test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing How KNN and SVN Behave "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC().fit(features_train, y_train)\n",
    "knn = KNeighborsClassifier().fit(features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svc.score(features_test, y_test), knn.score(features_test, y_test))\n",
    "y_pred_svc = knn.predict(features_test)\n",
    "y_pred_knn = svc.predict(features_test)\n",
    "\n",
    "print(adjusted_rand_score(y_test, y_pred_svc), adjusted_mutual_info_score(y_test, y_pred_svc))\n",
    "print(v_measure_score(y_pred_svc, y_test))\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "print()\n",
    "print(adjusted_rand_score(y_test, y_pred_knn), adjusted_mutual_info_score(y_test, y_pred_knn))\n",
    "print(v_measure_score(y_pred_knn, y_test))\n",
    "print(classification_report(y_test, y_pred_knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_helper = umap.UMAP(n_neighbors=50,  metric = \"manhattan\",\n",
    "                                  n_components=6,random_state=42).fit(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(svc.score(umap_helper.transform(features_test), y_test),\n",
    "#      knn.score(umap_helper.transform(features_test), y_test))\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier().fit(umap_helper.embedding_, y_train)\n",
    "y = knn.predict(umap_helper.embedding_)\n",
    "print(adjusted_mutual_info_score(y_train, y))\n",
    "y_pred_knn = umap_helper.transform(features_test)\n",
    "print(y_pred_knn.shape)\n",
    "y_pred_knn = knn.predict(y_pred_knn)\n",
    "print(adjusted_mutual_info_score(y_test, y_pred_knn))\n",
    "#print(v_measure_score(y_pred_knn, y_test))\n",
    "#print(classification_report(y_test, y_pred_knn))\n",
    "#print(v_measure_score(y_val, kmeans_200.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tsne clustering based on fine-tuned resnet18 outputs, with labels of correct answer\n",
    "from matplotlib import cm\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_with_labels(lowDWeights, labels):\n",
    "    #plt.cla() # Clear current axis\n",
    "    X, Y = lowDWeights[:, 0], lowDWeights[:, 1]\n",
    "    for x, y, s in zip(X, Y, labels):\n",
    "        c = cm.rainbow(int(255 * s / 6)); plt.text(x, y, s, backgroundcolor=c, fontsize=9)\n",
    "    plt.xlim(X.min(), X.max()) \n",
    "    plt.ylim(Y.min(), Y.max())\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "preplexity = [5, 50, 100]\n",
    "# 10000 took extremely long time and no improvements \n",
    "n = [5000, 10000]\n",
    "init = ['pca', 'random']\n",
    "\n",
    "for p in preplexity:\n",
    "    for i in init: \n",
    "        tsne = TSNE(perplexity=p, n_components=2, init=i, n_iter=5000)\n",
    "        low_dim_embs = tsne.fit_transform(features_train)\n",
    "        plot_with_labels(low_dim_embs, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN on ResNet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN()\n",
    "clusterer.fit(features_train)\n",
    "\n",
    "print(clusterer.labels_.max())\n",
    "print(clusterer.labels_.min())\n",
    "clustered = (clusterer.labels_ >= 0)\n",
    "print(adjusted_mutual_info_score(np.array(y_train)[clustered], clusterer.labels_[clustered]))\n",
    "\n",
    "test_labels, strengths = hdbscan.approximate_predict(clusterer, features_test)\n",
    "\n",
    "clustered_t = (test_labels >= 0)\n",
    "print(adjusted_mutual_info_score(np.array(y_test)[clustered_t], clustered_t[clustered_t]))\n",
    "clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classnames_2 = ['noise','Other', 'Saggital_Right', 'Transverse_Right', 'Saggital_Left','Transverse_Left', 'Bladder', 'unknown']\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "color_palette = sns.color_palette('deep', 20)\n",
    "cluster_colors = [color_palette[x] if x >= 0\n",
    "              else (0.5, 0.5, 0.5)\n",
    "              for x in clusterer.labels_]\n",
    "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
    "                     zip(cluster_colors, clusterer.probabilities_)]\n",
    "\n",
    "plt.scatter(mapper_res.embedding_[:, 0], mapper_res.embedding_[:, 1], s=20, linewidth=0, c=cluster_member_colors, alpha=0.25, cmap='Spectral')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(mapper_res.embedding_[:, 0], mapper_res.embedding_[:, 1], c=y_train, s=30, cmap='Spectral')\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(7)-0.5)\n",
    "cbar.set_ticks(np.arange(6))\n",
    "cbar.set_ticklabels(classnames)\n",
    "plt.title('Train Encoded Features Embedded via UMAP');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "clustered = (clusterer.labels_ >= 0)\n",
    "plt.scatter(mapper_res.embedding_[~clustered, 0],\n",
    "            mapper_res.embedding_[~clustered, 1],\n",
    "            c=(0.5, 0.5, 0.5),\n",
    "            s=2,\n",
    "            alpha=0.5)\n",
    "\n",
    "plt.scatter(mapper_res.embedding_[clustered, 0],\n",
    "            mapper_res.embedding_[clustered, 1],\n",
    "            c=clusterer.labels_[clustered],\n",
    "            s=2,\n",
    "            cmap='Spectral');\n",
    "cbar = plt.colorbar(boundaries=np.arange(9)-0.5)\n",
    "cbar.set_ticks(np.arange(8))\n",
    "cbar.set_ticklabels(classnames_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v_measure_score(y_train, clusterer.labels_))\n",
    "print(adjusted_rand_score(np.array(y_train)[clustered], clusterer.labels_[clustered]),\n",
    "      adjusted_mutual_info_score(np.array(y_train)[clustered], clusterer.labels_[clustered]))\n",
    "print(confusion_matrix(y_train, clusterer.labels_))\n",
    "print(classification_report(y_train,clusterer.labels_))\n",
    "\n",
    "# NUmber of datapoints used \n",
    "np.sum(clustered) / len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRYING CLUSTERING METHODS ON RESNET FEATURES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster as cluster\n",
    "import time\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}\n",
    "\n",
    "def plot_clusters(data,y, algorithm, args, kwds):\n",
    "    start_time = time.time()\n",
    "    labels = algorithm(*args, **kwds).fit_predict(data)\n",
    "    #print(v_measure_score(y, labels))\n",
    "    #print(confusion_matrix(y, labels))\n",
    "    #print(classification_report(y,labels))\n",
    "    print(adjusted_mutual_info_score(y_train, kmeans.labels_))\n",
    "    end_time = time.time()\n",
    "    palette = sns.color_palette('deep', np.unique(labels).max() + 1)\n",
    "    colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n",
    "    plt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds)\n",
    "    frame = plt.gca()\n",
    "    frame.axes.get_xaxis().set_visible(False)\n",
    "    frame.axes.get_yaxis().set_visible(False)\n",
    "    plt.title('Clusters found by {}'.format(str(algorithm.__name__)), fontsize=24)\n",
    "   # plt.text(-0.5, 0.7, 'Clustering took {:.2f} s'.format(end_time - start_time), fontsize=14)\n",
    "\n",
    "\n",
    "def test_clusters(train,y_train,test,y_test, algorithm, args, kwds):\n",
    "    start_time = time.time()\n",
    "    #fitted = algorithm(*args, **kwds).fit(train)\n",
    "    labels = algorithm(*args, **kwds).fit_predict(train)\n",
    "    #print(v_measure_score(y, labels))\n",
    "    #print(confusion_matrix(y, labels))\n",
    "    #print(classification_report(y,labels))\n",
    "    print(adjusted_mutual_info_score(y_train, labels))\n",
    "    labels = algorithm(*args, **kwds).fit_predict(test)\n",
    "    print(adjusted_mutual_info_score(y_test, labels))\n",
    "    \n",
    "    \n",
    "def t_clusters(train,y_train,test,y_test, algorithm, args, kwds):\n",
    "    start_time = time.time()\n",
    "    fitted = algorithm(*args, **kwds).fit(train)\n",
    "    labels = fitted.predict(train)\n",
    "    #print(v_measure_score(y, labels))\n",
    "    #print(confusion_matrix(y, labels))\n",
    "    #print(classification_report(y,labels))\n",
    "    print(adjusted_mutual_info_score(y_train, labels))\n",
    "    labels = fitted.predict(test)\n",
    "    print(adjusted_mutual_info_score(y_test, labels))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_clusters(features_train, y_train,features_test, y_test, cluster.KMeans, (), {'n_clusters':6})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_clusters(mapper_res.embedding_, y_train, val_embedding_1,y_test, cluster.KMeans, (), {'n_clusters':6})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(features_train, y_train, cluster.AgglomerativeClustering, (), {'n_clusters':6, 'linkage':'ward'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(features_train, y_train, cluster.DBSCAN, (), {'eps':0.025})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clusters(features_train, y_train,features_test, y_test, hdbscan.HDBSCAN, (), {'min_cluster_size':5})\n",
    "\n",
    "test_labels, strengths = hdbscan.approximate_predict(clusterer, test_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE (Navid's encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_50, self).__init__()\n",
    "        \n",
    "        hidden_dim = 800\n",
    "        latent_dim = 50\n",
    "        self.fc1 = nn.Linear(65536, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 65536)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        #print(\"z.size() =\", z.size())\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        #print(\"h3.size() =\", h3.size())\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 65536))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    \n",
    "class VAE_100(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_100, self).__init__()\n",
    "        \n",
    "        hidden_dim = 800\n",
    "        latent_dim = 100\n",
    "        self.fc1 = nn.Linear(65536, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 65536)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        #print(\"z.size() =\", z.size())\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        #print(\"h3.size() =\", h3.size())\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 65536))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "class VAE_200(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_200, self).__init__()\n",
    "        \n",
    "        hidden_dim = 800\n",
    "        latent_dim = 200\n",
    "        self.fc1 = nn.Linear(65536, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 65536)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        #print(\"z.size() =\", z.size())\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        #print(\"h3.size() =\", h3.size())\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 65536))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae50_dir = '/home/navidkorhani/Documents/HNProject/HNUltra/saved models/vae_model_h800_l50.pt'\n",
    "vae100_dir = \"/home/navidkorhani/Documents/HNProject/HNUltra/results/h800_l100_e30/vae_model.pt\"\n",
    "vae200_dir = \"/home/navidkorhani/Documents/HNProject/HNUltra/results/h800_l200_e30/vae_model.pt\"\n",
    "\n",
    "model_50 = VAE_50()\n",
    "checkpoint = torch.load(vae50_dir)\n",
    "model_50.load_state_dict(checkpoint)\n",
    "\n",
    "model_100 = VAE_100()\n",
    "checkpoint = torch.load(vae100_dir)\n",
    "model_100.load_state_dict(checkpoint)\n",
    "\n",
    "model_200 = VAE_200()\n",
    "checkpoint = torch.load(vae200_dir)\n",
    "model_200.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this such that I can read the labels as well \n",
    "\n",
    "z_all_50 = []\n",
    "z_all_100 = []\n",
    "z_all_200 = []\n",
    "#z_labels = []\n",
    "for  image, label in training_set:\n",
    "    #z_labels.append(label)\n",
    "    mu, logvar = model_50.encode(image.view(-1, 65536))\n",
    "    z = model_50.reparameterize(mu, logvar)\n",
    "    z = z.detach().numpy()\n",
    "    z_all_50.append(z)\n",
    "    mu, logvar = model_100.encode(image.view(-1, 65536))\n",
    "    z = model_100.reparameterize(mu, logvar)\n",
    "    z = z.detach().numpy()\n",
    "    z_all_100.append(z)\n",
    "    mu, logvar = model_200.encode(image.view(-1, 65536))\n",
    "    z = model_200.reparameterize(mu, logvar)\n",
    "    z = z.detach().numpy()\n",
    "    z_all_200.append(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_all_50_v = []\n",
    "z_all_100_v = []\n",
    "z_all_200_v = []\n",
    "for  image, label in test_set:\n",
    "    #z_labels.append(label)\n",
    "    mu, logvar = model_50.encode(image.view(-1, 65536))\n",
    "    z = model_50.reparameterize(mu, logvar)\n",
    "    z = z.detach().numpy()\n",
    "    z_all_50_v.append(z)\n",
    "    mu, logvar = model_100.encode(image.view(-1, 65536))\n",
    "    z = model_100.reparameterize(mu, logvar)\n",
    "    z = z.detach().numpy()\n",
    "    z_all_100_v.append(z)\n",
    "    mu, logvar = model_200.encode(image.view(-1, 65536))\n",
    "    z = model_200.reparameterize(mu, logvar)\n",
    "    z = z.detach().numpy()\n",
    "    z_all_200_v.append(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(z_all_50))\n",
    "print(len(z_all_100))\n",
    "print(len(z_all_200))\n",
    "print(len(y_train))\n",
    "print(len(z_all_50_v))\n",
    "print(len(z_all_100_v))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_all_50 = [z.reshape(-1,) for z in z_all_50]\n",
    "z_all_100 = [z.reshape(-1,) for z in z_all_100]\n",
    "z_all_200 = [z.reshape(-1,) for z in z_all_200]\n",
    "\n",
    "z_all_50_v = [z.reshape(-1,) for z in z_all_50_v]\n",
    "z_all_100_v = [z.reshape(-1,) for z in z_all_100_v]\n",
    "z_all_200_v = [z.reshape(-1,) for z in z_all_200_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z_all_50[0].shape)\n",
    "print(z_all_50_v[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN and SVC on VAE\n",
    "\n",
    "\n",
    "#svc_50 = SVC().fit(z_all_50, y_train)\n",
    "knn_50 = KNeighborsClassifier().fit(z_all_50, y_train)\n",
    "z_pred_knn = knn_50.predict(z_all_50)\n",
    "print(adjusted_rand_score(y_train, z_pred_knn), adjusted_mutual_info_score(y_train, z_pred_knn))\n",
    "#z_pred_svc = svc_50.predict(z_all_50_v)\n",
    "z_pred_knn = knn_50.predict(z_all_50_v)\n",
    "\n",
    "#print(svc_50.score(z_pred_svc, y_test), knn_50.score(z_pred_knn, y_test))\n",
    "#print(classification_report(y_test, z_pred_knn))\n",
    "print(adjusted_rand_score(y_test, z_pred_knn), adjusted_mutual_info_score(y_test, z_pred_knn))\n",
    "#print(v_measure_score(z_pred_knn, y_test))\n",
    "print()\n",
    "\n",
    "#svc_50 = SVC().fit(z_all_100, y_train)\n",
    "knn_50 = KNeighborsClassifier().fit(z_all_100, y_train)\n",
    "z_pred_knn = knn_50.predict(z_all_100)\n",
    "print(adjusted_rand_score(y_train, z_pred_knn), adjusted_mutual_info_score(y_train, z_pred_knn))\n",
    "#z_pred_svc = knn_50.predict(z_all_100_v)\n",
    "z_pred_knn = knn_50.predict(z_all_100_v)\n",
    "\n",
    "#print(svc_50.score(z_pred_svc, y_test), knn_50.score(z_pred_knn, y_test))\n",
    "#print(classification_report(y_test, z_pred_knn))\n",
    "print(adjusted_rand_score(y_test, z_pred_knn), adjusted_mutual_info_score(y_test, z_pred_knn))\n",
    "#print(v_measure_score(z_pred_knn, y_test))\n",
    "print()\n",
    "\n",
    "#svc_50 = SVC().fit(z_all_200, y_train)\n",
    "knn_50 = KNeighborsClassifier().fit(z_all_200, y_train)\n",
    "z_pred_knn = knn_50.predict(z_all_200)\n",
    "print(adjusted_rand_score(y_train, z_pred_knn), adjusted_mutual_info_score(y_train, z_pred_knn))\n",
    "#z_pred_svc = svc_50.predict(z_all_200_v)\n",
    "z_pred_knn = knn_50.predict(z_all_200_v)\n",
    "\n",
    "#print(svc_50.score(z_pred_svc, y_test), knn_50.score(z_pred_knn, y_test))\n",
    "#print(classification_report(y_test, z_pred_knn))\n",
    "print(adjusted_rand_score(y_test, z_pred_knn), adjusted_mutual_info_score(y_test, z_pred_knn))\n",
    "#print(v_measure_score(z_pred_knn, y_test))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMEANS on VAE\n",
    "\n",
    "# training\n",
    "kmeans_50 = KMeans(n_clusters=6, random_state=0).fit(z_all_50)\n",
    "kmeans_100 = KMeans(n_clusters=6, random_state=0).fit(z_all_100)\n",
    "kmeans_200 = KMeans(n_clusters=6, random_state=0).fit(z_all_200)\n",
    "# kmeans_labels = KMeans(n_clusters=6).fit_predict(features_train)\n",
    "\n",
    "#print(v_measure_score(y_train, kmeans_50.labels_))\n",
    "print(adjusted_rand_score(y_train, kmeans_50.labels_), adjusted_mutual_info_score(y_train, kmeans_50.labels_))\n",
    "\n",
    "#print(v_measure_score(y_train, kmeans_100.labels_))\n",
    "print(adjusted_rand_score(y_train, kmeans_100.labels_), adjusted_mutual_info_score(y_train, kmeans_100.labels_))\n",
    "\n",
    "#print(v_measure_score(y_train, kmeans_200.labels_))\n",
    "print(adjusted_rand_score(y_train, kmeans_200.labels_), adjusted_mutual_info_score(y_train, kmeans_200.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set\n",
    "l_50 = kmeans_50.predict(z_all_50_v)\n",
    "print(adjusted_mutual_info_score(y_test, l_50))\n",
    "\n",
    "l_100 = kmeans_100.predict(z_all_100_v)\n",
    "print(adjusted_mutual_info_score(y_test, l_100))\n",
    "\n",
    "l_200 = kmeans_200.predict(z_all_200_v)\n",
    "print(adjusted_mutual_info_score(y_test, l_200))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TABLE FOR PRESENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_clusters(train,y_train,test,y_test, algorithm, args, kwds):\n",
    "    start_time = time.time()\n",
    "    fitted = algorithm(*args, **kwds).fit(train)\n",
    "    labels = fitted.predict(train)\n",
    "    #print(v_measure_score(y, labels))\n",
    "    #print(confusion_matrix(y, labels))\n",
    "    #print(classification_report(y,labels))\n",
    "    print(adjusted_mutual_info_score(y_train, labels))\n",
    "    labels = fitted.predict(test)\n",
    "    print(adjusted_mutual_info_score(y_test, labels))\n",
    "    print('End')\n",
    "    print()\n",
    "        \n",
    "    \n",
    "def zTrain_clusters(train,y_train, algorithm, args, kwds):\n",
    "    start_time = time.time()\n",
    "    #fitted = algorithm(*args, **kwds).fit(train)\n",
    "    labels = algorithm(*args, **kwds).fit_predict(train)\n",
    "    #print(v_measure_score(y, labels))\n",
    "    #print(confusion_matrix(y, labels))\n",
    "    #print(classification_report(y,labels))\n",
    "    print(adjusted_mutual_info_score(y_train, labels))\n",
    "    #labels = algorithm(*args, **kwds).fit_predict(test)\n",
    "    #print(adjusted_mutual_info_score(y_test, labels))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMEAN EMBEDDINGS \n",
    "mapper_z = umap.UMAP(n_neighbors=10,  metric = \"manhattan\",\n",
    "                                  n_components=2,random_state=42).fit(z_all_50)\n",
    "val_embedding_1 = mapper_res.transform(z_all_50_v)\n",
    "z_clusters(mapper_res.embedding_, y_train, val_embedding_1,y_test, cluster.KMeans, (), {'n_clusters':6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mapper_z = umap.UMAP(n_neighbors=10,  metric = \"manhattan\",\n",
    "                                  n_components=2,random_state=42).fit(z_all_100)\n",
    "val_embedding_1 = mapper_res.transform(z_all_100_v)\n",
    "z_clusters(mapper_res.embedding_, y_train, val_embedding_1,y_test, cluster.KMeans, (), {'n_clusters':6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mapper_z = umap.UMAP(n_neighbors=10,  metric = \"manhattan\",\n",
    "                                  n_components=2,random_state=42).fit(z_all_200)\n",
    "val_embedding_1 = mapper_res.transform(z_all_200_v)\n",
    "z_clusters(mapper_res.embedding_, y_train, val_embedding_1,y_test, cluster.KMeans, (), {'n_clusters':6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative \n",
    "zTrain_clusters(z_all_50,y_train, cluster.AgglomerativeClustering, (), {'n_clusters':6, 'linkage':'ward'})\n",
    "zTrain_clusters(z_all_100,y_train, cluster.AgglomerativeClustering, (), {'n_clusters':6, 'linkage':'ward'})\n",
    "zTrain_clusters(z_all_200,y_train, cluster.AgglomerativeClustering, (), {'n_clusters':6, 'linkage':'ward'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN \n",
    "zTrain_clusters(z_all_50,y_train, cluster.DBSCAN, (), {'eps':0.025})\n",
    "zTrain_clusters(z_all_100,y_train, cluster.DBSCAN, (), {'eps':0.025})\n",
    "zTrain_clusters(z_all_200,y_train, cluster.DBSCAN, (), {'eps':0.025})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN \n",
    "zTrain_clusters(z_all_50,y_train, hdbscan.HDBSCAN, (), {'min_cluster_size':5})\n",
    "zTrain_clusters(z_all_100,y_train, hdbscan.HDBSCAN, (), {'min_cluster_size':5})\n",
    "zTrain_clusters(z_all_200,y_train, hdbscan.HDBSCAN, (), {'min_cluster_size':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(metric = \"manhattan\")\n",
    "clusterer.fit(z_all_50)\n",
    "\n",
    "print(clusterer.labels_.max())\n",
    "print(clusterer.labels_.min())\n",
    "clustered = (clusterer.labels_ >= 0)\n",
    "print(adjusted_mutual_info_score(np.array(y_train)[clustered], clusterer.labels_[clustered]))\n",
    "\n",
    "test_labels, strengths = hdbscan.approximate_predict(clusterer, z_all_50_v)\n",
    "\n",
    "clustered_t = (test_labels >= 0)\n",
    "print(adjusted_mutual_info_score(np.array(y_test)[clustered_t], clustered_t[clustered_t]))\n",
    "clusterer.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP VISUALIZATION FOR VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_z = umap.UMAP(n_neighbors=10,  metric = \"manhattan\",\n",
    "                                  n_components=2,random_state=42).fit(z_all_100)\n",
    "\n",
    "val_embedding_1 = mapper_res.transform(z_all_100_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(*mapper_res.embedding_.T, s=0.3, c=np.array(y_train), cmap='Spectral', alpha=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(7)-0.5)\n",
    "cbar.set_ticks(np.arange(6))\n",
    "cbar.set_ticklabels(classnames)\n",
    "plt.title('VAE 100 Train Data Embedded via UMAP');\n",
    "\n",
    "#plt.savefig('Unsupersived_Train.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(*val_embedding_1.T, s=10, c=np.array(y_test), cmap='Spectral', alpha=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(7)-0.5)\n",
    "cbar.set_ticks(np.arange(6))\n",
    "cbar.set_ticklabels(classnames)\n",
    "plt.title('VAE 100 Test Data Embedded via UMAP');\n",
    "#plt.savefig('Unsupersived_Test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zTrain_clusters(z_all_100,y_train, cluster.DBSCAN, (), {'eps':0.025})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_clusters(z_all_100,y_train, hdbscan.HDBSCAN, (), {'min_cluster_size':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP CLUSTERING FOR 50 FEATURES \n",
    "\n",
    "\n",
    "clusterable_embedding_50 = umap.UMAP(n_neighbors=30, min_dist=0.0,metric = \"manhattan\",\n",
    "                                  n_components=2,random_state=42).fit_transform(z_all_50)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(clusterable_embedding_50[:, 0], clusterable_embedding_50[:, 1], c=y_train, s=10, cmap='Spectral')\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(7)-0.5)\n",
    "cbar.set_ticks(np.arange(6))\n",
    "cbar.set_ticklabels(classnames)\n",
    "plt.title('Train Encoded Features Embedded via UMAP - VAE 50');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP CLUSTERING FOR 100 FEATURES \n",
    "\n",
    "mapper_100 = umap.UMAP(n_neighbors=30, min_dist=0.0,metric = \"manhattan\",\n",
    "                                  n_components=2,random_state=42).fit(z_all_100)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(mapper_100.embedding_[:,0], mapper_100.embedding_[:,1],c=y_train, s=10, cmap='Spectral')\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(7)-0.5)\n",
    "cbar.set_ticks(np.arange(6))\n",
    "cbar.set_ticklabels(classnames)\n",
    "plt.title('Train Encoded Features Embedded via UMAP - VAE 100');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterable_embedding_100.embedding_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "val_embedding = mapper_100.transform(z_all_100_v)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(val_embedding[:, 0], val_embedding[:, 1], c=y_val, s=20, cmap='Spectral')\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(7)-0.5)\n",
    "cbar.set_ticks(np.arange(6))\n",
    "cbar.set_ticklabels(classnames)\n",
    "plt.title('Validation Encoded Features Embedded via UMAP - VAE 100');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mapper_200 = umap.UMAP(n_neighbors=30, min_dist=0.0,metric = \"manhattan\",\n",
    "                                  n_components=2,random_state=42).fit(z_all_200)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(mapper_200.embedding_[:,0], mapper_200.embedding_[:,1], c=y_train, s=10, cmap='Spectral')\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(7)-0.5)\n",
    "cbar.set_ticks(np.arange(6))\n",
    "cbar.set_ticklabels(classnames)\n",
    "plt.title('Train Encoded Features Embedded via UMAP - VAE 200');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterable_embedding = umap.UMAP(n_neighbors=30, min_dist=0.0,metric = \"manhattan\",\n",
    "                                  n_components=2,random_state=42).fit_transform(z_all_200)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], c=y_train, s=10, cmap='Spectral')\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(7)-0.5)\n",
    "cbar.set_ticks(np.arange(6))\n",
    "cbar.set_ticklabels(classnames)\n",
    "plt.title('Train Encoded Features Embedded via UMAP');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 latent variables \n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], c=y_train, s=30, cmap='Spectral')\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(7)-0.5)\n",
    "cbar.set_ticks(np.arange(6))\n",
    "cbar.set_ticklabels(classnames)\n",
    "plt.title('Train Encoded Features Embedded via UMAP');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 latent variables \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(mapper_100.embedding_[:, 0], mapper_100.embedding_[:, 1], c=y_train, s=30, cmap='Spectral')\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "cbar = plt.colorbar(boundaries=np.arange(7)-0.5)\n",
    "cbar.set_ticks(np.arange(6))\n",
    "cbar.set_ticklabels(classnames)\n",
    "plt.title('Train Encoded Features Embedded via UMAP');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer_z = hdbscan.HDBSCAN(min_cluster_size=5, prediction_data=True, metric='manhattan')\n",
    "clusterer_z.fit(z_all_100)\n",
    "print(clusterer_z.labels_.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer_z_50 = hdbscan.HDBSCAN(metric='manhattan')\n",
    "clusterer_z_50.fit(z_all_50)\n",
    "print(clusterer_z_50.labels_.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer_z_50.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer_z = hdbscan.HDBSCAN(min_cluster_size=5,prediction_data=True, metric='manhattan')\n",
    "clusterer_z.fit(z_all_200)\n",
    "print(clusterer_z.labels_.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(clustered) / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer_z.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRYING BUNCH of CLUSTERING ALGORITHMS on ResNet18 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(mapper_res.embedding_,y_train, cluster.AffinityPropagation, (), {'preference':-5.0, 'damping':0.95})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(mapper_res.embedding_,y_train, cluster.MeanShift, (0.175,), {'cluster_all':False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(mapper_res.embedding_,y_train, cluster.MeanShift, (0.175,), {'cluster_all':False})\n",
    "plot_clusters(mapper_res.embedding_,y_train, cluster.SpectralClustering, (), {'n_clusters':6})\n",
    "plot_clusters(mapper_res.embedding_,y_train, cluster.AgglomerativeClustering, (), {'n_clusters':6, 'linkage':'ward'})\n",
    "plot_clusters(mapper_res.embedding_,y_train, cluster.DBSCAN, (), {'eps':0.025})\n",
    "plot_clusters(mapper_res.embedding_,y_train, hdbscan.HDBSCAN, (), {'min_cluster_size':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(mapper_res.embedding_,y_train, cluster.AgglomerativeClustering, (), {'n_clusters':6, 'linkage':'ward'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(mapper_res.embedding_,y_train, cluster.DBSCAN, (), {'eps':0.025})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(mapper_res.embedding_,y_train, hdbscan.HDBSCAN, (), {'min_cluster_size':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "\n",
    "color_palette = sns.color_palette('deep', 40)\n",
    "cluster_colors = [color_palette[x] if x >= 0\n",
    "                  else (0.5, 0.5, 0.5)\n",
    "                  for x in clusterer_z.labels_]\n",
    "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
    "                         zip(cluster_colors, clusterer_z.probabilities_)]\n",
    "plt.scatter(*clusterable_embedding.T, s=20, linewidth=0, c=cluster_member_colors, alpha=0.25)\n",
    "\n",
    "cbar = plt.colorbar(boundaries=np.arange(11)-0.5)\n",
    "cbar.set_ticks(np.arange(10))\n",
    "cbar.set_ticklabels(classnames_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterable_embedding = umap.UMAP(n_neighbors=30, min_dist=0.0,metric = \"manhattan\",\n",
    "                                  n_components=2,random_state=42).fit_transform(features_train)\n",
    "\n",
    "labels_hdbscan = hdbscan.HDBSCAN(\n",
    "    min_samples=10, min_cluster_size=500).fit_predict(clusterable_embedding)\n",
    "\n",
    "\n",
    "clustered = (labels_hdbscan >= 0)\n",
    "plt.scatter(clusterable_embedding[~clustered, 0],\n",
    "            clusterable_embedding[~clustered, 1],\n",
    "            c=(0.5, 0.5, 0.5),\n",
    "            s=2,\n",
    "            alpha=0.5)\n",
    "plt.scatter(clusterable_embedding[clustered, 0],\n",
    "            clusterable_embedding[clustered, 1],\n",
    "            c=labels_hdbscan[clustered],\n",
    "            s=2,\n",
    "            cmap='Spectral');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_hdbscan.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v_measure_score(y_train, labels_hdbscan))\n",
    "print(adjusted_rand_score(y_train, labels_hdbscan), adjusted_mutual_info_score(y_train, labels_hdbscan))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "image_label.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "hnu",
   "language": "python",
   "name": "hnu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}