{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZbZRovgc-In"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from torch.utils import data\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "import io\n",
    "import uuid\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import wandb\n",
    "wandb_username = 'andreasabo'\n",
    "local_username = 'andreasabo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2aVSXNyc-Iv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:1') \n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylfRtoN9c-Iy"
   },
   "outputs": [],
   "source": [
    "# root directory\n",
    "root_dir = \"/home/andreasabo/Documents/HNProject/\"\n",
    "split_file_base = \"/home/andreasabo/Documents/HNUltra/\"\n",
    "\n",
    "# data directory on current machine: abhishekmoturu, andreasabo, denizjafari, navidkorhani\n",
    "data_dir = \"/home/\" + local_username + \"/Documents/HNProject/all_label_img/\"\n",
    "\n",
    "# read target df\n",
    "csv_path = os.path.join(root_dir, \"all_splits_1000000.csv\")\n",
    "data_df = pd.read_csv(csv_path, usecols=['subj_id', 'scan_num', 'view_label', 'image_ids', 'reflux_label', 'function_label', 'surgery_label', 'outcome_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  function_label image_ids reflux_label surgery_label view_label  subj_id  \\\n",
      "0        Missing  1323_2_1      Missing       Missing    Missing     1323   \n",
      "1        Missing  1323_2_2      Missing       Missing    Missing     1323   \n",
      "2        Missing  1323_2_3      Missing       Missing    Missing     1323   \n",
      "3        Missing  1323_2_4      Missing       Missing    Missing     1323   \n",
      "4        Missing  1323_2_5      Missing       Missing    Missing     1323   \n",
      "\n",
      "   scan_num  outcome_train  \n",
      "0         2            NaN  \n",
      "1         2            NaN  \n",
      "2         2            NaN  \n",
      "3         2            NaN  \n",
      "4         2            NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72459"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_df.head())\n",
    "len(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_OCR_7uy52w"
   },
   "source": [
    "### **Reading Data Indicies and Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nnwavxcqGBv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 9581 images (7230 train and 2351 test)\n",
      "Reading splits from file\n"
     ]
    }
   ],
   "source": [
    "# Drop the images for which we do not have view labels or the label is \"other\"\n",
    "data_df = data_df[data_df.view_label != \"Missing\"]\n",
    "data_df = data_df[data_df.view_label != \"Other\"]\n",
    "train_df = data_df[data_df.outcome_train == 1]\n",
    "test_df = data_df[data_df.outcome_train == 0]\n",
    "\n",
    "print(f\"We have {len(test_df) + len(train_df)} images ({len(train_df)} train and {len(test_df)} test)\")\n",
    "unique_subj = train_df.subj_id.unique()\n",
    "\n",
    "# Create the splits for 5-fold cross validation based on subj_id\n",
    "data_split_file = split_file_base + 'data_splits_outcome.json'\n",
    "if not os.path.isfile(data_split_file):\n",
    "\n",
    "    kf = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    fold = 0\n",
    "    all_folds = {}\n",
    "    for train_subj, val_subj in kf.split(unique_subj):\n",
    "        train_ids  = unique_subj[train_subj]\n",
    "        val_ids = unique_subj[val_subj]\n",
    "        \n",
    "        # Save the image names\n",
    "        train_images = train_df[train_df.subj_id.isin(train_ids)].image_ids.tolist()\n",
    "        val_images = train_df[train_df.subj_id.isin(val_ids)].image_ids.tolist()\n",
    "        \n",
    "        # Save the scan number\n",
    "        train_scan = train_df[train_df.subj_id.isin(train_ids)].scan_num.tolist()\n",
    "        val_scan = train_df[train_df.subj_id.isin(val_ids)].scan_num.tolist()\n",
    "        \n",
    "        # Save the view \n",
    "        train_views = train_df[train_df.subj_id.isin(train_ids)].view_label.tolist()\n",
    "        val_views = train_df[train_df.subj_id.isin(val_ids)].view_label.tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Save the outcome labels\n",
    "        train_function = train_df[train_df.subj_id.isin(train_ids)].function_label.tolist()\n",
    "        val_function = train_df[train_df.subj_id.isin(val_ids)].function_label.tolist()\n",
    "        \n",
    "        train_reflux = train_df[train_df.subj_id.isin(train_ids)].reflux_label.tolist()\n",
    "        val_reflux = train_df[train_df.subj_id.isin(val_ids)].reflux_label.tolist() \n",
    "        \n",
    "        train_surgery = train_df[train_df.subj_id.isin(train_ids)].surgery_label.tolist()\n",
    "        val_surgery = train_df[train_df.subj_id.isin(val_ids)].surgery_label.tolist()\n",
    "        \n",
    "        \n",
    "        val_labels = train_df[train_df.subj_id.isin(val_ids)].view_label.tolist()\n",
    "        cur_fold = {'train_images': train_images, 'val_images': val_images, 'train_reflux': train_reflux, \n",
    "                    'val_reflux': val_reflux, 'train_function': train_function, 'val_function': val_function, \n",
    "                    'train_surgery': train_surgery, 'val_surgery': val_surgery, 'train_scan': train_scan,\n",
    "                    'val_scan': val_scan, 'train_views': train_views, 'val_views': val_views}\n",
    "        \n",
    "        all_folds[fold] = cur_fold\n",
    "        fold += 1\n",
    "\n",
    "    print(\"Saving data splits\")\n",
    "    with open(data_split_file, 'w') as f:\n",
    "        json.dump(all_folds, f)\n",
    "        \n",
    "else: # just load from file\n",
    "    print(\"Reading splits from file\")\n",
    "    with open(data_split_file, 'r') as f:\n",
    "        all_folds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception, viewnet]\n",
    "model_name = \"viewnet\"\n",
    "\n",
    "# Number of classes in the dataset: right_sag, right_trav, left_sag, left_trav, bladder, other\n",
    "num_classes = 6\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 100\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 200\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model; when True we only update the reshaped layer params\n",
    "feature_extract = False\n",
    "\n",
    "# Flag for whether or not to use pretrained model\n",
    "pretrain = False\n",
    "\n",
    "# Flag for whether or not to sample sets of images by scan (ensures that all scans are seen the same number of times)\n",
    "sample_by_scan = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from: https://gist.github.com/stefanonardo/693d96ceb2f531fa05db530f3e21517d\n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=True):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if np.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iikM7_G3c-JR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_images', 'val_images', 'train_reflux', 'val_reflux', 'train_function', 'val_function', 'train_surgery', 'val_surgery', 'train_scan', 'val_scan', 'train_views', 'val_views'])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "class ScanDataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, image_list, reflux_labels, surgery_labels, function_labels, view_labels, scan_labels, binarize_labels = True, transformations=None, sample_by_scan=True):\n",
    "        'Initialization'\n",
    "        self.image_list = image_list\n",
    "        self.subj_id = np.asarray([int(s[0:4]) for s in image_list])\n",
    "\n",
    "        self.reflux_labels = reflux_labels        \n",
    "        self.surgery_labels = surgery_labels\n",
    "        self.function_labels = function_labels       \n",
    "        self.view_labels = view_labels\n",
    "        self.scan_labels = np.asarray(scan_labels)\n",
    "        self.binarize_labels = binarize_labels\n",
    "        self.transformations = transformations\n",
    "        self.sample_by_scan = sample_by_scan\n",
    "        self.image_return_order = ['Saggital_Right', 'Transverse_Right', 'Saggital_Left', 'Transverse_Left', 'Bladder']\n",
    "        \n",
    "        # Create a list of indices that we will access the images in\n",
    "        random.seed(0)\n",
    "        list_of_inds = [num for num in range(len(image_list))]\n",
    "        \n",
    "        # UPDATE: don't need this since the dataloader shuffles for us\n",
    "        #random.shuffle(list_of_inds) #shuffle method\n",
    "\n",
    "        self.index_order = list_of_inds\n",
    "        self.all_view_names = list(set((view_labels)))\n",
    "        \n",
    "        # If we are going to group by scans, create the scan list\n",
    "        self.all_scan_ids = np.asarray([(str(self.subj_id[i]) + \"_\" + str(self.scan_labels[i])) for i in range(len(self.scan_labels))])\n",
    "        self.unique_scans = list(set(self.all_scan_ids))\n",
    "        \n",
    "  def __len__(self):\n",
    "        'Denotes the total number of image_list or scan_list'\n",
    "        if self.sample_by_scan:\n",
    "            return len(self.unique_scans)\n",
    "        else:\n",
    "            return len(self.image_list)\n",
    "\n",
    "  def __getitem__(self, ind):\n",
    "        'Generates one sample of data'\n",
    "        if self.sample_by_scan:\n",
    "            cur_scan = self.unique_scans[ind]\n",
    "            images_of_scan = (self.all_scan_ids == cur_scan)\n",
    "            indexes_in_scan = [i for i, x in enumerate(images_of_scan) if x]\n",
    "            index = indexes_in_scan[0]\n",
    "            \n",
    "        else:\n",
    "            # Select sample based on current images\n",
    "            index = self.index_order[ind]\n",
    "            image_id = self.image_list[index]\n",
    "\n",
    "            # We want to use the current image, as well as images from the same scan (from the same patient)\n",
    "            # To fill images of the other 4 classes of images\n",
    "            # First finds all of the images in this scan\n",
    "            images_of_scan = (self.subj_id == self.subj_id[index]) & (self.scan_labels == self.scan_labels[index])\n",
    "            indexes_in_scan = [i for i, x in enumerate(images_of_scan) if x]\n",
    "        \n",
    "        # Group images in the scan by view and select one of each (if available)\n",
    "        dict_of_scans_by_view = {}\n",
    "        # looping through views\n",
    "        for inds in indexes_in_scan:\n",
    "            if self.view_labels[inds] not in dict_of_scans_by_view:\n",
    "                dict_of_scans_by_view[self.view_labels[inds]] = [inds]\n",
    "            else:\n",
    "                dict_of_scans_by_view[self.view_labels[inds]].append(inds)\n",
    "\n",
    "        # If we directly picked an image, make sure this one is the only one for the view\n",
    "        if not self.sample_by_scan:\n",
    "            dict_of_scans_by_view[self.view_labels[index]] = [index]\n",
    "\n",
    "                \n",
    "        output_image_path_dict = {}\n",
    "        # Now loop through all the views (other than the one for this image) and randomly select an image.  \n",
    "        for view in dict_of_scans_by_view:\n",
    "                random_view_from_scan_ind = random.choice(dict_of_scans_by_view[view])\n",
    "                output_image_path_dict[view] = data_dir + self.image_list[random_view_from_scan_ind] + '.jpg'\n",
    "                \n",
    "        output_images = []\n",
    "        for view in self.image_return_order:\n",
    "            if view not in output_image_path_dict:\n",
    "                empty_im = [np.ones((256, 256))*np.nan]\n",
    "                empty_im = torch.FloatTensor(empty_im)\n",
    "                output_images.append( empty_im)\n",
    "            else:\n",
    "                image = Image.open(output_image_path_dict[view]).convert('L')\n",
    "                if self.transformations:\n",
    "                    image = self.transformations(image)\n",
    "                image = ToTensor()(image)\n",
    "                output_images.append(image)\n",
    "        \n",
    "        \n",
    "        \n",
    "        y = [self.reflux_labels[index], self.surgery_labels[index], self.function_labels[index]]\n",
    "        for i, outcome in enumerate(y):\n",
    "            # Convert \"yes\" and \"no\" to 0/1\n",
    "            if outcome == \"No\":\n",
    "                y[i] = 0\n",
    "            elif outcome == \"Yes\":\n",
    "                y[i] = 1\n",
    "            elif outcome == \"Missing\":\n",
    "                y[i] = np.nan\n",
    "\n",
    "            # Should we also binarize the function labels?\n",
    "            elif self.binarize_labels:\n",
    "                if float(outcome) > 60 or float(outcome) < 40:\n",
    "                    y[i] = 1\n",
    "                else:\n",
    "                    y[i] = 0\n",
    "            else:\n",
    "                y[i] = float(outcome)\n",
    "\n",
    "        y = torch.FloatTensor(y)\n",
    "        output_images = torch.stack(output_images)\n",
    "        return output_images, y\n",
    "    \n",
    "# Use the first fold for now\n",
    "partition = all_folds['0']\n",
    "\n",
    "# Test out dataloaders\n",
    "shuffle = True\n",
    "num_workers = 0\n",
    "binarize_outcomes = True\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': shuffle,\n",
    "          'num_workers': num_workers}\n",
    "\n",
    "# Tranforms\n",
    "trans = transforms.Compose([transforms.RandomAffine(degrees=8, translate=(0.1, 0.1), scale=(0.95,1.25))])\n",
    "\n",
    "# Generators\n",
    "training_set = ScanDataset(partition['train_images'], partition['train_reflux'], partition['train_surgery'],\n",
    "                       partition['train_function'], partition['train_views'],partition['train_scan'], binarize_outcomes, trans, sample_by_scan)\n",
    "val_set = ScanDataset(partition['val_images'], partition['val_reflux'], partition['val_surgery'],\n",
    "                       partition['val_function'], partition['val_views'],partition['val_scan'], binarize_outcomes, trans, sample_by_scan)\n",
    "\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "val_generator = data.DataLoader(val_set, **params)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for visualizing images, \n",
    "# MAKE SURE THIS IS COMMENTED OUT WHEN COMMITTING!!\n",
    "# image_return_order = ['Saggital_Right', 'Transverse_Right', 'Saggital_Left', 'Transverse_Left', 'Bladder']\n",
    "\n",
    "# for inputs, labs in training_generator:\n",
    "#     plt.figure(figsize=(20,10)) \n",
    "#     first_scan = inputs[0]\n",
    "#     for i in range(5):\n",
    "#         im = first_scan[i,:, :, :]\n",
    "\n",
    "#         im_np = np.asarray(im).squeeze()\n",
    "#         plt.subplot(2,5,i+ 1)\n",
    "#         plt.imshow(im_np, cmap='gray')\n",
    "#         frame1 = plt.gca()\n",
    "#         frame1.axes.get_xaxis().set_visible(False)\n",
    "#         frame1.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "#         plt.title(image_return_order[i])\n",
    "        \n",
    "        \n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "image_label.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
